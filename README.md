# vidkompy

`vidkompy` is an intelligent command-line tool designed to overlay a foreground video onto a background video with high-precision automatic alignment. The system is engineered to handle differences in resolution, frame rate, duration, and audio, prioritizing content integrity and accuracy over raw processing speed.

The core philosophy is to treat the **foreground video as the definitive source of quality and timing**. All its frames are preserved without modification or retiming. The background video is dynamically adapted—stretched, retimed, and selectively sampled—to synchronize perfectly with the foreground content.

---

## How It Works: The Processing Pipeline

The alignment and composition process is orchestrated by the `AlignmentEngine` and follows a meticulous, multi-stage pipeline:

1.  **Video Analysis**: The process begins by probing both background and foreground videos using `ffprobe` to extract essential metadata. This includes resolution, frames per second (FPS), duration, frame count, and audio presence. This information is used to check for compatibility and inform subsequent alignment decisions.

2.  **Spatial Alignment**: The engine determines the optimal (x, y) coordinates to place the foreground video on top of the background. A sample frame from the middle of each video is used for this calculation, and the system computes the offset needed to position the foreground video correctly within the background frame.

3.  **Temporal Alignment**: This is the most critical phase, where the videos are synchronized in time. `vidkompy` finds the perfect background frame to match _every single_ foreground frame, ensuring the foreground's timing is flawlessly preserved. This creates a detailed frame-by-frame mapping using advanced algorithms like Dynamic Time Warping.

4.  **Video Composition**: With alignment data computed, the final video is composed:

- First, a silent video is created using OpenCV. The engine iterates through the frame alignment map, reads the corresponding frames from each video, overlays them using the spatial offset, and writes the composite frame to a temporary file. The output FPS is set to the foreground video's FPS to ensure no foreground frames are dropped.
- Next, an audio track is added using FFmpeg. The tool intelligently selects the audio source, **prioritizing the foreground video's audio**. The audio is synchronized with the composed video, and the final output file is generated.

---

## Alignment Algorithms in Detail

`vidkompy` employs sophisticated algorithms for both temporal and spatial alignment, with different modes to balance speed and precision.

### Temporal Alignment: Finding the Perfect Sync

Temporal alignment synchronizes the two videos over time. This is the most complex and powerful feature of `vidkompy` .

#### **`--temporal_align dtw`** (Default, Recommended)

This method uses **Dynamic Time Warping (DTW)** with **Perceptual Hashing** to achieve the most robust and accurate temporal alignment, completely avoiding the timing drift and "catch-up" issues seen in simpler methods.

It creates a unique "fingerprint" for every frame in both videos. Then, it finds the optimal way to stretch and squeeze the background video's timeline to best match the foreground's timeline, guaranteeing a smooth, continuous result.

**How it works**:

1.  **Frame Fingerprinting**: Instead of comparing raw pixels (which is slow), the system first computes a "perceptual fingerprint" for a sample of frames from each video. It uses a combination of multiple hashing algorithms (pHash, AverageHash, ColorMomentHash) that capture different visual aspects like structure, brightness, and color distribution. This results in a compact and robust representation of each frame.
2.  **Cost Matrix**: It builds a cost matrix where each cell `(i, j)` represents the "distance" (dissimilarity) between the fingerprint of foreground frame `i` and background frame `j`.
3.  **Dynamic Time Warping**: The DTW algorithm then finds the lowest-cost path through this matrix from start to finish. This path represents the optimal "warping" of the background's timeline to match the foreground's. A **Sakoe-Chiba band** constraint is used to keep the alignment within a reasonable time window, dramatically speeding up the calculation from O(N²) to O(N).
4.  **Interpolation**: The final alignment map for _every single_ foreground frame is generated by interpolating between the points on the optimal DTW path. This ensures that every foreground frame has a corresponding background frame, resulting in perfectly smooth motion.

#### **`--temporal_align classic`**

This is the legacy frame-matching method. It can be faster for some content but is susceptible to temporal drift.

It finds a few good matching "anchor" frames (keyframes) between the two videos and then guesses the frames in between.

**How it works**:

1.  **Keyframe Matching**: It samples a limited number of frames (keyframes) from both videos and finds the best matches using **Structural Similarity (SSIM)**.
2.  **Monotonic Filtering**: The matches are filtered to ensure a logical time progression (i.e., time only moves forward).
3.  **Interpolation**: For all the frames _between_ the keyframes, the corresponding background frame is estimated using simple linear interpolation. This is where drift can occur if the videos have different playback speeds between keyframes.

#### **`--match_time fast`** (Audio-Based)

This mode leverages audio tracks for a very fast initial alignment. It can be used with either `dtw` or `classic` temporal alignment.

It listens to both audio tracks and slides one until they sound perfectly synced up.

**How it works**: The audio from both videos is extracted, and the system computes the **cross-correlation** between them. The peak of the correlation reveals the precise time offset. If successful, this offset is used to create the frame map. If audio is missing or doesn't match, it automatically falls back to the configured frame-based method ( `dtw` or `classic` ).

### Spatial Alignment: Finding the Perfect Position

Spatial alignment determines where the foreground video sits within the larger background frame. If the foreground is larger than the background, it is automatically scaled down to fit while preserving its aspect ratio.

#### ** `--match_space precise` / `template` ** (Default)

It treats the foreground frame like a puzzle piece and finds exactly where it fits inside the background frame.

**How it works**: This method uses **Template Matching**. It takes the entire foreground frame as a template and performs a pixel-by-pixel search across the background frame to find the location with the highest normalized cross-correlation. It's extremely accurate when the foreground is an exact, un-altered crop of the background.

#### ** `--match_space fast` / `feature` **

It finds hundreds of unique interest points (like corners and edges) in both frames and matches them up to figure out the position.

**How it works**: This method uses **Feature Matching** with the **ORB (Oriented FAST and Rotated BRIEF)** algorithm. It detects key feature points in both frames and matches them. This approach is more robust against variations in brightness, contrast, or compression artifacts. If too few features are found, it safely falls back to centering the foreground.

#### **`--skip_spatial_align`**

If this flag is used, all alignment calculations are skipped, and the foreground video is simply centered within the background frame.

---

## Usage

### Prerequisites

You must have the **FFmpeg** binary installed on your system and accessible in your system's PATH. `vidkompy` depends on it for all video and audio processing tasks.

### Installation

The tool is a Python package and can be installed locally:

```bash
# Clone the repository and install using uv (or pip)
git clone https://github.com/your-repo/vidkompy.git
cd vidkompy
uv pip install .
```

### Command-Line Interface (CLI)

The tool is run from the command line. The primary arguments are the paths to the background and foreground videos.

```bash
python -m vidkompy --bg <background_video> --fg <foreground_video> [OPTIONS]
```

**Key Arguments:**

- `--bg` (str): Path to the background video file. **[Required]**
- `--fg` (str): Path to the foreground video file. **[Required]**
- `-o`, `--output` (str): Path for the final output video. If not provided, a name is automatically generated (e.g., `bg-stem_overlay_fg-stem.mp4`).
- `--match_time` (str): The high-level temporal alignment strategy.
  - `'border'` (default): Uses border regions for temporal matching. Most accurate for videos with visible borders.
  - `'precise'`: Uses the chosen frame-based method directly for maximum accuracy.
  - `'fast'`: Attempts audio-based alignment first and falls back to the frame-based method.
- `--temporal_align` (str): The core algorithm for frame-based temporal alignment.
  - `'dtw'` (default): Dynamic Time Warping with perceptual hashing. Highly robust and accurate.
  - `'classic'`: The legacy keyframe matching and interpolation method.
- `--match_space` (str): The spatial alignment method.
  - `'precise'` or `'template'` (default): Slower but more exact pixel matching.
  - `'fast'` or `'feature'`: Faster, more robust feature-based matching.
- `--trim` (bool): If `True` (default), the output video is trimmed to the duration of the aligned segment.
- `--skip_spatial_align` (bool): If `True` (default: `False`), skips spatial alignment and centers the foreground video.
- `--verbose` (bool): Enables detailed debug logging for troubleshooting.
- `--max_keyframes` (int): Sets the approximate number of frames to sample for temporal alignment (default: 200). Higher values can increase accuracy but also memory usage and processing time.
- `--border` (int): Thickness of border region in pixels for border-based matching (default: 8).
- `--blend` (bool): Enable smooth alpha blending at foreground edges (default: False).
- `--window` (int): Window size for sliding window refinement in temporal alignment (default: 0, auto-determined).

## Performance Improvements

Recent updates have significantly improved vidkompy's performance and accuracy:

### Drift Elimination
- **Adaptive Keyframe Density**: The tool now automatically adjusts the number of keyframes based on FPS differences between videos, preventing temporal drift
- **Default Keyframes**: Reduced from 2000 to 200 for optimal balance between speed and accuracy

### Border Mode Enhancements  
- **DTW + Masked Perceptual Hashing**: Border mode now works with DTW alignment using masked fingerprints, providing both speed and accuracy
- **Smart Border Detection**: Only processes visible border regions, optimizing performance

### Processing Speed
- **Parallel Cost Matrix Building**: Multi-threaded computation for frame similarity comparisons
- **Sequential Frame Reading**: Eliminated random seeks for 10-100x speedup in video composition
- **Perceptual Hashing**: Fast frame fingerprinting that's 100-1000x faster than pixel comparison

### Benchmarking
A benchmark script is included to test various configurations:

```bash
python benchmark.py
```

This will run vidkompy with different settings and report performance metrics.
