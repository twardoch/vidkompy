This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: varia, .specstory
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.cursor/
  rules/
    alignment-algorithms.mdc
    frame-matching-models.mdc
    video-composition-rules.mdc
    video-processing-flow.mdc
.giga/
  specifications.json
.github/
  workflows/
    push.yml
    release.yml
src/
  vidkompy/
    core/
      __init__.py
      alignment_engine.py
      spatial_alignment.py
      temporal_alignment.py
      video_processor.py
    __init__.py
    __main__.py
    __version__.py
    models.py
    vidkompy_old.py
    vidkompy.py
tests/
  test_package.py
.cursorrules
.gitignore
.pre-commit-config.yaml
CLAUDE.md
LICENSE
package.toml
PROGRESS.md
pyproject.toml
README.md
TODO.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/alignment-algorithms.mdc">
---
description: Technical specification for video alignment and synchronization algorithms in vidkompy
globs: src/vidkompy/core/alignment_engine.py,src/vidkompy/core/spatial_alignment.py,src/vidkompy/core/temporal_alignment.py
alwaysApply: false
---


# alignment-algorithms

## Spatial Alignment (Importance: 95)
- Template Matching Algorithm
  - Precise pixel-level matching for exact overlay positioning
  - Intelligent sub-region detection within background frame
  - Confidence scoring based on match quality

- Feature Matching Algorithm
  - ORB keypoint detection for robust positioning
  - Median displacement calculation for overlay coordinates
  - Fallback to center alignment below confidence threshold

## Temporal Alignment (Importance: 90) 
- Audio-Based Synchronization
  - Cross-correlation of audio streams to determine optimal offset
  - Preserves foreground audio timing as reference
  - Match quality validation through confidence scoring

- Frame-Based Alignment
  - Structural similarity analysis between frame pairs
  - Dynamic frame mapping to handle timing variations
  - Monotonic progression enforcement for timeline consistency
  - Adaptive keyframe sampling based on content changes

## Core Business Rules
- Foreground Frame Integrity
  - All foreground frames must be preserved without modification
  - Background frames can be stretched/modified to match
  - Output maintains foreground timing as reference

- Synchronization Priority
  - Audio matching takes precedence when available
  - Falls back to frame matching for silent videos
  - Duration-based centering as final fallback

- Resolution Constraints
  - Foreground must fit within background dimensions
  - Aspect ratios preserved during any scaling
  - Automatic position adjustment for optimal overlay

Relevant Files:
- src/vidkompy/core/alignment_engine.py
- src/vidkompy/core/spatial_alignment.py
- src/vidkompy/core/temporal_alignment.py

$END$
</file>

<file path=".cursor/rules/frame-matching-models.mdc">
---
description: Technical specification for video frame matching and scoring algorithms used in intelligent video overlay alignment
globs: src/vidkompy/core/frame_matching/**,src/vidkompy/models/frame.py,src/vidkompy/core/temporal_alignment.py
alwaysApply: false
---


# frame-matching-models

## Frame Matching Core Logic (Importance: 95)

- Two-phase frame matching system for temporal alignment:
  1. Keyframe selection using structural similarity (SSIM) scoring
  2. Dynamic frame mapping between foreground and background videos 

- Frame scoring criteria:
  - Content similarity between frames
  - Temporal consistency with adjacent matches
  - Monotonic progression enforcement
  - Confidence thresholds for match acceptance

## Temporal Frame Mapping (Importance: 90)

- Maintains foreground frame integrity while adjusting background frames
- Handles mismatched frame rates by:
  - Using highest input FPS for output
  - Creating dynamic frame mappings
  - Interpolating between keyframe matches
  - Preventing timeline inconsistencies

## Frame Selection Logic (Importance: 85)

- Keyframe sampling with configurable density (max 2000 frames)
- Frame quality assessment:
  - Minimum similarity threshold: 0.6
  - Local maxima detection for optimal matches
  - Frame drift detection and correction
  - Automatic rejection of low-confidence matches

## Frame Rate Handling (Importance: 80)

- Intelligent FPS management:
  - Always uses highest input frame rate
  - Preserves foreground frame timing
  - Dynamically adjusts background frame selection
  - Maintains smooth playback despite rate differences

$END$
</file>

<file path=".cursor/rules/video-composition-rules.mdc">
---
description: Documents core video composition rules and constraints for overlaying foreground onto background videos while preserving quality and timing
globs: src/vidkompy/core/*.py,src/vidkompy/models.py
alwaysApply: false
---


# video-composition-rules

## Core Composition Rules (Importance: 95)

1. Foreground Video Preservation
- All foreground frames must be preserved without modification
- Foreground timing cannot be altered or resampled
- Foreground audio takes precedence when available

2. Background Video Adaptation
- Background frames can be stretched/modified to match foreground
- Frame selection optimized to maintain consistent motion
- Automatic spatial offset calculation when foreground is smaller

3. Audio Handling
- Prioritize foreground audio track
- Use audio cross-correlation for temporal alignment when available
- Fallback to frame matching if audio unavailable/mismatched

## Quality Thresholds (Importance: 85)

1. Spatial Alignment
- Template matching confidence threshold: 0.6
- Minimum feature points for matching: 4
- Automatic fallback to center alignment below thresholds

2. Temporal Alignment
- Frame similarity minimum threshold for keyframe pairs
- Maximum allowed frame drift before correction
- Monotonic progression enforcement in frame mapping

3. Output Requirements
- Always use highest input FPS for output
- Maintain foreground aspect ratio
- Trim to optimal overlapping segment

## Frame Rate Management (Importance: 80)

1. FPS Handling
- Higher FPS input determines output frame rate
- Background frames interpolated/dropped to match foreground
- Frame timing preserved for foreground content

2. Duration Reconciliation
- Find optimal start/end points for overlay
- Trim excess frames from longer video
- Center shorter video if no better match found

$END$
</file>

<file path=".cursor/rules/video-processing-flow.mdc">
---
description: Technical documentation for video processing pipeline, frame handling, and overlay composition workflows.
globs: src/vidkompy/core/*.py,src/vidkompy/vidkompy_old.py
alwaysApply: false
---


# video-processing-flow

## Frame Processing Pipeline (Importance: 95)
1. Video Ingestion
- Separate handling for background and foreground video streams
- Frame extraction with FPS synchronization to highest rate
- Audio stream separation for temporal alignment

2. Frame Analysis
- Keyframe detection and extraction
- Frame similarity scoring using SSIM metrics
- Feature point detection for spatial positioning
- Frame pair matching for temporal alignment

3. Overlay Composition
- Dynamic frame mapping between foreground and background
- Spatial positioning based on template/feature matching
- Frame compositing with foreground overlay
- Audio stream integration based on availability

## Core Processing Rules
1. Frame Timing Management
- Preserve all foreground frames without modification
- Adjust background frames to match foreground timing
- Handle FPS mismatches through frame interpolation
- Maintain monotonic frame progression

2. Content Alignment Controls
- Spatial: Ensure foreground fits within background bounds
- Temporal: Match frame pairs for optimal overlay
- Audio: Use sound streams for coarse alignment
- Quality: Preserve foreground video integrity

3. Output Generation
- Use highest input FPS for output video
- Trim to matched frame segments
- Integrate synchronized audio track
- Generate aligned composite frames

## File Structure
```
src/vidkompy/
├── core/
│   ├── alignment_engine.py      # Frame matching orchestration
│   ├── spatial_alignment.py     # Position detection
│   ├── temporal_alignment.py    # Frame synchronization
│   └── video_processor.py       # Frame extraction/composition
└── vidkompy_old.py             # Main processing pipeline
```

$END$
</file>

<file path=".giga/specifications.json">
[
  {
    "fileName": "main-overview.mdc",
    "description": "Complete system overview covering the video overlay architecture, key components, and processing pipeline flow"
  },
  {
    "fileName": "alignment-algorithms.mdc",
    "description": "Detailed documentation of the spatial and temporal alignment algorithms, including template matching, feature matching, and frame/audio synchronization techniques"
  },
  {
    "fileName": "video-processing-flow.mdc",
    "description": "End-to-end data flow documentation covering video ingestion, frame processing, alignment calculations, overlay composition, and output generation"
  },
  {
    "fileName": "frame-matching-models.mdc",
    "description": "Technical specifications for frame matching and mapping models, including similarity scoring, keyframe selection, and frame rate handling logic"
  },
  {
    "fileName": "video-composition-rules.mdc",
    "description": "Documentation of the business rules and constraints for video composition, including foreground preservation, audio handling, and quality thresholds"
  }
]
</file>

<file path="src/vidkompy/core/__init__.py">
# this_file: src/vidkompy/core/__init__.py
</file>

<file path="src/vidkompy/core/alignment_engine.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/alignment_engine.py

"""
Main alignment engine that coordinates spatial and temporal alignment.

This is the high-level orchestrator that manages the complete video
overlay process.
"""

import tempfile
import cv2
import ffmpeg
import numpy as np
from pathlib import Path
from typing import Optional, Tuple
from loguru import logger
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.console import Console

from ..models import (
    VideoInfo, MatchTimeMode, ProcessingOptions,
    SpatialAlignment, TemporalAlignment, FrameAlignment
)
from .video_processor import VideoProcessor
from .spatial_alignment import SpatialAligner
from .temporal_alignment import TemporalAligner


console = Console()


class AlignmentEngine:
    """Orchestrates the complete video alignment and overlay process."""
    
    def __init__(
        self,
        processor: VideoProcessor,
        verbose: bool = False,
        max_keyframes: int = 2000
    ):
        """Initialize alignment engine.
        
        Args:
            processor: Video processor instance
            verbose: Enable verbose logging
            max_keyframes: Maximum keyframes for frame matching
        """
        self.processor = processor
        self.spatial_aligner = SpatialAligner()
        self.temporal_aligner = TemporalAligner(processor, max_keyframes)
        self.verbose = verbose
    
    def process(
        self,
        bg_path: str,
        fg_path: str,
        output_path: str,
        time_mode: MatchTimeMode,
        space_method: str,
        skip_spatial: bool,
        trim: bool
    ):
        """Process video overlay with alignment.
        
        Args:
            bg_path: Background video path
            fg_path: Foreground video path
            output_path: Output video path
            time_mode: Temporal alignment mode
            space_method: Spatial alignment method
            skip_spatial: Skip spatial alignment
            trim: Trim to overlapping segment
        """
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console
        ) as progress:
            # Analyze videos
            task = progress.add_task("Analyzing videos...", total=None)
            bg_info = self.processor.get_video_info(bg_path)
            fg_info = self.processor.get_video_info(fg_path)
            progress.update(task, completed=True)
            
            # Log compatibility
            self._log_compatibility(bg_info, fg_info)
            
            # Spatial alignment
            task = progress.add_task("Computing spatial alignment...", total=None)
            spatial_alignment = self._compute_spatial_alignment(
                bg_info, fg_info, space_method, skip_spatial
            )
            progress.update(task, completed=True)
            
            # Temporal alignment
            task = progress.add_task("Computing temporal alignment...", total=None) 
            temporal_alignment = self._compute_temporal_alignment(
                bg_info, fg_info, time_mode, trim
            )
            progress.update(task, completed=True)
            
            # Compose final video
            task = progress.add_task("Composing output video...", total=None)
            self._compose_video(
                bg_info, fg_info, output_path,
                spatial_alignment, temporal_alignment, trim
            )
            progress.update(task, completed=True)
            
            logger.info(f"✅ Processing complete: {output_path}")
    
    def _log_compatibility(self, bg_info: VideoInfo, fg_info: VideoInfo):
        """Log video compatibility information."""
        logger.info("Video compatibility check:")
        logger.info(f"  Resolution: {bg_info.width}x{bg_info.height} vs {fg_info.width}x{fg_info.height}")
        logger.info(f"  FPS: {bg_info.fps:.2f} vs {fg_info.fps:.2f}")
        logger.info(f"  Duration: {bg_info.duration:.2f}s vs {fg_info.duration:.2f}s")
        logger.info(f"  Audio: {'yes' if bg_info.has_audio else 'no'} vs {'yes' if fg_info.has_audio else 'no'}")
        
        if fg_info.width > bg_info.width or fg_info.height > bg_info.height:
            logger.warning("⚠️  Foreground is larger than background - will be scaled down")
    
    def _compute_spatial_alignment(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        method: str,
        skip: bool
    ) -> SpatialAlignment:
        """Compute spatial alignment using sample frames."""
        if skip:
            logger.info("Skipping spatial alignment - centering foreground")
            x_offset = (bg_info.width - fg_info.width) // 2
            y_offset = (bg_info.height - fg_info.height) // 2
            return SpatialAlignment(x_offset, y_offset, 1.0, 1.0)
        
        # Extract sample frames for alignment
        bg_frames = self.processor.extract_frames(bg_info.path, [bg_info.frame_count // 2])
        fg_frames = self.processor.extract_frames(fg_info.path, [fg_info.frame_count // 2])
        
        if not bg_frames or not fg_frames:
            logger.error("Failed to extract frames for spatial alignment")
            x_offset = (bg_info.width - fg_info.width) // 2
            y_offset = (bg_info.height - fg_info.height) // 2
            return SpatialAlignment(x_offset, y_offset, 1.0, 0.0)
        
        return self.spatial_aligner.align(
            bg_frames[0], fg_frames[0], method, skip
        )
    
    def _compute_temporal_alignment(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        mode: MatchTimeMode,
        trim: bool
    ) -> TemporalAlignment:
        """Compute temporal alignment based on mode."""
        if mode == MatchTimeMode.PRECISE:
            # Always use frame-based alignment
            return self.temporal_aligner.align_frames(bg_info, fg_info, trim)
        
        elif mode == MatchTimeMode.FAST:
            # Try audio first if available
            if bg_info.has_audio and fg_info.has_audio:
                logger.info("Attempting audio-based temporal alignment")
                
                with tempfile.TemporaryDirectory() as tmpdir:
                    bg_audio = Path(tmpdir) / "bg_audio.wav"
                    fg_audio = Path(tmpdir) / "fg_audio.wav"
                    
                    # Extract audio
                    if (self.processor.extract_audio(bg_info.path, str(bg_audio)) and
                        self.processor.extract_audio(fg_info.path, str(fg_audio))):
                        
                        offset = self.temporal_aligner.align_audio(
                            str(bg_audio), str(fg_audio)
                        )
                        
                        # Create simple frame alignment based on audio offset
                        frame_alignments = self._create_audio_based_alignment(
                            bg_info, fg_info, offset, trim
                        )
                        
                        return TemporalAlignment(
                            offset_seconds=offset,
                            frame_alignments=frame_alignments,
                            method_used="audio",
                            confidence=0.8
                        )
                    else:
                        logger.warning("Audio extraction failed, falling back to frames")
            else:
                logger.info("No audio available, using frame-based alignment")
            
            # Fallback to frame-based
            return self.temporal_aligner.align_frames(bg_info, fg_info, trim)
    
    def _create_audio_based_alignment(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        offset_seconds: float,
        trim: bool
    ) -> list[FrameAlignment]:
        """Create frame alignments based on audio offset."""
        alignments = []
        
        # Calculate frame offset
        bg_frame_offset = int(offset_seconds * bg_info.fps)
        
        # Determine range
        if trim:
            start_fg = 0
            end_fg = min(
                fg_info.frame_count,
                int((bg_info.duration - offset_seconds) * fg_info.fps)
            )
        else:
            start_fg = 0
            end_fg = fg_info.frame_count
        
        # Create alignments
        fps_ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0
        
        for fg_idx in range(start_fg, end_fg):
            bg_idx = int(fg_idx * fps_ratio + bg_frame_offset)
            bg_idx = max(0, min(bg_idx, bg_info.frame_count - 1))
            
            alignments.append(FrameAlignment(
                fg_frame_idx=fg_idx,
                bg_frame_idx=bg_idx,
                similarity_score=0.7  # Assumed good match from audio
            ))
        
        return alignments
    
    def _compose_video(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        output_path: str,
        spatial: SpatialAlignment,
        temporal: TemporalAlignment,
        trim: bool
    ):
        """Compose the final output video."""
        logger.info(
            f"Composing video with {temporal.method_used} temporal alignment"
        )
        
        # Use OpenCV for frame-accurate composition
        with tempfile.TemporaryDirectory() as tmpdir:
            # Create silent video first
            temp_video = Path(tmpdir) / "temp_silent.mp4"
            
            self._compose_with_opencv(
                bg_info, fg_info, str(temp_video),
                spatial, temporal.frame_alignments
            )
            
            # Add audio
            self._add_audio_track(
                str(temp_video), output_path,
                bg_info, fg_info, temporal
            )
    
    def _compose_with_opencv(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        output_path: str,
        spatial: SpatialAlignment,
        alignments: list[FrameAlignment]
    ):
        """Compose video using OpenCV for frame accuracy."""
        # Create video writer
        writer = self.processor.create_video_writer(
            output_path,
            bg_info.width,
            bg_info.height,
            fg_info.fps  # Use FG fps to preserve all FG frames
        )
        
        # Open video captures
        cap_bg = cv2.VideoCapture(bg_info.path)
        cap_fg = cv2.VideoCapture(fg_info.path)
        
        if not cap_bg.isOpened() or not cap_fg.isOpened():
            raise ValueError("Failed to open video files")
        
        try:
            frames_written = 0
            total_frames = len(alignments)
            
            for i, alignment in enumerate(alignments):
                # Read frames
                cap_fg.set(cv2.CAP_PROP_POS_FRAMES, alignment.fg_frame_idx)
                ret_fg, fg_frame = cap_fg.read()
                
                cap_bg.set(cv2.CAP_PROP_POS_FRAMES, alignment.bg_frame_idx)
                ret_bg, bg_frame = cap_bg.read()
                
                if not ret_fg or not ret_bg:
                    logger.warning(
                        f"Failed to read frames at fg={alignment.fg_frame_idx}, "
                        f"bg={alignment.bg_frame_idx}"
                    )
                    continue
                
                # Apply spatial alignment
                composite = self._overlay_frames(
                    bg_frame, fg_frame, spatial
                )
                
                writer.write(composite)
                frames_written += 1
                
                # Progress update
                if frames_written % 100 == 0:
                    pct = (frames_written / total_frames) * 100
                    logger.info(f"Composition progress: {pct:.1f}%")
            
            logger.info(f"Wrote {frames_written} frames")
            
        finally:
            cap_bg.release()
            cap_fg.release()
            writer.release()
    
    def _overlay_frames(
        self,
        bg_frame: np.ndarray,
        fg_frame: np.ndarray,
        spatial: SpatialAlignment
    ) -> np.ndarray:
        """Overlay foreground on background with spatial alignment."""
        composite = bg_frame.copy()
        
        # Apply scaling if needed
        if spatial.scale_factor != 1.0:
            new_w = int(fg_frame.shape[1] * spatial.scale_factor)
            new_h = int(fg_frame.shape[0] * spatial.scale_factor)
            fg_frame = cv2.resize(fg_frame, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        fg_h, fg_w = fg_frame.shape[:2]
        bg_h, bg_w = bg_frame.shape[:2]
        
        # Calculate ROI with bounds checking
        x_start = max(0, spatial.x_offset)
        y_start = max(0, spatial.y_offset)
        x_end = min(bg_w, spatial.x_offset + fg_w)
        y_end = min(bg_h, spatial.y_offset + fg_h)
        
        # Calculate foreground crop if needed
        fg_x_start = max(0, -spatial.x_offset)
        fg_y_start = max(0, -spatial.y_offset)
        fg_x_end = fg_x_start + (x_end - x_start)
        fg_y_end = fg_y_start + (y_end - y_start)
        
        # Overlay
        if x_end > x_start and y_end > y_start:
            composite[y_start:y_end, x_start:x_end] = \
                fg_frame[fg_y_start:fg_y_end, fg_x_start:fg_x_end]
        
        return composite
    
    def _add_audio_track(
        self,
        video_path: str,
        output_path: str,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        temporal: TemporalAlignment
    ):
        """Add audio track to the composed video."""
        # Prefer foreground audio as it's "better quality"
        if fg_info.has_audio:
            audio_source = fg_info.path
            audio_offset = 0.0  # FG audio is already aligned
            logger.info("Using foreground audio track")
        elif bg_info.has_audio:
            audio_source = bg_info.path
            audio_offset = -temporal.offset_seconds  # Compensate for alignment
            logger.info("Using background audio track")
        else:
            # No audio, just copy video
            logger.info("No audio tracks available")
            Path(video_path).rename(output_path)
            return
        
        # Merge audio with ffmpeg
        try:
            input_video = ffmpeg.input(video_path)
            
            if audio_offset != 0:
                input_audio = ffmpeg.input(audio_source, itsoffset=audio_offset)
            else:
                input_audio = ffmpeg.input(audio_source)
            
            stream = ffmpeg.output(
                input_video['v'],
                input_audio['a'],
                output_path,
                c='copy',
                acodec='aac',
                shortest=None
            )
            
            ffmpeg.run(stream, overwrite_output=True, capture_stderr=True)
            
        except ffmpeg.Error as e:
            logger.error(f"Audio merge failed: {e.stderr.decode()}")
            # Fallback - save without audio
            Path(video_path).rename(output_path)
            logger.warning("Saved video without audio")
</file>

<file path="src/vidkompy/core/spatial_alignment.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/spatial_alignment.py

"""
Spatial alignment module for finding optimal overlay positions.

Implements template matching and feature-based alignment methods.
"""

import cv2
import numpy as np
from typing import Tuple, Optional
from loguru import logger

from ..models import SpatialAlignment, SpatialMethod


class SpatialAligner:
    """Handles spatial alignment of foreground on background frames."""
    
    def align(
        self,
        bg_frame: np.ndarray,
        fg_frame: np.ndarray,
        method: str = "precise",
        skip_alignment: bool = False
    ) -> SpatialAlignment:
        """Find optimal position for foreground on background.
        
        Args:
            bg_frame: Background frame
            fg_frame: Foreground frame
            method: Alignment method ('precise'/'template' or 'fast'/'feature')
            skip_alignment: If True, just center the foreground
            
        Returns:
            SpatialAlignment with offset and scale
        """
        bg_h, bg_w = bg_frame.shape[:2]
        fg_h, fg_w = fg_frame.shape[:2]
        
        # Check if scaling needed
        scale_factor = 1.0
        if fg_w > bg_w or fg_h > bg_h:
            scale_factor = min(bg_w / fg_w, bg_h / fg_h)
            logger.warning(f"Foreground larger than background, scaling by {scale_factor:.3f}")
            
            # Scale foreground
            new_w = int(fg_w * scale_factor)
            new_h = int(fg_h * scale_factor)
            fg_frame = cv2.resize(fg_frame, (new_w, new_h), interpolation=cv2.INTER_AREA)
            fg_h, fg_w = new_h, new_w
        
        if skip_alignment:
            # Center alignment
            x_offset = (bg_w - fg_w) // 2
            y_offset = (bg_h - fg_h) // 2
            return SpatialAlignment(
                x_offset=x_offset,
                y_offset=y_offset,
                scale_factor=scale_factor,
                confidence=1.0
            )
        
        # Perform alignment
        if method in ["precise", "template"]:
            return self._template_matching(bg_frame, fg_frame, scale_factor)
        elif method in ["fast", "feature"]:
            return self._feature_matching(bg_frame, fg_frame, scale_factor)
        else:
            logger.warning(f"Unknown spatial method {method}, using center alignment")
            x_offset = (bg_w - fg_w) // 2
            y_offset = (bg_h - fg_h) // 2
            return SpatialAlignment(
                x_offset=x_offset,
                y_offset=y_offset,
                scale_factor=scale_factor,
                confidence=0.5
            )
    
    def _template_matching(
        self,
        bg_frame: np.ndarray,
        fg_frame: np.ndarray,
        scale_factor: float
    ) -> SpatialAlignment:
        """Find best position using template matching.
        
        Uses normalized cross-correlation to find the best match.
        """
        logger.debug("Using template matching for spatial alignment")
        
        # Convert to grayscale
        bg_gray = cv2.cvtColor(bg_frame, cv2.COLOR_BGR2GRAY)
        fg_gray = cv2.cvtColor(fg_frame, cv2.COLOR_BGR2GRAY)
        
        # Apply template matching
        result = cv2.matchTemplate(bg_gray, fg_gray, cv2.TM_CCOEFF_NORMED)
        
        # Find best match
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
        
        # Top-left corner of match
        x_offset, y_offset = max_loc
        
        logger.info(
            f"Template match found at ({x_offset}, {y_offset}) "
            f"with confidence {max_val:.3f}"
        )
        
        return SpatialAlignment(
            x_offset=x_offset,
            y_offset=y_offset,
            scale_factor=scale_factor,
            confidence=float(max_val)
        )
    
    def _feature_matching(
        self,
        bg_frame: np.ndarray,
        fg_frame: np.ndarray,
        scale_factor: float
    ) -> SpatialAlignment:
        """Find alignment using feature matching (ORB).
        
        More robust to changes but potentially less precise.
        """
        logger.debug("Using feature matching for spatial alignment")
        
        # Convert to grayscale
        bg_gray = cv2.cvtColor(bg_frame, cv2.COLOR_BGR2GRAY)
        fg_gray = cv2.cvtColor(fg_frame, cv2.COLOR_BGR2GRAY)
        
        # Initialize ORB detector
        orb = cv2.ORB_create(nfeatures=500)
        
        # Find keypoints and descriptors
        kp1, des1 = orb.detectAndCompute(bg_gray, None)
        kp2, des2 = orb.detectAndCompute(fg_gray, None)
        
        if des1 is None or des2 is None or len(des1) < 4 or len(des2) < 4:
            logger.warning("Not enough features for matching, using center alignment")
            bg_h, bg_w = bg_frame.shape[:2]
            fg_h, fg_w = fg_frame.shape[:2]
            return SpatialAlignment(
                x_offset=(bg_w - fg_w) // 2,
                y_offset=(bg_h - fg_h) // 2,
                scale_factor=scale_factor,
                confidence=0.3
            )
        
        # Match features
        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        matches = bf.match(des1, des2)
        
        if len(matches) < 4:
            logger.warning("Not enough matches, using center alignment")
            bg_h, bg_w = bg_frame.shape[:2]
            fg_h, fg_w = fg_frame.shape[:2]
            return SpatialAlignment(
                x_offset=(bg_w - fg_w) // 2,
                y_offset=(bg_h - fg_h) // 2,
                scale_factor=scale_factor,
                confidence=0.3
            )
        
        # Sort matches by distance
        matches = sorted(matches, key=lambda x: x.distance)
        
        # Extract matched points
        src_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        
        # Find homography
        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
        
        if M is None:
            logger.warning("Failed to find homography, using center alignment")
            bg_h, bg_w = bg_frame.shape[:2]
            fg_h, fg_w = fg_frame.shape[:2]
            return SpatialAlignment(
                x_offset=(bg_w - fg_w) // 2,
                y_offset=(bg_h - fg_h) // 2,
                scale_factor=scale_factor,
                confidence=0.3
            )
        
        # Transform corner points
        h, w = fg_gray.shape
        pts = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)
        dst = cv2.perspectiveTransform(pts, M)
        
        # Calculate offset from top-left corner
        x_offset = int(dst[0][0][0])
        y_offset = int(dst[0][0][1])
        
        # Calculate confidence based on inliers
        matches_mask = mask.ravel().tolist() if mask is not None else []
        inlier_ratio = sum(matches_mask) / len(matches_mask) if matches_mask else 0
        
        logger.info(
            f"Feature match found at ({x_offset}, {y_offset}) "
            f"with {sum(matches_mask)} inliers ({inlier_ratio:.2%})"
        )
        
        return SpatialAlignment(
            x_offset=x_offset,
            y_offset=y_offset,
            scale_factor=scale_factor,
            confidence=inlier_ratio
        )
</file>

<file path="src/vidkompy/core/temporal_alignment.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/temporal_alignment.py

"""
Temporal alignment module for synchronizing videos.

Implements audio-based and frame-based temporal alignment with
emphasis on preserving all foreground frames without retiming.
"""

import cv2
import numpy as np
import soundfile as sf
from scipy import signal
from skimage.metrics import structural_similarity as ssim
from typing import List, Tuple, Optional
from loguru import logger
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn
from rich.console import Console

from ..models import VideoInfo, FrameAlignment, TemporalAlignment
from .video_processor import VideoProcessor

console = Console()


class TemporalAligner:
    """Handles temporal alignment between videos."""
    
    def __init__(self, processor: VideoProcessor, max_keyframes: int = 2000):
        """Initialize temporal aligner.
        
        Args:
            processor: Video processor instance
            max_keyframes: Maximum keyframes for frame matching
        """
        self.processor = processor
        self.max_keyframes = max_keyframes
    
    def align_audio(
        self,
        bg_audio_path: str,
        fg_audio_path: str
    ) -> float:
        """Compute temporal offset using audio cross-correlation.
        
        Args:
            bg_audio_path: Background audio WAV file
            fg_audio_path: Foreground audio WAV file
            
        Returns:
            Offset in seconds (positive means FG starts later)
        """
        logger.debug("Computing audio cross-correlation")
        
        # Load audio
        bg_audio, bg_sr = sf.read(bg_audio_path)
        fg_audio, fg_sr = sf.read(fg_audio_path)
        
        if bg_sr != fg_sr:
            logger.warning(f"Sample rates differ: {bg_sr} vs {fg_sr}")
            return 0.0
        
        # Compute cross-correlation
        correlation = signal.correlate(bg_audio, fg_audio, mode='full', method='fft')
        
        # Find peak
        peak_idx = np.argmax(np.abs(correlation))
        
        # Convert to time offset
        center = len(bg_audio) - 1
        lag_samples = peak_idx - center
        offset_seconds = lag_samples / bg_sr
        
        # Calculate confidence
        peak_value = np.abs(correlation[peak_idx])
        avg_value = np.mean(np.abs(correlation))
        confidence = peak_value / avg_value if avg_value > 0 else 0
        
        logger.info(
            f"Audio alignment: offset={offset_seconds:.3f}s, "
            f"confidence={confidence:.2f}"
        )
        
        return offset_seconds
    
    def align_frames(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        trim: bool = False
    ) -> TemporalAlignment:
        """Align videos using frame content matching.
        
        This method ensures ALL foreground frames are preserved without
        retiming. It finds the optimal background frame for each foreground
        frame.
        
        Args:
            bg_info: Background video metadata
            fg_info: Foreground video metadata  
            trim: Whether to trim to overlapping segment
            
        Returns:
            TemporalAlignment with frame mappings
        """
        logger.info("Starting frame-based temporal alignment")
        
        # Sample frames for keyframe matching
        keyframe_matches = self._find_keyframe_matches(bg_info, fg_info)
        
        if not keyframe_matches:
            logger.warning("No keyframe matches found, using direct mapping")
            # Fallback to simple frame mapping
            return self._create_direct_mapping(bg_info, fg_info)
        
        # Build complete frame alignment preserving all FG frames
        frame_alignments = self._build_frame_alignments(
            bg_info, fg_info, keyframe_matches, trim
        )
        
        # Calculate overall temporal offset
        first_match = keyframe_matches[0]
        offset_seconds = (first_match[0] / bg_info.fps) - (first_match[1] / fg_info.fps)
        
        return TemporalAlignment(
            offset_seconds=offset_seconds,
            frame_alignments=frame_alignments,
            method_used="frames",
            confidence=self._calculate_alignment_confidence(keyframe_matches)
        )
    
    def _find_keyframe_matches(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo
    ) -> List[Tuple[int, int, float]]:
        """Find matching keyframes between videos.
        
        Returns:
            List of (bg_frame_idx, fg_frame_idx, similarity) tuples
        """
        # Determine sampling rate
        target_keyframes = min(self.max_keyframes, fg_info.frame_count)
        sample_interval = max(1, fg_info.frame_count // target_keyframes)
        
        logger.info(f"Sampling every {sample_interval} frames for keyframe matching")
        
        # Extract sampled frames
        fg_indices = list(range(0, fg_info.frame_count, sample_interval))
        fg_frames = self.processor.extract_frames(
            fg_info.path, fg_indices, resize_factor=0.25
        )
        
        # Sample background more densely for better matching
        bg_sample_interval = max(1, sample_interval // 2)
        bg_indices = list(range(0, bg_info.frame_count, bg_sample_interval))
        bg_frames = self.processor.extract_frames(
            bg_info.path, bg_indices, resize_factor=0.25
        )
        
        if not fg_frames or not bg_frames:
            logger.error("Failed to extract frames for matching")
            return []
        
        # Find matches with progress
        matches = []
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            "[progress.percentage]{task.percentage:>3.0f}%",
            TimeRemainingColumn(),
            console=console,
            transient=True
        ) as progress:
            task = progress.add_task(
                "Matching keyframes...", 
                total=len(fg_frames)
            )
            
            for fg_idx, fg_frame in enumerate(fg_frames):
                if fg_frame is None:
                    progress.update(task, advance=1)
                    continue
                    
                fg_actual_idx = fg_indices[fg_idx]
                best_match = self._find_best_match(
                    fg_frame, fg_actual_idx, bg_frames, bg_indices, 
                    fg_info, bg_info
                )
                
                if best_match and best_match[2] > 0.6:  # Similarity threshold
                    matches.append(best_match)
                
                progress.update(task, advance=1)
        
        # Filter to ensure monotonic progression
        matches = self._filter_monotonic(matches)
        
        logger.info(f"Found {len(matches)} keyframe matches")
        return matches
    
    def _find_best_match(
        self,
        fg_frame: np.ndarray,
        fg_idx: int,
        bg_frames: List[np.ndarray],
        bg_indices: List[int],
        fg_info: VideoInfo,
        bg_info: VideoInfo
    ) -> Optional[Tuple[int, int, float]]:
        """Find best matching background frame for a foreground frame.
        
        Returns:
            (bg_frame_idx, fg_frame_idx, similarity) or None
        """
        # Estimate expected position
        time_ratio = bg_info.duration / fg_info.duration if fg_info.duration > 0 else 1.0
        expected_bg_idx = int(fg_idx * time_ratio * (bg_info.fps / fg_info.fps))
        
        # Search window (adaptive based on confidence)
        window_size = max(len(bg_frames) // 10, 20)  # At least 20 frames
        
        best_similarity = 0.0
        best_bg_idx = -1
        
        for i, bg_frame in enumerate(bg_frames):
            if bg_frame is None:
                continue
                
            actual_bg_idx = bg_indices[i]
            
            # Skip if too far from expected position (unless early in video)
            if fg_idx > 50 and abs(actual_bg_idx - expected_bg_idx) > window_size * 2:
                continue
            
            similarity = self._compute_frame_similarity(bg_frame, fg_frame)
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_bg_idx = actual_bg_idx
        
        if best_bg_idx >= 0:
            return (best_bg_idx, fg_idx, best_similarity)
        return None
    
    def _compute_frame_similarity(
        self,
        frame1: np.ndarray,
        frame2: np.ndarray
    ) -> float:
        """Compute similarity between two frames using SSIM."""
        # Convert to grayscale
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
        
        # Ensure same size
        if gray1.shape != gray2.shape:
            gray2 = cv2.resize(gray2, (gray1.shape[1], gray1.shape[0]))
        
        # Compute SSIM
        score, _ = ssim(gray1, gray2, full=True)
        return float(score)
    
    def _filter_monotonic(
        self,
        matches: List[Tuple[int, int, float]]
    ) -> List[Tuple[int, int, float]]:
        """Filter matches to ensure monotonic progression."""
        if not matches:
            return matches
        
        # Sort by foreground index
        matches.sort(key=lambda x: x[1])
        
        # Filter non-monotonic background indices
        filtered = []
        last_bg_idx = -1
        
        for bg_idx, fg_idx, sim in matches:
            if bg_idx > last_bg_idx:
                filtered.append((bg_idx, fg_idx, sim))
                last_bg_idx = bg_idx
            else:
                logger.debug(
                    f"Filtering non-monotonic match: bg[{bg_idx}] for fg[{fg_idx}]"
                )
        
        return filtered
    
    def _build_frame_alignments(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        keyframe_matches: List[Tuple[int, int, float]],
        trim: bool
    ) -> List[FrameAlignment]:
        """Build complete frame-to-frame alignment.
        
        Creates alignment for EVERY foreground frame, finding the optimal
        background frame based on keyframe matches.
        """
        alignments = []
        
        # Determine range of foreground frames to process
        if trim and keyframe_matches:
            start_fg = keyframe_matches[0][1]
            end_fg = keyframe_matches[-1][1] + 1
        else:
            start_fg = 0
            end_fg = fg_info.frame_count
        
        logger.info(f"Building alignment for FG frames {start_fg} to {end_fg-1}")
        
        # For each foreground frame, find optimal background frame
        for fg_idx in range(start_fg, end_fg):
            bg_idx = self._interpolate_bg_frame(
                fg_idx, keyframe_matches, bg_info, fg_info
            )
            
            # Ensure bg_idx is valid
            bg_idx = max(0, min(bg_idx, bg_info.frame_count - 1))
            
            # Estimate similarity based on nearby keyframes
            similarity = self._estimate_similarity(fg_idx, keyframe_matches)
            
            alignments.append(FrameAlignment(
                fg_frame_idx=fg_idx,
                bg_frame_idx=bg_idx,
                similarity_score=similarity
            ))
        
        logger.info(f"Created {len(alignments)} frame alignments")
        return alignments
    
    def _interpolate_bg_frame(
        self,
        fg_idx: int,
        keyframe_matches: List[Tuple[int, int, float]],
        bg_info: VideoInfo,
        fg_info: VideoInfo
    ) -> int:
        """Interpolate background frame index for given foreground frame.
        
        Uses smooth interpolation between keyframe matches to avoid
        sudden jumps or speed changes.
        """
        if not keyframe_matches:
            # Simple ratio-based mapping
            ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0
            return int(fg_idx * ratio)
        
        # Find surrounding keyframes
        prev_match = None
        next_match = None
        
        for match in keyframe_matches:
            if match[1] <= fg_idx:
                prev_match = match
            elif match[1] > fg_idx and next_match is None:
                next_match = match
                break
        
        # Handle edge cases
        if prev_match is None:
            prev_match = keyframe_matches[0]
        if next_match is None:
            next_match = keyframe_matches[-1]
        
        # If at or beyond edges, extrapolate
        if fg_idx <= prev_match[1]:
            # Before first keyframe
            fps_ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0
            offset = (prev_match[1] - fg_idx) * fps_ratio
            return int(prev_match[0] - offset)
        
        if fg_idx >= next_match[1]:
            # After last keyframe
            fps_ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0
            offset = (fg_idx - next_match[1]) * fps_ratio
            return int(next_match[0] + offset)
        
        # Interpolate between keyframes
        if prev_match[1] == next_match[1]:
            return prev_match[0]
        
        # Calculate position ratio with smooth interpolation
        ratio = (fg_idx - prev_match[1]) / (next_match[1] - prev_match[1])
        
        # Apply smoothstep for more natural motion
        smooth_ratio = ratio * ratio * (3.0 - 2.0 * ratio)
        
        # Interpolate background frame
        bg_idx = prev_match[0] + smooth_ratio * (next_match[0] - prev_match[0])
        
        return int(bg_idx)
    
    def _estimate_similarity(
        self,
        fg_idx: int,
        keyframe_matches: List[Tuple[int, int, float]]
    ) -> float:
        """Estimate similarity score for a frame based on nearby keyframes."""
        if not keyframe_matches:
            return 0.5
        
        # Find closest keyframe
        min_dist = float('inf')
        closest_sim = 0.5
        
        for _, kf_fg_idx, similarity in keyframe_matches:
            dist = abs(fg_idx - kf_fg_idx)
            if dist < min_dist:
                min_dist = dist
                closest_sim = similarity
        
        # Decay similarity based on distance
        decay_rate = 0.95 ** min_dist
        return closest_sim * decay_rate
    
    def _calculate_alignment_confidence(
        self,
        keyframe_matches: List[Tuple[int, int, float]]
    ) -> float:
        """Calculate overall confidence in the alignment."""
        if not keyframe_matches:
            return 0.0
        
        # Average similarity of matches
        avg_similarity = sum(m[2] for m in keyframe_matches) / len(keyframe_matches)
        
        # Coverage (how well distributed the matches are)
        coverage = len(keyframe_matches) / max(len(keyframe_matches), 20)
        
        return min(1.0, avg_similarity * coverage)
    
    def _create_direct_mapping(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo
    ) -> TemporalAlignment:
        """Create simple direct frame mapping as fallback."""
        fps_ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0
        
        alignments = []
        for fg_idx in range(fg_info.frame_count):
            bg_idx = int(fg_idx * fps_ratio)
            bg_idx = min(bg_idx, bg_info.frame_count - 1)
            
            alignments.append(FrameAlignment(
                fg_frame_idx=fg_idx,
                bg_frame_idx=bg_idx,
                similarity_score=0.5
            ))
        
        return TemporalAlignment(
            offset_seconds=0.0,
            frame_alignments=alignments,
            method_used="direct",
            confidence=0.3
        )
</file>

<file path="src/vidkompy/core/video_processor.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/video_processor.py

"""
Core video processing functionality.

Handles video I/O, metadata extraction, and frame operations.
"""

import cv2
import ffmpeg
import numpy as np
from pathlib import Path
from typing import List, Optional, Tuple
from loguru import logger
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.console import Console

from ..models import VideoInfo

console = Console()


class VideoProcessor:
    """Handles core video processing operations."""
    
    def get_video_info(self, video_path: str) -> VideoInfo:
        """Extract video metadata using ffprobe.
        
        Args:
            video_path: Path to video file
            
        Returns:
            VideoInfo object with metadata
            
        Raises:
            ValueError: If video cannot be probed
        """
        logger.debug(f"Probing video: {video_path}")
        
        try:
            probe = ffmpeg.probe(video_path)
            
            # Find video stream
            video_stream = next(
                (s for s in probe["streams"] if s["codec_type"] == "video"), 
                None
            )
            
            if not video_stream:
                raise ValueError(f"No video stream found in {video_path}")
            
            # Extract properties
            width = int(video_stream["width"])
            height = int(video_stream["height"])
            
            # Parse frame rate
            fps_str = video_stream.get("r_frame_rate", "0/1")
            if "/" in fps_str:
                num, den = map(int, fps_str.split("/"))
                fps = num / den if den != 0 else 0
            else:
                fps = float(fps_str)
            
            duration = float(probe["format"].get("duration", 0))
            
            # Calculate frame count
            frame_count = int(video_stream.get("nb_frames", 0))
            if frame_count == 0 and duration > 0 and fps > 0:
                frame_count = int(duration * fps)
            
            # Check audio
            audio_stream = next(
                (s for s in probe["streams"] if s["codec_type"] == "audio"),
                None
            )
            
            has_audio = audio_stream is not None
            audio_sample_rate = None
            audio_channels = None
            
            if audio_stream:
                audio_sample_rate = int(audio_stream.get("sample_rate", 0))
                audio_channels = int(audio_stream.get("channels", 0))
            
            info = VideoInfo(
                width=width,
                height=height,
                fps=fps,
                duration=duration,
                frame_count=frame_count,
                has_audio=has_audio,
                audio_sample_rate=audio_sample_rate,
                audio_channels=audio_channels,
                path=video_path
            )
            
            logger.info(
                f"Video info for {Path(video_path).name}: "
                f"{width}x{height}, {fps:.2f} fps, {duration:.2f}s, "
                f"{frame_count} frames, audio: {'yes' if has_audio else 'no'}"
            )
            
            return info
            
        except Exception as e:
            logger.error(f"Failed to probe video {video_path}: {e}")
            raise
    
    def extract_frames(
        self, 
        video_path: str, 
        frame_indices: List[int],
        resize_factor: float = 1.0
    ) -> List[np.ndarray]:
        """Extract specific frames from video.
        
        Args:
            video_path: Path to video file
            frame_indices: List of frame indices to extract
            resize_factor: Factor to resize frames (for performance)
            
        Returns:
            List of frames as numpy arrays
        """
        frames = []
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            logger.error(f"Failed to open video: {video_path}")
            return frames
        
        try:
            # Only show progress for large frame extractions
            if len(frame_indices) > 50:
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    BarColumn(),
                    console=console,
                    transient=True
                ) as progress:
                    task = progress.add_task(
                        f"Extracting {len(frame_indices)} frames...",
                        total=len(frame_indices)
                    )
                    
                    for idx in frame_indices:
                        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                        ret, frame = cap.read()
                        
                        if ret:
                            if resize_factor != 1.0:
                                height, width = frame.shape[:2]
                                new_width = int(width * resize_factor)
                                new_height = int(height * resize_factor)
                                frame = cv2.resize(frame, (new_width, new_height))
                            frames.append(frame)
                        else:
                            logger.warning(f"Failed to read frame {idx} from {video_path}")
                        
                        progress.update(task, advance=1)
            else:
                # No progress bar for small extractions
                for idx in frame_indices:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                    ret, frame = cap.read()
                    
                    if ret:
                        if resize_factor != 1.0:
                            height, width = frame.shape[:2]
                            new_width = int(width * resize_factor)
                            new_height = int(height * resize_factor)
                            frame = cv2.resize(frame, (new_width, new_height))
                        frames.append(frame)
                    else:
                        logger.warning(f"Failed to read frame {idx} from {video_path}")
                    
        finally:
            cap.release()
            
        return frames
    
    def extract_frame_range(
        self,
        video_path: str,
        start_frame: int,
        end_frame: int,
        step: int = 1,
        resize_factor: float = 1.0
    ) -> List[Tuple[int, np.ndarray]]:
        """Extract a range of frames with their indices.
        
        Args:
            video_path: Path to video
            start_frame: Starting frame index
            end_frame: Ending frame index (exclusive)
            step: Frame step size
            resize_factor: Resize factor for frames
            
        Returns:
            List of (frame_index, frame) tuples
        """
        frames = []
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            logger.error(f"Failed to open video: {video_path}")
            return frames
        
        try:
            for idx in range(start_frame, end_frame, step):
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                
                if ret:
                    if resize_factor != 1.0:
                        height, width = frame.shape[:2]
                        new_width = int(width * resize_factor)
                        new_height = int(height * resize_factor)
                        frame = cv2.resize(frame, (new_width, new_height))
                    frames.append((idx, frame))
                else:
                    break
                    
        finally:
            cap.release()
            
        return frames
    
    def extract_audio(
        self, 
        video_path: str, 
        output_path: str,
        sample_rate: int = 16000
    ) -> bool:
        """Extract audio from video to WAV file.
        
        Args:
            video_path: Input video path
            output_path: Output WAV path
            sample_rate: Target sample rate
            
        Returns:
            True if extraction successful
        """
        logger.debug(f"Extracting audio from {video_path}")
        
        try:
            stream = ffmpeg.input(video_path)
            stream = ffmpeg.output(
                stream,
                output_path,
                acodec="pcm_s16le",
                ac=1,  # Mono
                ar=sample_rate,
                loglevel="error"
            )
            ffmpeg.run(stream, overwrite_output=True)
            return True
            
        except ffmpeg.Error as e:
            logger.error(f"Audio extraction failed: {e.stderr.decode()}")
            return False
    
    def create_video_writer(
        self,
        output_path: str,
        width: int,
        height: int,
        fps: float,
        codec: str = "mp4v"
    ) -> cv2.VideoWriter:
        """Create OpenCV video writer.
        
        Args:
            output_path: Output video path
            width: Video width
            height: Video height  
            fps: Frame rate
            codec: Video codec (default mp4v)
            
        Returns:
            VideoWriter object
        """
        fourcc = cv2.VideoWriter_fourcc(*codec)
        writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        if not writer.isOpened():
            raise ValueError(f"Failed to create video writer for {output_path}")
            
        return writer
</file>

<file path="src/vidkompy/__init__.py">
# this_file: src/vidkompy/__init__.py

from .__version__ import __version__

__all__ = ["__version__"]
</file>

<file path="src/vidkompy/__main__.py">
#!/usr/bin/env python
# this_file: src/vidkompy/__main__.py

"""Enable running vidkompy as a module with python -m vidkompy."""

import fire
from vidkompy.vidkompy import main

if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="src/vidkompy/models.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/models.py

"""
Data models and enums for vidkompy.

Contains all shared data structures used across the application.
"""

from dataclasses import dataclass
from enum import Enum
from typing import List, Optional, Tuple


class MatchTimeMode(Enum):
    """Temporal alignment modes."""
    FAST = "fast"      # Try audio first, fallback to frames
    PRECISE = "precise"  # Use frame-based matching


class SpatialMethod(Enum):
    """Spatial alignment methods."""
    TEMPLATE = "template"  # Precise template matching
    FEATURE = "feature"    # Fast feature-based matching
    CENTER = "center"      # Simple center alignment


@dataclass
class VideoInfo:
    """Video metadata container."""
    width: int
    height: int
    fps: float
    duration: float
    frame_count: int
    has_audio: bool
    audio_sample_rate: Optional[int] = None
    audio_channels: Optional[int] = None
    path: str = ""
    
    @property
    def resolution(self) -> Tuple[int, int]:
        """Get video resolution as (width, height)."""
        return (self.width, self.height)
    
    @property
    def aspect_ratio(self) -> float:
        """Calculate aspect ratio."""
        return self.width / self.height if self.height > 0 else 0


@dataclass
class FrameAlignment:
    """Represents alignment between a foreground and background frame."""
    fg_frame_idx: int      # Foreground frame index (never changes)
    bg_frame_idx: int      # Corresponding background frame index
    similarity_score: float  # Similarity between frames (0-1)
    
    def __repr__(self) -> str:
        return f"FrameAlignment(fg={self.fg_frame_idx}, bg={self.bg_frame_idx}, sim={self.similarity_score:.3f})"


@dataclass 
class SpatialAlignment:
    """Spatial offset for overlaying foreground on background."""
    x_offset: int
    y_offset: int
    scale_factor: float = 1.0
    confidence: float = 1.0
    
    @property
    def offset(self) -> Tuple[int, int]:
        """Get offset as tuple."""
        return (self.x_offset, self.y_offset)


@dataclass
class TemporalAlignment:
    """Temporal alignment results."""
    offset_seconds: float  # Time offset in seconds
    frame_alignments: List[FrameAlignment]  # Frame-by-frame mapping
    method_used: str  # Method that produced this alignment
    confidence: float = 1.0
    
    @property
    def start_frame(self) -> Optional[int]:
        """Get first aligned foreground frame."""
        return self.frame_alignments[0].fg_frame_idx if self.frame_alignments else None
    
    @property 
    def end_frame(self) -> Optional[int]:
        """Get last aligned foreground frame."""
        return self.frame_alignments[-1].fg_frame_idx if self.frame_alignments else None


@dataclass
class ProcessingOptions:
    """Options for video processing."""
    time_mode: MatchTimeMode
    space_method: str
    skip_spatial: bool
    trim: bool
    max_keyframes: int = 2000
    verbose: bool = False
</file>

<file path=".cursorrules">
## Coding style

<guidelines>
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

Work in rounds: 

- Create `PROGRESS.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PROGRESS.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py312-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</guidelines>


## Project Overview

vidkompy is an intelligent video overlay tool that overlays foreground videos onto background videos with automatic alignment. The project prioritizes 
precision and intelligence over raw speed.

`vidkompy` is a Python CLI (Fire) tool that lets me overlay two videos: --bg (background) and --fg (foreground), and saves the result to --output (name is built automatically if not provided).

The tool needs to be robust and smart.

1. The bg video can be larger than the fg video. In that case, the tool needs to automatically find the best offset to overlay the fg video on the bg video.

2. The bg and fg videos can have different FPSes. In that case, the tool needs to use the higher FPS for the output video.

3. The bg and fg videos may have different durations (the difference would be typically rather slight). Ths tool needs to find the optimal time offset to overlay the fg video on the bg video. We need to find "the perfect pair of frames that when overlaid will form the START frame of the output video", and "the perfect pair of frames that when overlaid will form the END frame of the output video". The output video will be trimmed to go from that start to that end. Some starting and/or ending frames from either the fg or the bg video may be dropped. 

4. The bg and fg videos may have sound (it would be identical, but may be offset). If both have sound, the sound may be used to find the right time offset for the overlay (if I specify --match_time audio). The fg sound would be is for the output video. If only one has sound, that sound is used for the output video.

5. The content of the two videos would typically be very similar but not identical, because one video is typically a recompressed, edited variant of the other.

6. Overall, the fg video is thought to be "better quality", and temporally properly aligned. The bg video can be streched, modified, re-timed, frames added or dropped etc. 

7. All these conditions may be combined. That means, the tool needs to be able to find both the spatial and the temporal offsets. Spatially, the fg video is never larger than the bg video. Temporally, either video can be longer.



## Key Commands

### Development & Testing
```bash
# Run tests
hatch run test

# Run tests with coverage
hatch run test-cov

# Type checking
hatch run type-check

# Linting and formatting
hatch run lint      # Check code
hatch run fix       # Auto-fix issues
hatch run fmt       # Format code

# All lint checks
hatch run lint:all
```

### Running the Tool
```bash
python src/vidkompy/vidkompy_old.py --help|cat

INFO: Showing help with the command 'vidkompy_old.py -- --help'.

NAME
    vidkompy_old.py - Overlay foreground video onto background video with intelligent alignment.

SYNOPSIS
    vidkompy_old.py BG FG <flags>

DESCRIPTION
    Overlay foreground video onto background video with intelligent alignment.

POSITIONAL ARGUMENTS
    BG
        Type: str
        Background video path
    FG
        Type: str
        Foreground video path

FLAGS
    -o, --output=OUTPUT
        Type: Optional[str | None]
        Default: None
        Output video path (auto-generated if not provided)
    --match_space=MATCH_SPACE
        Type: str
        Default: 'precise'
        Spatial alignment method ('precise' or 'fast')
    --temporal_align=TEMPORAL_ALIGN
        Type: str
        Default: 'frames'
        Temporal alignment method ('audio', 'duration', or 'frames') [default: 'frames']
    --trim=TRIM
        Type: bool
        Default: True
        Trim output to overlapping segments only [default: True]
    -s, --skip_spatial_align=SKIP_SPATIAL_ALIGN
        Type: bool
        Default: False
        Skip spatial alignment (use center)
    -v, --verbose=VERBOSE
        Type: bool
        Default: False
        Enable verbose logging
    --max_keyframes=MAX_KEYFRAMES
        Type: int
        Default: 2000
        Maximum number of keyframes to use in frames mode [default: 2000]

NOTES
    You can also use flags syntax for POSITIONAL ARGUMENTS


# Direct execution (main implementation in vidkompy_old.py)
python -m vidkompy --bg tests/bg.mp4 --fg tests/fg.mp4 -o tests/output_new.mp4 --match_time precise --verbose
```

## Architecture Notes

1. **Main Implementation**: The core functionality resides in `src/vidkompy/vidkompy_old.py` (60KB). 

The `vidkompy.py` file is a placeholder for future refactoring.

2. **Temporal Alignment Modes**:
   - `audio`: Cross-correlation of audio tracks
   - `duration`: Centers foreground within background duration
   - `frames`: Advanced frame-by-frame content matching with dynamic mapping

3. **Spatial Alignment Methods**:
   - `template` (precise, default): OpenCV template matching for exact sub-regions
   - `feature` (fast): ORB feature matching for more robust alignment

4. **Known Issue**: The `frames` temporal alignment method has drift-and-catchup issues where the background portion runs at varying speeds 

5. **Dependencies**: The tool requires FFmpeg binary installed separately. Python dependencies are listed in the script header and include opencv-python, ffmpeg-python, scipy, numpy, soundfile, scikit-image, fire, rich, and loguru.

## Testing Approach

The project uses pytest with minimal test coverage currently (only version check). When adding new functionality:
- Place tests in `tests/` directory
- Use pytest markers for categorization (unit, integration, benchmark)
- Run with `hatch run test` or `hatch run test-cov` for coverage

START SPECIFICATION:
---
description: Create high-level documentation for video processing systems that handle intelligent alignment and overlay of video content with automatic spatial and temporal synchronization
globs: *.py,*.md
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


vidkompy is an intelligent video overlay system that automatically aligns and composites foreground videos onto background videos while preserving content integrity.

Core Business Components:

1. Alignment Engine (90/100)
- Coordinates spatial and temporal video alignment 
- Maintains foreground frame integrity
- Handles mismatched video properties
- Selects optimal alignment methods based on content

2. Temporal Synchronization (85/100)
Three primary methods:
- Audio alignment using cross-correlation
- Frame-based matching with dynamic mapping
- Duration-based centering as fallback
- Preserves all foreground frames without modification

3. Spatial Positioning (80/100)
- Template matching for precise pixel alignment
- Feature matching for robust positioning
- Automatic positioning validation
- Center alignment fallback mechanism

4. Video Composition Rules (75/100)
- Uses higher FPS between inputs
- Preserves foreground timing and quality
- Adapts background content to match foreground
- Intelligent audio source selection

Business Logic Highlights:
- Never modifies foreground content
- Background can be stretched/modified
- Automatic method selection
- Smart fallbacks at each stage
- Focus on precision over performance

$END$
END SPECIFICATION
</file>

<file path="CLAUDE.md">
## Coding style

<guidelines>
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

Work in rounds: 

- Create `PROGRESS.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PROGRESS.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py312-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</guidelines>


## Project Overview

vidkompy is an intelligent video overlay tool that overlays foreground videos onto background videos with automatic alignment. The project prioritizes 
precision and intelligence over raw speed.

`vidkompy` is a Python CLI (Fire) tool that lets me overlay two videos: --bg (background) and --fg (foreground), and saves the result to --output (name is built automatically if not provided).

The tool needs to be robust and smart.

1. The bg video can be larger than the fg video. In that case, the tool needs to automatically find the best offset to overlay the fg video on the bg video.

2. The bg and fg videos can have different FPSes. In that case, the tool needs to use the higher FPS for the output video.

3. The bg and fg videos may have different durations (the difference would be typically rather slight). Ths tool needs to find the optimal time offset to overlay the fg video on the bg video. We need to find "the perfect pair of frames that when overlaid will form the START frame of the output video", and "the perfect pair of frames that when overlaid will form the END frame of the output video". The output video will be trimmed to go from that start to that end. Some starting and/or ending frames from either the fg or the bg video may be dropped. 

4. The bg and fg videos may have sound (it would be identical, but may be offset). If both have sound, the sound may be used to find the right time offset for the overlay (if I specify --match_time audio). The fg sound would be is for the output video. If only one has sound, that sound is used for the output video.

5. The content of the two videos would typically be very similar but not identical, because one video is typically a recompressed, edited variant of the other.

6. Overall, the fg video is thought to be "better quality", and temporally properly aligned. The bg video can be streched, modified, re-timed, frames added or dropped etc. 

7. All these conditions may be combined. That means, the tool needs to be able to find both the spatial and the temporal offsets. Spatially, the fg video is never larger than the bg video. Temporally, either video can be longer.



## Key Commands

### Development & Testing
```bash
# Run tests
hatch run test

# Run tests with coverage
hatch run test-cov

# Type checking
hatch run type-check

# Linting and formatting
hatch run lint      # Check code
hatch run fix       # Auto-fix issues
hatch run fmt       # Format code

# All lint checks
hatch run lint:all
```

### Running the Tool
```bash
python src/vidkompy/vidkompy_old.py --help|cat

INFO: Showing help with the command 'vidkompy_old.py -- --help'.

NAME
    vidkompy_old.py - Overlay foreground video onto background video with intelligent alignment.

SYNOPSIS
    vidkompy_old.py BG FG <flags>

DESCRIPTION
    Overlay foreground video onto background video with intelligent alignment.

POSITIONAL ARGUMENTS
    BG
        Type: str
        Background video path
    FG
        Type: str
        Foreground video path

FLAGS
    -o, --output=OUTPUT
        Type: Optional[str | None]
        Default: None
        Output video path (auto-generated if not provided)
    --match_space=MATCH_SPACE
        Type: str
        Default: 'precise'
        Spatial alignment method ('precise' or 'fast')
    --temporal_align=TEMPORAL_ALIGN
        Type: str
        Default: 'frames'
        Temporal alignment method ('audio', 'duration', or 'frames') [default: 'frames']
    --trim=TRIM
        Type: bool
        Default: True
        Trim output to overlapping segments only [default: True]
    -s, --skip_spatial_align=SKIP_SPATIAL_ALIGN
        Type: bool
        Default: False
        Skip spatial alignment (use center)
    -v, --verbose=VERBOSE
        Type: bool
        Default: False
        Enable verbose logging
    --max_keyframes=MAX_KEYFRAMES
        Type: int
        Default: 2000
        Maximum number of keyframes to use in frames mode [default: 2000]

NOTES
    You can also use flags syntax for POSITIONAL ARGUMENTS


# Direct execution (main implementation in vidkompy_old.py)
python -m vidkompy --bg tests/bg.mp4 --fg tests/fg.mp4 -o tests/output_new.mp4 --match_time precise --verbose
```

## Architecture Notes

1. **Main Implementation**: The core functionality resides in `src/vidkompy/vidkompy_old.py` (60KB). 

The `vidkompy.py` file is a placeholder for future refactoring.

2. **Temporal Alignment Modes**:
   - `audio`: Cross-correlation of audio tracks
   - `duration`: Centers foreground within background duration
   - `frames`: Advanced frame-by-frame content matching with dynamic mapping

3. **Spatial Alignment Methods**:
   - `template` (precise, default): OpenCV template matching for exact sub-regions
   - `feature` (fast): ORB feature matching for more robust alignment

4. **Known Issue**: The `frames` temporal alignment method has drift-and-catchup issues where the background portion runs at varying speeds 

5. **Dependencies**: The tool requires FFmpeg binary installed separately. Python dependencies are listed in the script header and include opencv-python, ffmpeg-python, scipy, numpy, soundfile, scikit-image, fire, rich, and loguru.

## Testing Approach

The project uses pytest with minimal test coverage currently (only version check). When adding new functionality:
- Place tests in `tests/` directory
- Use pytest markers for categorization (unit, integration, benchmark)
- Run with `hatch run test` or `hatch run test-cov` for coverage
</file>

<file path="PROGRESS.md">
# Progress: vidkompy Refactoring

## Round 1: Analysis and Architecture Design

### Phase 1: Understanding Current Implementation

- [x] Analyze vidkompy_old.py structure and functionality
- [x] Document key functions and their purposes
- [x] Identify pain points and areas for improvement
- [x] Map out dependencies and data flow

### Phase 2: Modular Architecture Design

- [x] Design module structure (core, alignment, temporal, spatial, cli)
- [x] Define interfaces between modules
- [x] Plan data structures for video metadata and alignment results
- [x] Create module dependency diagram

### Phase 3: Core Module Implementation

- [x] Create vidkompy.py as main entry point
- [x] Implement video_processor.py for core video operations
- [x] Create alignment_engine.py for coordination
- [x] Implement metadata extraction and validation

## Round 2: Alignment Module Implementation

### Phase 4: Spatial Alignment

- [x] Extract spatial alignment logic from old code
- [x] Create spatial_alignment.py module
- [x] Implement template matching method
- [x] Implement feature matching method
- [x] Add center alignment fallback

### Phase 5: Temporal Alignment - Audio

- [x] Create temporal_alignment.py module
- [x] Implement audio extraction and analysis
- [x] Implement cross-correlation for audio sync
- [x] Add fallback mechanisms

### Phase 6: Temporal Alignment - Frames (Complete Rewrite)

- [x] Analyze current frames method issues
- [x] Design new algorithm preserving all fg frames
- [x] Implement frame similarity metrics
- [x] Create optimal bg-to-fg frame mapping
- [x] Ensure no fg frame retiming occurs
- [x] Test with various frame rate combinations

## Round 3: CLI and Integration

### Phase 7: CLI Improvements

- [ ] Update CLI arguments (--match_time fast/precise)
- [ ] Implement argument validation
- [ ] Add better progress reporting with rich
- [ ] Enhance verbose logging with loguru

### Phase 8: Testing and Validation

- [ ] Test with provided bg.mp4 and fg.mp4
- [ ] Verify spatial alignment accuracy
- [ ] Validate temporal alignment (all methods)
- [ ] Check output quality and frame preservation
- [ ] Performance benchmarking

### Phase 9: Documentation and Cleanup

- [ ] Update CHANGELOG.md with all changes
- [ ] Update README.md with new usage
- [ ] Add inline documentation
- [ ] Clean up old code references
- [ ] Final code formatting and linting

## Key Principles for Implementation

1. Foreground video is never retimed - all frames preserved
2. Background frames are mapped to foreground frames optimally
3. Modular design with clear separation of concerns
4. Robust error handling and fallbacks
5. Comprehensive logging for debugging
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(fd:*)",
      "Bash(chmod:*)",
      "Bash(python:*)",
      "Bash(ls:*)",
      "Bash(ffprobe:*)",
      "Bash(cp:*)",
      "Bash(rm:*)",
      "Bash(mkdir:*)"
    ],
    "deny": []
  }
}
</file>

<file path=".github/workflows/push.yml">
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/vidkompo --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/vidkompo
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path="src/vidkompy/__version__.py">
# file generated by setuptools-scm
# don't change, don't track in version control

__all__ = ["__version__", "__version_tuple__", "version", "version_tuple"]

TYPE_CHECKING = False
if TYPE_CHECKING:
    from typing import Tuple
    from typing import Union

    VERSION_TUPLE = Tuple[Union[int, str], ...]
else:
    VERSION_TUPLE = object

version: str
__version__: str
__version_tuple__: VERSION_TUPLE
version_tuple: VERSION_TUPLE

__version__ = version = '0.0.post2+gec710f3.d20250524'
__version_tuple__ = version_tuple = (0, 0, 'post2', 'gec710f3.d20250524')
</file>

<file path="src/vidkompy/vidkompy_old.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "rich", "loguru", "opencv-python", "numpy", "scipy", "ffmpeg-python", "soundfile", "scikit-image"]
# ///
# this_file: vidoverlay.py

"""
Video overlay tool with intelligent spatial and temporal alignment.

Overlays a foreground video onto a background video with automatic:
- Spatial alignment using template matching and feature detection
- Temporal alignment using audio cross-correlation
- Frame rate harmonization
- Duration management
"""

import sys
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import Tuple, List
from enum import Enum

import cv2
import ffmpeg
import fire
import numpy as np
import soundfile as sf
from loguru import logger
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from scipy import signal
from skimage.metrics import structural_similarity as ssim

console = Console()


class AlignmentMode(Enum):
    """Temporal alignment methods."""

    AUDIO = "audio"
    DURATION = "duration"
    FRAMES = "frames"


@dataclass
class VideoInfo:
    """Video metadata container."""

    width: int
    height: int
    fps: float
    duration: float
    has_audio: bool
    audio_sample_rate: int | None = None
    audio_channels: int | None = None
    path: str = ""
    frame_count: int = 0


@dataclass
class FrameAlignment:
    """Frame-to-frame alignment mapping."""

    bg_frame_idx: int
    fg_frame_idx: int
    similarity_score: float
    temporal_offset: float = 0.0


class VideoOverlay:
    """Main video overlay processor."""

    def __init__(self, verbose: bool = False):
        """Initialize the video overlay processor.

        Args:
            verbose: Enable verbose logging
        """
        self.verbose = verbose
        self._setup_logging()

    def _setup_logging(self):
        """Configure loguru logging."""
        logger.remove()  # Remove default handler

        if self.verbose:
            logger.add(
                sys.stderr,
                format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan> - <level>{message}</level>",
                level="DEBUG",
            )
        else:
            logger.add(
                sys.stderr,
                format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
                level="INFO",
            )

    def get_video_info(self, video_path: str) -> VideoInfo:
        """Extract video metadata using ffprobe.

        Args:
            video_path: Path to video file

        Returns:
            VideoInfo object with metadata
        """
        logger.debug(f"Probing video: {video_path}")

        try:
            # Get video stream info
            probe = ffmpeg.probe(video_path)

            video_stream = next(
                (s for s in probe["streams"] if s["codec_type"] == "video"), None
            )

            if not video_stream:
                raise ValueError(f"No video stream found in {video_path}")

            # Extract video properties
            width = int(video_stream["width"])
            height = int(video_stream["height"])

            # Parse frame rate (can be rational like "24000/1001")
            fps_str = video_stream.get("r_frame_rate", "0/1")
            if "/" in fps_str:
                num, den = map(int, fps_str.split("/"))
                fps = num / den if den != 0 else 0
            else:
                fps = float(fps_str)

            # Get duration
            duration = float(probe["format"].get("duration", 0))

            # Calculate frame count
            frame_count = int(video_stream.get("nb_frames", 0))
            if frame_count == 0 and duration > 0 and fps > 0:
                frame_count = int(duration * fps)

            # Check for audio
            audio_stream = next(
                (s for s in probe["streams"] if s["codec_type"] == "audio"), None
            )

            has_audio = audio_stream is not None
            audio_sample_rate = None
            audio_channels = None

            if has_audio and audio_stream:
                audio_sample_rate = int(audio_stream.get("sample_rate", 0))
                audio_channels = int(audio_stream.get("channels", 0))

            info = VideoInfo(
                width=width,
                height=height,
                fps=fps,
                duration=duration,
                has_audio=has_audio,
                audio_sample_rate=audio_sample_rate,
                audio_channels=audio_channels,
                path=video_path,
                frame_count=frame_count,
            )

            logger.info(
                f"Video info for {Path(video_path).name}: "
                f"{width}x{height}, {fps:.2f} fps, {duration:.2f}s, "
                f"{frame_count} frames, audio: {'yes' if has_audio else 'no'}"
            )

            return info

        except Exception as e:
            logger.error(f"Failed to probe video {video_path}: {e}")
            raise

    def extract_audio(
        self, video_path: str, output_path: str, sample_rate: int = 16000
    ) -> bool:
        """Extract audio from video to WAV file.

        Args:
            video_path: Input video path
            output_path: Output WAV path
            sample_rate: Target sample rate for audio

        Returns:
            True if extraction successful, False otherwise
        """
        logger.debug(f"Extracting audio from {video_path} to {output_path}")

        try:
            stream = ffmpeg.input(video_path)
            stream = ffmpeg.output(
                stream,
                output_path,
                acodec="pcm_s16le",
                ac=1,  # Convert to mono
                ar=sample_rate,
                loglevel="error",
            )
            ffmpeg.run(stream, overwrite_output=True)
            return True

        except ffmpeg.Error as e:
            logger.error(f"Audio extraction failed: {e.stderr.decode()}")
            return False

    def compute_audio_offset(self, bg_audio_path: str, fg_audio_path: str) -> float:
        """Compute temporal offset using audio cross-correlation.

        Args:
            bg_audio_path: Background audio WAV file
            fg_audio_path: Foreground audio WAV file

        Returns:
            Offset in seconds (positive means FG starts later)
        """
        logger.debug("Computing audio cross-correlation")

        # Load audio files
        bg_audio, bg_sr = sf.read(bg_audio_path)
        fg_audio, fg_sr = sf.read(fg_audio_path)

        if bg_sr != fg_sr:
            logger.warning(f"Sample rates don't match: {bg_sr} vs {fg_sr}")
            return 0.0

        # Compute cross-correlation
        correlation = signal.correlate(bg_audio, fg_audio, mode="full", method="fft")

        # Find peak
        peak_idx = np.argmax(np.abs(correlation))

        # Convert to time offset
        # Center of correlation is at len(bg_audio) - 1
        center = len(bg_audio) - 1
        lag_samples = peak_idx - center
        offset_seconds = lag_samples / bg_sr

        # Get correlation strength
        peak_value = np.abs(correlation[peak_idx])
        avg_value = np.mean(np.abs(correlation))
        confidence = peak_value / avg_value if avg_value > 0 else 0

        logger.info(
            f"Audio alignment: offset={offset_seconds:.3f}s, "
            f"confidence={confidence:.2f}"
        )

        return offset_seconds

    def compute_spatial_offset(
        self, bg_frame: np.ndarray, fg_frame: np.ndarray, method: str = "precise"
    ) -> tuple[int, int]:
        """Compute spatial offset for overlay positioning.

        Args:
            bg_frame: Background video frame
            fg_frame: Foreground video frame
            method: Alignment method ('precise' or 'fast')

        Returns:
            (x, y) offset for foreground placement
        """
        bg_h, bg_w = bg_frame.shape[:2]
        fg_h, fg_w = fg_frame.shape[:2]

        # If foreground is larger, we'll need to scale it down
        if fg_w > bg_w or fg_h > bg_h:
            logger.warning("Foreground larger than background, will scale down")
            return 0, 0

        if method == "precise":
            return self._template_matching(bg_frame, fg_frame)
        elif method == "fast":
            return self._feature_matching(bg_frame, fg_frame)
        else:
            # Default to center
            return (bg_w - fg_w) // 2, (bg_h - fg_h) // 2

    def _template_matching(
        self, bg_frame: np.ndarray, fg_frame: np.ndarray
    ) -> tuple[int, int]:
        """Find best position using template matching.

        Args:
            bg_frame: Background frame
            fg_frame: Foreground frame (template)

        Returns:
            (x, y) position with best match
        """
        logger.debug("Using template matching for spatial alignment")

        # Convert to grayscale for matching
        bg_gray = cv2.cvtColor(bg_frame, cv2.COLOR_BGR2GRAY)
        fg_gray = cv2.cvtColor(fg_frame, cv2.COLOR_BGR2GRAY)

        # Apply template matching
        result = cv2.matchTemplate(bg_gray, fg_gray, cv2.TM_CCOEFF_NORMED)

        # Find best match location
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)

        logger.info(f"Template match confidence: {max_val:.3f}")

        # If confidence is too low, fall back to center
        if max_val < 0.7:
            logger.warning("Low template match confidence, defaulting to center")
            bg_h, bg_w = bg_frame.shape[:2]
            fg_h, fg_w = fg_frame.shape[:2]
            return (bg_w - fg_w) // 2, (bg_h - fg_h) // 2

        return max_loc

    def _feature_matching(
        self, bg_frame: np.ndarray, fg_frame: np.ndarray
    ) -> tuple[int, int]:
        """Find alignment using feature matching.

        Args:
            bg_frame: Background frame
            fg_frame: Foreground frame

        Returns:
            (x, y) offset based on matched features
        """
        logger.debug("Using feature matching for spatial alignment")

        # Convert to grayscale
        bg_gray = cv2.cvtColor(bg_frame, cv2.COLOR_BGR2GRAY)
        fg_gray = cv2.cvtColor(fg_frame, cv2.COLOR_BGR2GRAY)

        # Create ORB detector
        orb = cv2.ORB_create(nfeatures=1000)

        # Find keypoints and descriptors
        kp1, des1 = orb.detectAndCompute(bg_gray, None)
        kp2, des2 = orb.detectAndCompute(fg_gray, None)

        if des1 is None or des2 is None:
            logger.warning("No features found, defaulting to center")
            bg_h, bg_w = bg_frame.shape[:2]
            fg_h, fg_w = fg_frame.shape[:2]
            return (bg_w - fg_w) // 2, (bg_h - fg_h) // 2

        # Match features
        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        matches = bf.match(des1, des2)
        matches = sorted(matches, key=lambda x: x.distance)

        if len(matches) < 10:
            logger.warning("Too few feature matches, defaulting to center")
            bg_h, bg_w = bg_frame.shape[:2]
            fg_h, fg_w = fg_frame.shape[:2]
            return (bg_w - fg_w) // 2, (bg_h - fg_h) // 2

        # Calculate average offset from good matches
        offsets = []
        for match in matches[:20]:  # Use top 20 matches
            pt1 = kp1[match.queryIdx].pt
            pt2 = kp2[match.trainIdx].pt
            offset = (int(pt1[0] - pt2[0]), int(pt1[1] - pt2[1]))
            offsets.append(offset)

        # Use median offset for robustness
        x_offset = int(np.median([o[0] for o in offsets]))
        y_offset = int(np.median([o[1] for o in offsets]))

        logger.info(f"Feature matching found offset: ({x_offset}, {y_offset})")

        return x_offset, y_offset

    def get_frame_at_time(self, video_path: str, time_seconds: float) -> np.ndarray:
        """Extract a single frame from video at specified time.

        Args:
            video_path: Path to video file
            time_seconds: Time in seconds to extract frame

        Returns:
            Frame as numpy array
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise IOError(f"Cannot open video file: {video_path}")

        # Seek to time
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps == 0:  # Handle case where FPS might be 0
            logger.warning(
                f"Video {video_path} has FPS of 0. Cannot seek by time accurately."
            )
            frame_number = 0  # Default to first frame or handle error
        else:
            frame_number = int(fps * time_seconds)

        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)

        # Read frame
        ret, frame = cap.read()
        cap.release()

        if not ret:
            logger.warning(
                f"Could not read frame at time {time_seconds}s from {video_path}"
            )
            raise IOError(f"Could not read frame at {time_seconds}s from {video_path}")

        return frame

    def extract_frames_sample(
        self, video_path: str, video_fps: float, target_sample_fps: float = 2.0
    ) -> List[np.ndarray]:
        """Extract sample frames from video for analysis.

        Args:
            video_path: Path to video file
            video_fps: Original FPS of the video
            target_sample_fps: Target number of frames to sample per second

        Returns:
            List of frames as numpy arrays
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise IOError(f"Cannot open video file for frame sampling: {video_path}")

        frames = []
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / video_fps if video_fps > 0 else 0

        # Calculate which frames to capture based on target_sample_fps
        if video_fps <= 0:  # Avoid division by zero
            logger.warning(
                f"Video {video_path} has invalid FPS ({video_fps}), cannot sample frames effectively."
            )
            cap.release()
            return []

        capture_interval = int(video_fps / target_sample_fps)
        if capture_interval == 0:  # if target_sample_fps is higher than video_fps
            capture_interval = 1

        # Calculate frames to extract
        frames_to_extract = []
        frame_idx = 0
        while frame_idx < total_frames:
            if frame_idx % capture_interval == 0:
                frames_to_extract.append(frame_idx)
            frame_idx += 1

        logger.info(
            f"Extracting {len(frames_to_extract)} sample frames from {Path(video_path).name} "
            f"(video: {total_frames} frames @ {video_fps:.2f} fps, duration: {duration:.2f}s)"
        )

        # Extract frames using seeking for better performance
        for i, frame_idx in enumerate(frames_to_extract):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
            else:
                logger.debug(
                    f"Failed to read frame {frame_idx} from {Path(video_path).name}"
                )

            if i % 10 == 0:
                logger.debug(f"Extracted {i}/{len(frames_to_extract)} sample frames...")

        cap.release()
        logger.info(
            f"Successfully extracted {len(frames)} sample frames from {Path(video_path).name} "
            f"(sampled at approx {target_sample_fps} FPS)"
        )
        return frames

    def compute_frame_similarity(self, frame1: np.ndarray, frame2: np.ndarray) -> float:
        """Compute similarity between two frames using SSIM.

        Args:
            frame1: First frame
            frame2: Second frame

        Returns:
            Similarity score (0-1)
        """
        # Resize frames to same size if needed
        if frame1.shape != frame2.shape:
            h = min(frame1.shape[0], frame2.shape[0])
            w = min(frame1.shape[1], frame2.shape[1])
            frame1 = cv2.resize(frame1, (w, h))
            frame2 = cv2.resize(frame2, (w, h))

        # Convert to grayscale for SSIM
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)

        # Compute SSIM
        score, _ = ssim(gray1, gray2, full=True)
        return score

    def find_keyframe_matches(
        self,
        bg_frames_sampled: List[np.ndarray],  # Renamed for clarity
        fg_frames_sampled: List[np.ndarray],  # Renamed for clarity
        bg_info: VideoInfo,  # Added for original FPS access
        fg_info: VideoInfo,  # Added for original FPS access
        target_sample_fps: float,  # Added to know sample interval
        threshold: float = 0.6,  # Adjusted threshold slightly
    ) -> List[Tuple[int, int, float]]:  # Returns original frame indices
        """Find matching keyframes between two videos.

        Args:
            bg_frames_sampled: Background video sampled frames
            fg_frames_sampled: Foreground video sampled frames
            bg_info: Background video metadata
            fg_info: Foreground video metadata
            target_sample_fps: The rate at which frames were sampled
            threshold: Minimum similarity threshold

        Returns:
            List of (original_bg_idx, original_fg_idx, similarity) tuples
        """
        logger.info("Finding keyframe matches between sampled video frames")
        matches = []

        bg_capture_interval = (
            int(bg_info.fps / target_sample_fps)
            if target_sample_fps > 0 and bg_info.fps > 0
            else 1
        )
        fg_capture_interval = (
            int(fg_info.fps / target_sample_fps)
            if target_sample_fps > 0 and fg_info.fps > 0
            else 1
        )
        if bg_capture_interval == 0:
            bg_capture_interval = 1
        if fg_capture_interval == 0:
            fg_capture_interval = 1

        # Search a wider window in background frames for each foreground frame
        for fg_sample_idx, fg_frame in enumerate(fg_frames_sampled):
            best_match_for_fg_sample = (-1, 0.0)  # (bg_sample_idx, similarity)

            # Search a large portion of bg_frames_sampled, e.g., +/- 25% of total bg_frames_sampled length
            # or even all of them if performance allows and initial sync is very off.
            # For now, let's try a large window or full scan for robustness.
            # Consider a window that adapts to the expected position.

            # Estimate expected position of fg_sample_idx in bg_frames_sampled
            # This rough estimate helps to center the search window if not scanning fully.
            proportional_idx = int(
                fg_sample_idx * len(bg_frames_sampled) / len(fg_frames_sampled)
            )

            # Define a search window. Example: +/- 10 sampled frames, or a percentage of total.
            # A large window like 30-50% of bg_frames_sampled around proportional_idx.
            # Or even full scan if not too many samples.
            window_radius_samples = (
                len(bg_frames_sampled) // 2
            )  # Search a very large window

            start_idx = max(0, proportional_idx - window_radius_samples)
            end_idx = min(
                len(bg_frames_sampled), proportional_idx + window_radius_samples + 1
            )

            # If the number of sampled frames is small, just search all of them.
            if len(bg_frames_sampled) < 50:  # Heuristic value
                start_idx = 0
                end_idx = len(bg_frames_sampled)

            for bg_sample_idx in range(start_idx, end_idx):
                similarity = self.compute_frame_similarity(
                    bg_frames_sampled[bg_sample_idx], fg_frame
                )
                if similarity > best_match_for_fg_sample[1]:
                    best_match_for_fg_sample = (bg_sample_idx, similarity)

            if best_match_for_fg_sample[1] >= threshold:
                original_bg_idx = best_match_for_fg_sample[0] * bg_capture_interval
                original_fg_idx = fg_sample_idx * fg_capture_interval
                matches.append(
                    (original_bg_idx, original_fg_idx, best_match_for_fg_sample[1])
                )
                logger.debug(
                    f"Keyframe match: BG_sample[{best_match_for_fg_sample[0]}] (orig:{original_bg_idx}) <-> FG_sample[{fg_sample_idx}] (orig:{original_fg_idx}) (similarity: {best_match_for_fg_sample[1]:.3f})"
                )

        # Sort matches by foreground frame index
        matches.sort(key=lambda x: x[1])

        # Filter out non-monotonic matches in background frames (optional, but can help)
        # If BG frame index for a later FG frame is earlier than for a previous FG frame, it's problematic.
        filtered_matches = []
        last_bg_idx = -1
        for bg_idx, fg_idx, sim in matches:
            if bg_idx >= last_bg_idx:
                filtered_matches.append((bg_idx, fg_idx, sim))
                last_bg_idx = bg_idx
            else:
                logger.debug(
                    f"Skipping non-monotonic match: BG[{bg_idx}] for FG[{fg_idx}] (last BG was {last_bg_idx})"
                )

        logger.info(
            f"Found {len(filtered_matches)} monotonic keyframe matches out of {len(matches)} total matches"
        )

        # Log match details for debugging
        if filtered_matches and self.verbose:
            logger.debug("Keyframe match details:")
            for i, (bg_idx, fg_idx, sim) in enumerate(
                filtered_matches[:5]
            ):  # Show first 5
                bg_time = bg_idx / bg_info.fps if bg_info.fps > 0 else 0
                fg_time = fg_idx / fg_info.fps if fg_info.fps > 0 else 0
                logger.debug(
                    f"  Match {i + 1}: BG[{bg_idx}]@{bg_time:.2f}s <-> FG[{fg_idx}]@{fg_time:.2f}s (sim={sim:.3f})"
                )
            if len(filtered_matches) > 5:
                logger.debug(f"  ... and {len(filtered_matches) - 5} more matches")

        return filtered_matches

    def compute_frame_alignment(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        keyframe_matches: List[Tuple[int, int, float]],  # Original frame indices
        trim: bool = False,  # Added trim argument
    ) -> Tuple[
        List[FrameAlignment], float
    ]:  # Returns list of alignments and an overall initial offset
        """Compute frame-by-frame alignment based on keyframe matches.
           This version creates a direct map from each FG frame to a BG frame.

        Args:
            bg_info: Background video info
            fg_info: Foreground video info
            keyframe_matches: List of (original_bg_idx, original_fg_idx, similarity) tuples
            trim: If true, only align frames within the span of keyframe_matches.

        Returns:
            (frame_alignments_list, initial_temporal_offset)
            frame_alignments_list: a list where each element is FrameAlignment(bg_frame_idx, fg_frame_idx, similarity_score, 0)
            initial_temporal_offset: The temporal offset of the first matched FG frame relative to BG start.
        """
        logger.info("Computing frame-by-frame alignment map from keyframe_matches")
        alignments = []

        if not keyframe_matches:
            logger.warning("No keyframe matches found. Cannot compute frame alignment.")
            return [], 0.0

        first_match_bg_time = keyframe_matches[0][0] / bg_info.fps
        first_match_fg_time = keyframe_matches[0][1] / fg_info.fps
        initial_temporal_offset = first_match_bg_time - first_match_fg_time
        logger.info(
            f"Initial temporal offset from first keyframe match: {initial_temporal_offset:.3f}s"
        )

        fg_start_frame_for_alignment = 0
        fg_end_frame_for_alignment = fg_info.frame_count

        if trim:
            fg_start_frame_for_alignment = keyframe_matches[0][
                1
            ]  # original_fg_idx of first match
            fg_end_frame_for_alignment = (
                keyframe_matches[-1][1] + 1
            )  # original_fg_idx of last match (exclusive end)
            logger.info(
                f"Trim active for frame alignment: processing FG frames from {fg_start_frame_for_alignment} to {fg_end_frame_for_alignment - 1}"
            )

        current_keyframe_idx = 0
        # Ensure current_keyframe_idx starts at the first keyframe relevant to fg_start_frame_for_alignment
        while (
            current_keyframe_idx < len(keyframe_matches)
            and keyframe_matches[current_keyframe_idx][1] < fg_start_frame_for_alignment
        ):
            current_keyframe_idx += 1
        if (
            current_keyframe_idx > 0
            and keyframe_matches[current_keyframe_idx - 1][1]
            < fg_start_frame_for_alignment
        ):
            # This means fg_start_frame_for_alignment is between keyframe_matches[current_keyframe_idx-1] and keyframe_matches[current_keyframe_idx]
            pass  # current_keyframe_idx is correctly pointing to the *next* keyframe or is at 0

        for fg_frame_num in range(
            fg_start_frame_for_alignment, fg_end_frame_for_alignment
        ):
            kf_prev = None
            kf_next = None

            # Advance current_keyframe_idx to find the segment for fg_frame_num
            # We need keyframe_matches[idx-1].fg_idx <= fg_frame_num <= keyframe_matches[idx].fg_idx
            temp_search_idx = current_keyframe_idx
            while (
                temp_search_idx < len(keyframe_matches)
                and keyframe_matches[temp_search_idx][1] < fg_frame_num
            ):
                temp_search_idx += 1

            # After loop, keyframe_matches[temp_search_idx] is the first one with .fg_idx >= fg_frame_num
            # Or temp_search_idx is len(keyframe_matches)

            if temp_search_idx == 0:  # fg_frame_num is before or at the first keyframe
                kf_prev = keyframe_matches[0]
                kf_next = keyframe_matches[0]
            elif temp_search_idx == len(
                keyframe_matches
            ):  # fg_frame_num is after or at the last keyframe
                kf_prev = keyframe_matches[-1]
                kf_next = keyframe_matches[-1]
            else:  # fg_frame_num is between two keyframes or at keyframe_matches[temp_search_idx]
                kf_prev = keyframe_matches[temp_search_idx - 1]
                kf_next = keyframe_matches[temp_search_idx]

            # Update current_keyframe_idx for next iteration's starting point (small optimization)
            current_keyframe_idx = max(
                0, temp_search_idx - 1
            )  # Start search for next fg_frame_num near here

            fg_prev_idx, bg_prev_idx, sim_prev = kf_prev[1], kf_prev[0], kf_prev[2]
            fg_next_idx, bg_next_idx, sim_next = kf_next[1], kf_next[0], kf_next[2]

            mapped_bg_frame_num = 0
            current_similarity = 0.5

            if fg_next_idx == fg_prev_idx:
                # At a keyframe or extrapolating from a single keyframe (start/end of sequence)
                # Calculate bg_frame_num by maintaining relative distance from the keyframe
                # Use the average frame rate ratio for better extrapolation
                fps_ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0
                mapped_bg_frame_num = int(
                    bg_prev_idx + (fg_frame_num - fg_prev_idx) * fps_ratio
                )
                current_similarity = sim_prev
            else:
                # Interpolate between keyframes
                # Use a smoothed interpolation that considers the frame rate differences
                ratio = (fg_frame_num - fg_prev_idx) / (fg_next_idx - fg_prev_idx)

                # Apply a smoothing function to the ratio to reduce abrupt changes
                # Using a cubic smoothing function for more natural transitions
                smooth_ratio = (
                    ratio * ratio * (3.0 - 2.0 * ratio)
                )  # Smoothstep function

                mapped_bg_frame_num = int(
                    bg_prev_idx + smooth_ratio * (bg_next_idx - bg_prev_idx)
                )
                current_similarity = sim_prev + smooth_ratio * (sim_next - sim_prev)

            mapped_bg_frame_num = max(
                0, min(mapped_bg_frame_num, bg_info.frame_count - 1)
            )

            alignments.append(
                FrameAlignment(
                    bg_frame_idx=mapped_bg_frame_num,
                    fg_frame_idx=fg_frame_num,
                    similarity_score=current_similarity,
                    temporal_offset=0,
                )
            )

        logger.info(
            f"Generated frame-to-frame alignment map for {len(alignments)} FG frames."
        )

        # Log some statistics about the alignment
        if alignments and self.verbose:
            # Calculate average frame step ratio
            bg_steps = []
            fg_steps = []
            for i in range(1, min(100, len(alignments))):  # Sample first 100 frames
                bg_steps.append(
                    alignments[i].bg_frame_idx - alignments[i - 1].bg_frame_idx
                )
                fg_steps.append(
                    alignments[i].fg_frame_idx - alignments[i - 1].fg_frame_idx
                )

            if bg_steps and fg_steps:
                avg_bg_step = sum(bg_steps) / len(bg_steps)
                avg_fg_step = sum(fg_steps) / len(fg_steps)
                step_ratio = avg_bg_step / avg_fg_step if avg_fg_step > 0 else 0
                logger.debug(f"Average frame step ratio (BG/FG): {step_ratio:.3f}")
                logger.debug(
                    f"Expected FPS ratio (BG/FG): {bg_info.fps / fg_info.fps if fg_info.fps > 0 else 0:.3f}"
                )

        return alignments, initial_temporal_offset

    def compose_videos(
        self,
        bg_path: str,
        fg_path: str,
        output_path: str,
        spatial_offset: tuple[int, int],
        temporal_offset: float,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        use_bg_audio: bool = True,
        frame_alignments: List[FrameAlignment] | None = None,
        trim: bool = False,
    ):
        """Compose the final video using FFmpeg.

        Args:
            bg_path: Background video path
            fg_path: Foreground video path
            output_path: Output video path
            spatial_offset: (x, y) position for overlay
            temporal_offset: Time offset in seconds
            bg_info: Background video metadata
            fg_info: Foreground video metadata
            use_bg_audio: Whether to use background audio
            frame_alignments: Optional frame-by-frame alignment data
            trim: Whether to trim output to overlapping segments only
        """
        logger.info("Composing final video")

        # Determine output frame rate (use higher)
        output_fps = max(bg_info.fps, fg_info.fps)

        # Handle trimming if requested
        trim_start = 0.0
        trim_duration = None

        if trim and temporal_offset > 0:
            # Trim beginning of background to start where foreground starts
            trim_start = temporal_offset
            trim_duration = min(bg_info.duration - temporal_offset, fg_info.duration)
            logger.info(
                f"Trimming output: start={trim_start:.3f}s, duration={trim_duration:.3f}s"
            )

        # Build FFmpeg command
        if trim and trim_start > 0:
            inputs = [ffmpeg.input(bg_path, ss=trim_start)]
        else:
            inputs = [ffmpeg.input(bg_path)]

        # Add foreground with time offset if needed
        if temporal_offset > 0 and not trim:
            fg_input = ffmpeg.input(fg_path, itsoffset=temporal_offset)
        else:
            fg_input = ffmpeg.input(fg_path)
        inputs.append(fg_input)

        # Build filter graph
        bg_video = inputs[0]["v"]
        fg_video = inputs[1]["v"]

        # Scale foreground if needed
        if fg_info.width > bg_info.width or fg_info.height > bg_info.height:
            # Scale to fit within background
            scale_factor = min(
                bg_info.width / fg_info.width, bg_info.height / fg_info.height
            )
            new_width = int(fg_info.width * scale_factor)
            new_height = int(fg_info.height * scale_factor)

            fg_video = fg_video.filter("scale", new_width, new_height)
            logger.info(f"Scaling foreground to {new_width}x{new_height}")

        # Apply overlay
        x, y = spatial_offset
        video = ffmpeg.filter(
            [bg_video, fg_video],
            "overlay",
            x=x,
            y=y,
            eof_action="pass",  # Continue background after foreground ends
        )

        # Handle audio
        if bg_info.has_audio and use_bg_audio:
            audio = inputs[0]["a"]
        elif fg_info.has_audio and not use_bg_audio:
            audio = inputs[1]["a"]
        else:
            audio = None

        # Build output
        output_args = {
            "c:v": "libx264",
            "preset": "medium",
            "crf": 18,
            "r": output_fps,
            "pix_fmt": "yuv420p",
        }

        if trim and trim_duration:
            output_args["t"] = trim_duration

        if audio is not None:
            output_args["c:a"] = "aac"
            output_args["b:a"] = "192k"
            output = ffmpeg.output(video, audio, output_path, **output_args)
        else:
            output = ffmpeg.output(video, output_path, **output_args)

        # Run FFmpeg
        try:
            ffmpeg.run(output, overwrite_output=True, capture_stderr=True)
            logger.info(f"Video saved to: {output_path}")
        except ffmpeg.Error as e:
            logger.error(f"FFmpeg error: {e.stderr.decode()}")
            raise

    def _merge_audio_with_ffmpeg(
        self,
        video_path_no_audio: str,
        audio_source_path: str,
        output_path: str,
        audio_offset: float = 0.0,  # Offset for the audio stream
        video_duration: float | None = None,
        target_fps: float | None = None,
    ):
        """Merge audio from audio_source_path into video_path_no_audio using FFmpeg."""
        logger.info(
            f"Merging audio from {Path(audio_source_path).name} into video, saving to {output_path}"
        )

        input_video = ffmpeg.input(video_path_no_audio)
        # input_audio = ffmpeg.input(audio_source_path)
        # Redefine input_audio with itsoffset for clarity and correctness
        effective_audio_input_args = {}
        if audio_offset != 0:
            logger.debug(
                f"Applying audio offset of {audio_offset:.3f}s to {Path(audio_source_path).name}"
            )
            effective_audio_input_args["itsoffset"] = str(audio_offset)

        input_audio = ffmpeg.input(audio_source_path, **effective_audio_input_args)

        video_stream = input_video["v"]
        audio_stream = input_audio["a"]

        output_args = {
            "c:v": "copy",  # Copy video stream as is
            "c:a": "aac",
            "b:a": "192k",
            "shortest": None,
        }
        if video_duration:
            output_args["t"] = str(video_duration)  # Ensure it's a string for ffmpeg
        # target_fps is for video stream, but we copy video, so not directly used here.

        try:
            output_path_str = str(output_path)
            stream = ffmpeg.output(
                video_stream, audio_stream, output_path_str, **output_args
            )
            ffmpeg.run(stream, overwrite_output=True, capture_stderr=True)
            logger.info(f"Video with merged audio saved to: {output_path_str}")
        except ffmpeg.Error as e:
            logger.error(f"FFmpeg audio merge error: {e.stderr.decode()}")
            try:
                Path(video_path_no_audio).rename(output_path_str)
                logger.warning(
                    f"Audio merge failed. Saved video without audio to {output_path_str}"
                )
            except Exception as ren_err:
                logger.error(
                    f"Failed to rename video after audio merge failure: {ren_err}"
                )
            raise

    def compose_videos_opencv(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        output_path_str: str,  # Final output path
        spatial_offset: tuple[int, int],
        frame_alignments: List[FrameAlignment],
        trim: bool = False,
    ):
        """Compose final video using OpenCV for frame-by-frame overlay based on alignments.
           The resulting video will be silent. Audio needs to be merged separately.
        Args:
            bg_info: Background video metadata.
            fg_info: Foreground video metadata.
            output_path_str: Path to save the silent composited video.
            spatial_offset: (x,y) for foreground placement.
            frame_alignments: List of FrameAlignment objects mapping fg to bg frames.
            trim: Trim output to overlapping segments.
        Returns:
            True if composition successful, False otherwise.
        """
        logger.info(f"Composing video with OpenCV, outputting to {output_path_str}")

        if not frame_alignments:
            logger.error(
                "No frame alignments provided for OpenCV composition. Aborting."
            )
            return False

        output_fps = fg_info.fps  # Output FPS driven by foreground video's rate
        output_width = bg_info.width
        output_height = bg_info.height

        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        video_writer = cv2.VideoWriter(
            output_path_str, fourcc, output_fps, (output_width, output_height)
        )

        if not video_writer.isOpened():
            logger.error(f"Failed to open VideoWriter for {output_path_str}")
            return False

        cap_bg = cv2.VideoCapture(bg_info.path)
        cap_fg = cv2.VideoCapture(fg_info.path)

        if not cap_bg.isOpened() or not cap_fg.isOpened():
            logger.error(
                "Failed to open background or foreground video for OpenCV composition."
            )
            if cap_bg.isOpened():
                cap_bg.release()
            if cap_fg.isOpened():
                cap_fg.release()
            video_writer.release()
            return False

        actual_frames_written = 0

        # Determine start and end FG frames for trimming
        if trim and frame_alignments:
            # If trim, only process the aligned segment based on keyframe matches
            # This means the frame_alignments list itself should represent the trimmed segment.
            # The calling code should prepare frame_alignments to only contain trimmed parts if trim is true.
            # Here, we assume frame_alignments IS the segment to render.
            pass

        # Use existing console for progress updates instead of creating new Progress context
        logger.info(f"Starting OpenCV composition of {len(frame_alignments)} frames")

        # Track progress without nested Progress context
        progress_interval = max(1, len(frame_alignments) // 20)  # Update every 5%

        for align_info in frame_alignments:
            fg_frame_to_get = align_info.fg_frame_idx
            bg_frame_to_get = align_info.bg_frame_idx

            cap_fg.set(cv2.CAP_PROP_POS_FRAMES, fg_frame_to_get)
            ret_fg, fg_frame_img = cap_fg.read()

            cap_bg.set(cv2.CAP_PROP_POS_FRAMES, bg_frame_to_get)
            ret_bg, bg_frame_img = cap_bg.read()

            if not ret_fg or not ret_bg:
                logger.warning(
                    f"Failed to read FG frame {fg_frame_to_get} (ret={ret_fg}) or BG frame {bg_frame_to_get} (ret={ret_bg}). Skipping."
                )
                continue

            current_fg_h, current_fg_w = fg_frame_img.shape[:2]
            final_fg_frame = fg_frame_img

            if current_fg_w > bg_info.width or current_fg_h > bg_info.height:
                scale_factor = min(
                    bg_info.width / current_fg_w, bg_info.height / current_fg_h
                )
                new_w = int(current_fg_w * scale_factor)
                new_h = int(current_fg_h * scale_factor)
                final_fg_frame = cv2.resize(
                    fg_frame_img, (new_w, new_h), interpolation=cv2.INTER_AREA
                )
                current_fg_h, current_fg_w = new_h, new_w

            x_offset, y_offset = spatial_offset
            composited_frame = bg_frame_img.copy()

            roi_x_start = max(0, x_offset)
            roi_y_start = max(0, y_offset)
            roi_x_end = min(bg_info.width, x_offset + current_fg_w)
            roi_y_end = min(bg_info.height, y_offset + current_fg_h)

            fg_crop_x_start = 0
            fg_crop_y_start = 0
            if x_offset < 0:
                fg_crop_x_start = abs(x_offset)
            if y_offset < 0:
                fg_crop_y_start = abs(y_offset)

            fg_to_overlay_w = roi_x_end - roi_x_start
            fg_to_overlay_h = roi_y_end - roi_y_start

            if fg_to_overlay_w > 0 and fg_to_overlay_h > 0:
                fg_cropped_for_overlay = final_fg_frame[
                    fg_crop_y_start : fg_crop_y_start + fg_to_overlay_h,
                    fg_crop_x_start : fg_crop_x_start + fg_to_overlay_w,
                ]

                if (
                    fg_cropped_for_overlay.shape[0] == fg_to_overlay_h
                    and fg_cropped_for_overlay.shape[1] == fg_to_overlay_w
                ):
                    composited_frame[roi_y_start:roi_y_end, roi_x_start:roi_x_end] = (
                        fg_cropped_for_overlay
                    )
                else:
                    logger.warning(
                        f"Skipping overlay for FG frame {fg_frame_to_get} due to unexpected crop size. "
                        f"ROI: {fg_to_overlay_w}x{fg_to_overlay_h}, Cropped: {fg_cropped_for_overlay.shape[1]}x{fg_cropped_for_overlay.shape[0]}"
                    )
            else:
                logger.debug(
                    f"FG frame {fg_frame_to_get} is completely outside BG frame after offset. Writing BG frame only."
                )

            video_writer.write(composited_frame)
            actual_frames_written += 1

            # Log progress at intervals
            if actual_frames_written % progress_interval == 0:
                progress_pct = (actual_frames_written / len(frame_alignments)) * 100
                logger.info(
                    f"OpenCV composition progress: {progress_pct:.1f}% ({actual_frames_written}/{len(frame_alignments)} frames)"
                )

        logger.info(
            f"OpenCV composition finished. {actual_frames_written} frames written to {output_path_str}."
        )
        cap_bg.release()
        cap_fg.release()
        video_writer.release()
        return True

    def process(
        self,
        bg: str,
        fg: str,
        output: str | None = None,
        match_space: str = "precise",
        temporal_align: str = "audio",
        trim: bool = False,
        skip_spatial_align: bool = False,
        max_keyframes: int = 2000,
    ):
        """Main processing function.

        Args:
            bg: Background video path
            fg: Foreground video path
            output: Output video path (auto-generated if not provided)
            match_space: Spatial alignment method ('precise' or 'fast')
            temporal_align: Temporal alignment method ('audio', 'duration', or 'frames')
            trim: Trim output to overlapping segments only
            skip_spatial_align: Skip spatial alignment (use center)
            max_keyframes: Maximum number of keyframes to use in frames mode
        """
        # Validate inputs
        bg_path = Path(bg)
        fg_path = Path(fg)

        if not bg_path.exists():
            logger.error(f"Background video not found: {bg}")
            return

        if not fg_path.exists():
            logger.error(f"Foreground video not found: {fg}")
            return

        # Generate output path if not provided
        if output is None:
            output = bg_path.stem + "_overlay_" + fg_path.stem + ".mp4"
            logger.info(f"Output path: {output}")

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            # Get video info
            task = progress.add_task("Analyzing videos...", total=None)
            logger.info(
                f"Processing overlay: {Path(bg_path).name} + {Path(fg_path).name}"
            )
            bg_info = self.get_video_info(str(bg_path))
            fg_info = self.get_video_info(str(fg_path))
            progress.update(task, completed=True)

            # Log video compatibility
            logger.info("Video compatibility check:")
            logger.info(
                f"  Resolution: BG {bg_info.width}x{bg_info.height} vs FG {fg_info.width}x{fg_info.height}"
            )
            logger.info(f"  Frame rate: BG {bg_info.fps:.2f} vs FG {fg_info.fps:.2f}")
            logger.info(
                f"  Duration: BG {bg_info.duration:.2f}s vs FG {fg_info.duration:.2f}s"
            )
            logger.info(
                f"  Audio: BG {'yes' if bg_info.has_audio else 'no'} vs FG {'yes' if fg_info.has_audio else 'no'}"
            )

            # Temporal alignment
            temporal_offset = 0.0
            frame_alignments = None
            alignment_mode = AlignmentMode(temporal_align)

            if (
                alignment_mode == AlignmentMode.AUDIO
                and bg_info.has_audio
                and fg_info.has_audio
            ):
                task = progress.add_task("Aligning by audio...", total=None)

                with tempfile.NamedTemporaryFile(
                    suffix=".wav", delete=False
                ) as bg_audio:
                    with tempfile.NamedTemporaryFile(
                        suffix=".wav", delete=False
                    ) as fg_audio:
                        try:
                            # Extract audio
                            if self.extract_audio(str(bg_path), bg_audio.name):
                                if self.extract_audio(str(fg_path), fg_audio.name):
                                    temporal_offset = self.compute_audio_offset(
                                        bg_audio.name, fg_audio.name
                                    )

                                    # Clamp negative offsets to 0
                                    if temporal_offset < 0:
                                        logger.warning(
                                            f"Negative offset {temporal_offset:.3f}s, "
                                            "clamping to 0"
                                        )
                                        temporal_offset = 0.0
                        finally:
                            # Clean up temp files
                            Path(bg_audio.name).unlink(missing_ok=True)
                            Path(fg_audio.name).unlink(missing_ok=True)

                progress.update(task, completed=True)

            elif alignment_mode == AlignmentMode.FRAMES:
                task = progress.add_task("Aligning by frames...", total=None)

                # Calculate target sample FPS based on max_keyframes
                # We want to sample enough frames but not exceed max_keyframes
                min_frame_count = min(bg_info.frame_count, fg_info.frame_count)
                min_duration = min(bg_info.duration, fg_info.duration)

                # Calculate the ideal sample FPS to get around max_keyframes/2 samples
                # (we use /2 because we want room for both videos)
                if min_duration > 0:
                    target_sample_fps = min(
                        max_keyframes / 2.0 / min_duration,
                        min_frame_count / min_duration,
                    )
                    # Ensure at least 1 fps sampling, but cap at reasonable rate
                    target_sample_fps = max(1.0, min(30.0, target_sample_fps))
                else:
                    target_sample_fps = 2.0  # Fallback

                logger.info(
                    f"Frame-based alignment selected. Extracting sample frames (target {target_sample_fps:.1f} FPS)"
                )
                logger.info(f"Max keyframes allowed: {max_keyframes}")
                logger.debug(
                    f"Background video: {bg_info.frame_count} frames @ {bg_info.fps:.2f} fps"
                )
                logger.debug(
                    f"Foreground video: {fg_info.frame_count} frames @ {fg_info.fps:.2f} fps"
                )

                bg_frames_sampled = self.extract_frames_sample(
                    str(bg_path), bg_info.fps, target_sample_fps
                )
                fg_frames_sampled = self.extract_frames_sample(
                    str(fg_path), fg_info.fps, target_sample_fps
                )

                if not bg_frames_sampled or not fg_frames_sampled:
                    logger.error(
                        "Could not extract sample frames. Falling back to duration alignment."
                    )
                    alignment_mode = AlignmentMode.DURATION  # Fallback
                else:
                    keyframe_matches = self.find_keyframe_matches(
                        bg_frames_sampled,
                        fg_frames_sampled,
                        bg_info,
                        fg_info,
                        target_sample_fps,
                    )

                    if not keyframe_matches:
                        logger.warning(
                            "No keyframe matches found from sampled frames. Falling back to duration alignment."
                        )
                        alignment_mode = AlignmentMode.DURATION  # Fallback
                    else:
                        frame_alignments_list, frame_based_initial_offset = (
                            self.compute_frame_alignment(
                                bg_info, fg_info, keyframe_matches, trim
                            )
                        )
                        if not frame_alignments_list:
                            logger.warning(
                                "Frame alignment list is empty (possibly due to trim or no matches). Falling back to duration alignment."
                            )
                            alignment_mode = AlignmentMode.DURATION  # Fallback
                        else:
                            frame_alignments = frame_alignments_list
                            temporal_offset = frame_based_initial_offset
                            logger.info(
                                f"Using frame-based initial temporal offset: {temporal_offset:.3f}s for subsequent steps."
                            )
                progress.update(task, completed=True)

            elif alignment_mode == AlignmentMode.DURATION:
                # Simple duration-based alignment (center fg within bg duration)
                logger.info(
                    "Using duration-based alignment (or fallback for frames mode)"
                )
                if bg_info.duration > fg_info.duration:
                    temporal_offset = (bg_info.duration - fg_info.duration) / 2
                else:
                    temporal_offset = 0.0
                logger.info(f"Duration-based offset: {temporal_offset:.3f}s")

            else:
                logger.info(f"No temporal alignment applied (mode: {temporal_align})")

            # Spatial alignment (common for all modes, uses the determined temporal_offset)
            spatial_offset = (0, 0)
            if not skip_spatial_align:
                task = progress.add_task("Aligning spatially...", total=None)
                try:
                    # Get frames for comparison using the determined temporal_offset
                    fg_frame_time_in_fg = 1.0  # e.g., 1 second into FG video
                    bg_frame_time_in_bg = fg_frame_time_in_fg + temporal_offset

                    # Ensure bg_frame_time_in_bg is valid
                    if (
                        bg_frame_time_in_bg < 0
                        or bg_frame_time_in_bg >= bg_info.duration
                    ):
                        logger.warning(
                            f"Calculated BG frame time {bg_frame_time_in_bg:.2f}s is out of bounds for spatial alignment. Using BG start."
                        )
                        bg_frame_time_in_bg = (
                            1.0  # Fallback to a frame near start of BG
                        )

                    fg_frame = self.get_frame_at_time(str(fg_path), fg_frame_time_in_fg)
                    bg_frame = self.get_frame_at_time(str(bg_path), bg_frame_time_in_bg)

                    spatial_offset = self.compute_spatial_offset(
                        bg_frame, fg_frame, method=match_space
                    )
                except IOError as e:
                    logger.warning(
                        f"Could not read frames for spatial alignment: {e}. Defaulting to center."
                    )
                    spatial_offset = (
                        (bg_info.width - fg_info.width) // 2,
                        (bg_info.height - fg_info.height) // 2,
                    )
                progress.update(task, completed=True)
            else:
                spatial_offset = (
                    (bg_info.width - fg_info.width) // 2,
                    (bg_info.height - fg_info.height) // 2,
                )
                logger.info(
                    f"Spatial alignment skipped. Using center position: {spatial_offset}"
                )

            # --- Video Composition Stage ---
            final_output_path = Path(output)
            task = progress.add_task(
                "Composing video...", total=None
            )  # Main composition task

            audio_source_for_merge = None
            use_bg_audio = True  # Default
            if bg_info.has_audio and fg_info.has_audio:
                use_bg_audio = True
                audio_source_for_merge = bg_info.path
            elif not bg_info.has_audio and fg_info.has_audio:
                use_bg_audio = False
                audio_source_for_merge = fg_info.path
            elif bg_info.has_audio and not fg_info.has_audio:
                use_bg_audio = True
                audio_source_for_merge = bg_info.path

            # `temporal_offset` here is the one determined by audio, duration, or the *initial* one from frames mode.

            if alignment_mode == AlignmentMode.FRAMES and frame_alignments:
                logger.info(
                    f"Using OpenCV for frame-by-frame composition ({len(frame_alignments)} frame mappings)"
                )
                temp_video_path = final_output_path.with_suffix(
                    ".temp_opencv_video.mp4"
                )

                compose_success = self.compose_videos_opencv(
                    bg_info,
                    fg_info,
                    str(temp_video_path),
                    spatial_offset,
                    frame_alignments,
                    trim,
                )
                progress.update(
                    task, completed=True
                )  # Mark main composing task as done for video part

                if compose_success and audio_source_for_merge:
                    audio_merge_task = progress.add_task(
                        "Merging audio (FFmpeg)...", total=None
                    )
                    # The `temporal_offset` is the initial overall offset of FG relative to BG.
                    # This `temporal_offset` should be applied to the `audio_source_for_merge` if it's the BG audio.
                    # If FG audio is used, its offset should be 0 relative to its own start, because frame_alignments map FG frames.
                    first_aligned_bg_frame_idx = frame_alignments[0].bg_frame_idx
                    audio_offset_for_final_merge = (
                        first_aligned_bg_frame_idx / bg_info.fps
                    )
                    logger.info(
                        f"Frame mode using BG audio: Audio source {Path(audio_source_for_merge).name} will be started from {audio_offset_for_final_merge:.3f}s for merge."
                    )

                    self._merge_audio_with_ffmpeg(
                        str(temp_video_path),
                        audio_source_for_merge,
                        str(final_output_path),
                        audio_offset=audio_offset_for_final_merge,
                        video_duration=len(frame_alignments) / fg_info.fps,
                        target_fps=fg_info.fps,
                    )
                    if temp_video_path.exists():
                        temp_video_path.unlink(missing_ok=True)
                    progress.update(audio_merge_task, completed=True)
                elif compose_success:
                    logger.info(
                        "OpenCV composition successful. No audio source to merge or audio merge skipped. Renaming temp video."
                    )
                    if temp_video_path.exists():
                        temp_video_path.rename(final_output_path)
                else:
                    logger.error(
                        "OpenCV video composition failed. No output generated."
                    )
                    if temp_video_path.exists():
                        temp_video_path.unlink(
                            missing_ok=True
                        )  # Clean up failed attempt

            else:
                logger.info(
                    "Using FFmpeg for composition (audio/duration mode or frames fallback)."
                )
                # `temporal_offset` here is from audio, duration, or initial frame based offset if frames mode fell back early
                self.compose_videos(
                    str(bg_path),
                    str(fg_path),
                    str(final_output_path),  # Ensure output is string
                    spatial_offset,
                    temporal_offset,  # This is the global offset for FFmpeg
                    bg_info,
                    fg_info,
                    use_bg_audio,
                    None,  # FFmpeg path does not use detailed frame_alignments map
                    trim,
                )
                progress.update(task, completed=True)

        # Final summary
        logger.info("Overlay processing complete:")
        logger.info(f"  Method: {alignment_mode.value} alignment")
        logger.info(f"  Spatial offset: {spatial_offset}")
        logger.info(f"  Temporal offset: {temporal_offset:.3f}s")
        if frame_alignments:
            logger.info(f"  Frame mappings: {len(frame_alignments)}")
        logger.info(f"  Trim: {'enabled' if trim else 'disabled'}")
        logger.info(f"  Output: {final_output_path}")

        console.print(f"[green]Video overlay complete: {final_output_path}")


def main(
    bg: str,
    fg: str,
    output: str | None = None,
    match_space: str = "precise",
    temporal_align: str = "frames",
    trim: bool = True,
    skip_spatial_align: bool = False,
    verbose: bool = False,
    max_keyframes: int = 2000,
):
    """Overlay foreground video onto background video with intelligent alignment.

    Args:
        bg: Background video path
        fg: Foreground video path
        output: Output video path (auto-generated if not provided)
        match_space: Spatial alignment method ('precise' or 'fast')
        temporal_align: Temporal alignment method ('audio', 'duration', or 'frames') [default: 'frames']
        trim: Trim output to overlapping segments only [default: True]
        skip_spatial_align: Skip spatial alignment (use center)
        verbose: Enable verbose logging
        max_keyframes: Maximum number of keyframes to use in frames mode [default: 2000]
    """
    processor = VideoOverlay(verbose=verbose)
    processor.process(
        bg,
        fg,
        output,
        match_space,
        temporal_align,
        trim,
        skip_spatial_align,
        max_keyframes,
    )


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="src/vidkompy/vidkompy.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "rich", "loguru", "opencv-python", "numpy", "scipy", "ffmpeg-python", "soundfile", "scikit-image"]
# ///
# this_file: src/vidkompy/vidkompy.py

"""
Intelligent video overlay tool with automatic spatial and temporal alignment.

This tool overlays foreground videos onto background videos with smart alignment:
- Preserves all foreground frames without retiming
- Finds optimal background frames for each foreground frame
- Supports audio-based and frame-based temporal alignment
- Automatic spatial alignment with template/feature matching
"""

import sys
import fire
from loguru import logger
from pathlib import Path
from typing import Optional

from .core.video_processor import VideoProcessor
from .core.alignment_engine import AlignmentEngine
from .models import MatchTimeMode


def main(
    bg: str,
    fg: str,
    output: Optional[str] = None,
    match_time: str = "precise",
    match_space: str = "precise",
    skip_spatial_align: bool = False,
    trim: bool = True,
    verbose: bool = False,
    max_keyframes: int = 2000,
):
    """Overlay foreground video onto background video with intelligent alignment.

    Args:
        bg: Background video path
        fg: Foreground video path
        output: Output video path (auto-generated if not provided)
        match_time: Temporal alignment - 'fast' (audio then frames) or 'precise' (frames)
        match_space: Spatial alignment - 'precise' (template) or 'fast' (feature)
        skip_spatial_align: Skip spatial alignment, center foreground
        trim: Trim output to overlapping segments only
        verbose: Enable verbose logging
        max_keyframes: Maximum keyframes for frame-based alignment
    """
    # Setup logging
    logger.remove()
    if verbose:
        logger.add(
            sys.stderr,
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan> - <level>{message}</level>",
            level="DEBUG",
        )
    else:
        logger.add(
            sys.stderr,
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
            level="INFO",
        )

    # Validate inputs
    bg_path = Path(bg)
    fg_path = Path(fg)

    if not bg_path.exists():
        logger.error(f"Background video not found: {bg}")
        return

    if not fg_path.exists():
        logger.error(f"Foreground video not found: {fg}")
        return

    # Generate output path if needed
    if output is None:
        output = f"{bg_path.stem}_overlay_{fg_path.stem}.mp4"
        logger.info(f"Output path: {output}")

    # Validate output path
    output_path = Path(output)
    if output_path.exists():
        logger.warning(f"Output file already exists: {output}")
        logger.warning("It will be overwritten")

    # Ensure output directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Validate match_time mode
    try:
        time_mode = MatchTimeMode(match_time)
    except ValueError:
        logger.error(f"Invalid match_time mode: {match_time}. Use 'fast' or 'precise'")
        return

    # Validate match_space mode
    valid_space_methods = ["precise", "template", "fast", "feature"]
    if match_space not in valid_space_methods:
        logger.error(
            f"Invalid match_space mode: {match_space}. Use one of: {', '.join(valid_space_methods)}"
        )
        return

    # Normalize space method names
    if match_space == "precise":
        match_space = "template"
    elif match_space == "fast":
        match_space = "feature"

    # Validate max_keyframes
    if max_keyframes < 10:
        logger.error(f"max_keyframes must be at least 10, got {max_keyframes}")
        return
    elif max_keyframes > 10000:
        logger.warning(
            f"max_keyframes is very high ({max_keyframes}), this may be slow"
        )

    # Create processor and engine
    processor = VideoProcessor()
    engine = AlignmentEngine(
        processor=processor, verbose=verbose, max_keyframes=max_keyframes
    )

    # Process the videos
    try:
        engine.process(
            bg_path=str(bg_path),
            fg_path=str(fg_path),
            output_path=output,
            time_mode=time_mode,
            space_method=match_space,
            skip_spatial=skip_spatial_align,
            trim=trim,
        )
    except Exception as e:
        logger.error(f"Processing failed: {e}")
        raise


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="package.toml">
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows
</file>

<file path="TODO.md">
# TODO: 

- The old code is in `src/vidkompy/vidkompy_old.py`.  
- Always assume that the fg video is the "better quality" video, and never should be re-timed 
- Use `tests/bg.mp4` and `tests/fg.mp4` for testing. 

Work in rounds: 

- Create `PROGRESS.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PROGRESS.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

```
python -m vidkompy --bg tests/bg.mp4 --fg tests/fg.mp4 -o tests/output_new.mp4 --match_time precise --verbose
```

```
⠋ Analyzing videos...17:48:03 | DEBUG    | get_video_info - Probing video: tests/bg.mp4
⠴ Analyzing videos...17:48:03 | INFO     | get_video_info - Video info for bg.mp4: 1920x1080, 60.00 fps, 7.85s, 472 frames, audio: yes
17:48:03 | DEBUG    | get_video_info - Probing video: tests/fg.mp4
⠏ Analyzing videos...17:48:04 | INFO     | get_video_info - Video info for fg.mp4: 1920x870, 60.89 fps, 8.04s, 483 frames, audio: yes
17:48:04 | INFO     | _log_compatibility - Video compatibility check:
17:48:04 | INFO     | _log_compatibility -   Resolution: 1920x1080 vs 1920x870
17:48:04 | INFO     | _log_compatibility -   FPS: 60.00 vs 60.89
17:48:04 | INFO     | _log_compatibility -   Duration: 7.85s vs 8.04s
17:48:04 | INFO     | _log_compatibility -   Audio: yes vs yes
⠏ Analyzing videos...           
⠋ Analyzing videos...           
⠋ Computing spatial alignment...17:48:05 | INFO     | _template_matching - Template match found at (0, 0) with confidence 0.9⠙ Analyzing videos...            
⠙ Computing spatial alignment... 
⠴ Analyzing videos...            
⠴ Computing spatial alignment... 
⠴ Computing temporal alignment...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/src/vidkompy/__main__.py", line 10, in <module>
    fire.Fire(main)
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/.venv/lib/python3.12/site-packages/fire/core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/.venv/lib/python3.12/site-packages/fire/core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/.venv/lib/python3.12/site-packages/fire/core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/src/vidkompy/vidkompy.py", line 131, in main
    engine.process(
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/src/vidkompy/core/alignment_engine.py", line 98, in process
    temporal_alignment = self._compute_temporal_alignment(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/src/vidkompy/core/alignment_engine.py", line 162, in _compute_temporal_alignment
    return self.temporal_aligner.align_frames(bg_info, fg_info, trim)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/src/vidkompy/core/temporal_alignment.py", line 110, in align_frames
    keyframe_matches = self._find_keyframe_matches(bg_info, fg_info)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/src/vidkompy/core/temporal_alignment.py", line 188, in _find_keyframe_matches
    best_match = self._find_best_match(
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/src/vidkompy/core/temporal_alignment.py", line 238, in _find_best_match
    similarity = self._compute_frame_similarity(bg_frame, fg_frame)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/src/vidkompy/core/temporal_alignment.py", line 263, in _compute_frame_similarity
    score, _ = ssim(gray1, gray2, full=True)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/.venv/lib/python3.12/site-packages/skimage/metrics/_structural_similarity.py", line 248, in structural_similarity
    uy = filter_func(im2, **filter_args)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/.venv/lib/python3.12/site-packages/scipy/ndimage/_filters.py", line 1181, in uniform_filter
    uniform_filter1d(input, int(size), axis, output, mode,
  File "/Users/adam/Developer/vcs/github.twardoch/pub/vidkompy/.venv/lib/python3.12/site-packages/scipy/ndimage/_filters.py", line 1109, in uniform_filter1d
    _nd_image.uniform_filter1d(input, size, axis, output, mode, cval,
KeyboardInterrupt
```
</file>

<file path="tests/test_package.py">
"""Test suite for vidkompy."""

def test_version():
    """Verify package exposes version."""
    import vidkompy
    assert vidkompy.__version__
</file>

<file path=".gitignore">
__pycache__/
__pypackages__/
._*
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pdm.toml
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.sass-cache
.scrapy
.specstory/.what-is-this.md
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.out
*.pot
*.py,cover
*.py[cod]
*.pyc
*.sage.py
*.so
*.spec
*$py.class
/site
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
Desktop.ini
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
node_modules
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
target/
Thumbs.db
var/
venv
venv.bak/
venv/
wheels/
</file>

<file path="pyproject.toml">
[project]
name = 'vidkompy'
dynamic = ['version']
description = ''
readme = 'README.md'
requires-python = '>=3.10'
license = 'MIT'
keywords = []
classifiers = [
    'Development Status :: 4 - Beta',
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Programming Language :: Python :: Implementation :: PyPy',
]
dependencies = [
    'fire',
    'rich',
    'loguru',
    'opencv-python',
    'numpy',
    'scipy',
    'ffmpeg-python',
    'soundfile',
    'scikit-image',
]

[[project.authors]]
name = 'Adam Twardoch'
email = 'adam+github@twardoch.com'

[project.optional-dependencies]
dev = [
    'pre-commit>=4.2.0',
    'ruff>=0.1.0',
    'mypy>=1.0.0',
    'pyupgrade>=3.19.0',
]
test = [
    'pytest>=8.3.5',
    'pytest-cov>=6.1.1',
]
all = [
    'fire',
    'rich',
    'loguru',
    'opencv-python',
    'numpy',
    'scipy',
    'ffmpeg-python',
    'soundfile',
    'scikit-image',
    'pre-commit>=4.2.0',
    'ruff>=0.1.0',
    'mypy>=1.0.0',
    'pyupgrade>=3.19.0',
    'pytest>=8.3.5',
    'pytest-cov>=6.1.1',
    'hatchling>=1.21.0',
    'hatch-vcs>=0.3.0',
]

[project.scripts]

[project.urls]
Documentation = 'https://github.com/twardoch/vidkompy#readme'
Issues = 'https://github.com/twardoch/vidkompy/issues'
Source = 'https://github.com/twardoch/vidkompy'

[build-system]
build-backend = 'hatchling.build'
requires = [
    'hatchling>=1.21.0',
    'hatch-vcs>=0.3.0',
]
[tool.coverage.paths]
vidkompy = [
    'src/vidkompy',
    '*/vidkompy/src/vidkompy',
]
tests = [
    'tests',
    '*/vidkompy/tests',
]

[tool.coverage.report]
exclude_lines = [
    'no cov',
    'if __name__ == .__main__.:',
    'if TYPE_CHECKING:',
]

[tool.coverage.run]
source_pkgs = [
    'vidkompy',
    'tests',
]
branch = true
parallel = true
omit = ['src/vidkompy/__about__.py']
[tool.hatch.build.hooks.vcs]
version-file = 'src/vidkompy/__version__.py'
[tool.hatch.build.targets.wheel]
packages = ['src/vidkompy']
[tool.hatch.envs.default]
dependencies = []

[tool.hatch.envs.default.scripts]
test = 'pytest {args:tests}'
test-cov = 'pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/vidkompy --cov=tests {args:tests}'
type-check = 'mypy src/vidkompy tests'
lint = [
    'ruff check src/vidkompy tests',
    'ruff format --respect-gitignore src/vidkompy tests',
]
fix = [
    'ruff check  --fix --unsafe-fixes src/vidkompy tests',
    'ruff format --respect-gitignore src/vidkompy tests',
]
[[tool.hatch.envs.all.matrix]]
python = [
    '3.10',
    '3.11',
    '3.12',
]

[tool.hatch.envs.lint]
detached = true
dependencies = []

[tool.hatch.envs.lint.scripts]
typing = 'mypy --install-types --non-interactive {args:src/vidkompy tests}'
style = [
    'ruff check {args:.}',
    'ruff format --respect-gitignore {args:.}',
]
fmt = [
    'ruff format --respect-gitignore {args:.}',
    'ruff check --fix {args:.}',
]
all = [
    'style',
    'typing',
]

[tool.hatch.envs.test]
dependencies = []

[tool.hatch.envs.test.scripts]
test = 'python -m pytest -n auto -p no:briefcase {args:tests}'
test-cov = 'python -m pytest -n auto -p no:briefcase --cov-report=term-missing --cov-config=pyproject.toml --cov=src/vidkompy --cov=tests {args:tests}'
bench = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only'
bench-save = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json'

[tool.hatch.version]
source = 'vcs'

[tool.hatch.version.raw-options]
version_scheme = 'post-release'

[tool.mypy]
python_version = '3.10'
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[tool.ruff]
target-version = 'py310'
line-length = 88

[tool.ruff.lint]
extend-select = [
    'A',
    'ARG',
    'B',
    'C',
    'DTZ',
    'E',
    'EM',
    'F',
    'FBT',
    'I',
    'ICN',
    'ISC',
    'N',
    'PLC',
    'PLE',
    'PLR',
    'PLW',
    'Q',
    'RUF',
    'S',
    'T',
    'TID',
    'UP',
    'W',
    'YTT',
]
ignore = [
    'ARG001',
    'E501',
    'I001',
    'RUF001',
    'PLR2004',
    'EXE003',
    'ISC001',
]

[tool.ruff.per-file-ignores]
"tests/*" = ['S101']
[tool.pytest.ini_options]
addopts = '-v --durations=10 -p no:briefcase'
asyncio_mode = 'auto'
console_output_style = 'progress'
filterwarnings = [
    'ignore::DeprecationWarning',
    'ignore::UserWarning',
]
log_cli = true
log_cli_level = 'INFO'
markers = [
    '''benchmark: marks tests as benchmarks (select with '-m benchmark')''',
    'unit: mark a test as a unit test',
    'integration: mark a test as an integration test',
    'permutation: tests for permutation functionality',
    'parameter: tests for parameter parsing',
    'prompt: tests for prompt parsing',
]
norecursedirs = [
    '.*',
    'build',
    'dist',
    'venv',
    '__pycache__',
    '*.egg-info',
    '_private',
]
python_classes = ['Test*']
python_files = ['test_*.py']
python_functions = ['test_*']
testpaths = ['tests']

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = 'file'
save-data = true
compare = [
    'min',
    'max',
    'mean',
    'stddev',
    'median',
    'iqr',
    'ops',
    'rounds',
]
</file>

<file path="README.md">
# vidkompy

`vidkompy` is an intelligent command-line tool designed to overlay a foreground video onto a background video with high-precision automatic alignment. The system is engineered to handle differences in resolution, frame rate, duration, and audio, prioritizing content integrity and accuracy over raw processing speed.

The core philosophy is to treat the **foreground video as the definitive source of quality and timing**. All its frames are preserved without modification or retiming. The background video is dynamically adapted—stretched, retimed, and selectively sampled—to synchronize perfectly with the foreground content.

## How It Works: The Processing Pipeline

The alignment and composition process is orchestrated by the `AlignmentEngine` and follows a meticulous, multi-stage pipeline:

1.  **Video Analysis**: The process begins by probing both background and foreground videos using `ffprobe` to extract essential metadata. This includes resolution, frames per second (FPS), duration, frame count, and audio presence. This information is used to check for compatibility and inform subsequent alignment decisions.

2.  **Spatial Alignment**: The engine determines the optimal (x, y) coordinates to place the foreground video on top of the background. A sample frame from the middle of each video is used for this calculation. The system then calculates the offset needed to position the foreground video correctly within the background frame.

3.  **Temporal Alignment**: This is the most critical phase, where the videos are synchronized in time. `vidkompy` finds the perfect background frame to match _every single_ foreground frame, ensuring the foreground's timing is flawlessly preserved. This creates a detailed frame-by-frame mapping.

4.  **Video Composition**: With alignment data computed, the final video is composed:

- First, a silent video is created using OpenCV. The engine iterates through the frame alignment map, reads the corresponding frames from each video, overlays them using the spatial offset, and writes the composite frame to a temporary file. The output FPS is set to the foreground video's FPS to ensure no foreground frames are dropped.
- Next, an audio track is added using FFmpeg. The tool intelligently selects the audio source, **prioritizing the foreground video's audio**. The audio is synchronized with the composed video, and the final output file is generated.

## Alignment Methods in Detail

`vidkompy` employs sophisticated algorithms for both temporal and spatial alignment, with different modes to balance speed and precision.

### Temporal Alignment: Finding the Perfect Sync

Temporal alignment synchronizes the two videos over time. This can be done using audio cues or by analyzing visual content.

#### **`--match_time fast`**: This mode first attempts to align the videos using their audio tracks, which is very fast and efficient.

- **Audio-Based Synchronization**: The audio from both videos is extracted into temporary WAV files. The system then computes the **cross-correlation** of the two audio signals. The peak of the correlation reveals the time offset needed to sync the tracks. A confidence score is calculated to validate the match. If audio alignment succeeds, a simple frame mapping is created based on this single time offset.
- **Fallback**: If audio is unavailable in either video or the extraction fails, this mode automatically falls back to the more intensive frame-based alignment method.

#### **`--match_time precise`** (Default): This mode goes directly to frame-based alignment for the most accurate synchronization, which is essential for videos without clear audio cues or with slight timing drifts.

- **1. Keyframe Selection**: To work efficiently, the aligner samples a limited number of frames (keyframes) from both videos. It samples the foreground video based on a configurable `max_keyframes` limit and then samples the background more densely to ensure a good search space.
- **2. Keyframe Matching**: For each foreground keyframe, the system searches for the best-matching background keyframe. The similarity between frame pairs is calculated using the **Structural Similarity Index (SSIM)**. A match is only accepted if the similarity score exceeds a confidence threshold of 0.6.
- **3. Monotonic Filtering**: The list of keyframe matches is filtered to ensure a logical time progression. Any match where a later foreground frame corresponds to an earlier background frame is discarded to prevent temporal inconsistencies.
- **4. Frame Interpolation**: With a clean set of keyframe matches, the engine builds a complete alignment map for _every single foreground frame_. For frames that fall between keyframes, the corresponding background frame index is calculated using **smooth interpolation** (a smoothstep function). This is a crucial step that ensures the background video's motion appears natural and fluid, avoiding the jerky "drift-and-catchup" issues that can plague simpler methods.

### Spatial Alignment: Finding the Perfect Position

Spatial alignment determines where the foreground video sits within the larger background frame. If the foreground video is larger than the background, it is automatically scaled down to fit while preserving its aspect ratio.

#### ** `--match_space precise` / `template` ** (Default): This method uses **Template Matching**.

- It treats the entire foreground frame as a template and searches for its most likely position within the background frame.
- The match is performed using normalized cross-correlation (`cv2.TM_CCOEFF_NORMED`), which is highly precise for finding exact pixel-level positions. A confidence score is generated based on the quality of the match.

#### ** `--match_space fast` / `feature` **: This method uses **Feature Matching**, which is more robust against slight variations like compression or color changes.

- It uses the **ORB (Oriented FAST and Rotated BRIEF)** algorithm to detect hundreds of key feature points in both frames.
- It then matches these keypoints between the two frames and calculates the median displacement to find the overlay coordinates.
- If too few features are found or matched, it automatically falls back to a safe center alignment.

#### **`--skip_spatial_align`**: If this flag is used, all alignment calculations are skipped, and the foreground video is simply centered within the background frame.

## Usage

### Prerequisites

You must have the **FFmpeg** binary installed on your system and accessible in your system's PATH. `vidkompy` depends on it for all video and audio processing tasks.

### Installation

The tool is a Python package and can be installed using pip:

```bash
uv pip install .
```

### Command-Line Interface (CLI)

The tool is run from the command line. The primary arguments are the paths to the background and foreground videos.

```bash
python -m vidkompy --bg <background_video> --fg <foreground_video> [OPTIONS]
```

**Key Arguments:**

- `--bg` (str): Path to the background video file. **[Required]**
- `--fg` (str): Path to the foreground video file. **[Required]**
- `-o`, `--output` (str): Path for the final output video. If not provided, a name is automatically generated (e.g., `bg-stem_overlay_fg-stem.mp4`).
- `--match_time` (str): The temporal alignment method.
  - `'precise'` (default): Uses frame-based matching for maximum accuracy.
  - `'fast'`: Attempts audio-based alignment first and falls back to frames.
- `--match_space` (str): The spatial alignment method.
  - `'precise'` or `'template'` (default): Slower but more exact pixel matching.
  - `'fast'` or `'feature'`: Faster, more robust feature-based matching.
- `--trim` (bool): If `True` (default), the output video is trimmed to the duration of the aligned segment, dropping any un-matched frames from the beginning or end.
- `--skip_spatial_align` (bool): If `True` (default: `False`), skips spatial alignment and centers the foreground video.
- `--verbose` (bool): Enables detailed debug logging for troubleshooting.
- `--max_keyframes` (int): Sets the maximum number of keyframes to use for the `'precise'` temporal alignment mode (default: 2000).
</file>

</files>



---

<TASK>

TODO: 

Conduct extensive research for alternative, better methods of spatial and temporal alignment of video that we could use in our code. I’m interested in performant methods, smart methods, optimal methods. Right now the frame-by-frame alignment is kind of slow. 

Perform the research, and write a spec that a junior software development can use to improve the current code.  

</TASK>