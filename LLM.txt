This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: varia, .specstory, TODO.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
.cursor/
  rules/
    alignment-algorithms.mdc
    frame-matching-models.mdc
    video-composition-rules.mdc
    video-processing-flow.mdc
.giga/
  specifications.json
.github/
  workflows/
    push.yml
    release.yml
src/
  vidkompy/
    core/
      __init__.py
      alignment_engine.py
      dtw_aligner.py
      frame_fingerprint.py
      spatial_alignment.py
      temporal_alignment.py
      video_processor.py
    __init__.py
    __main__.py
    __version__.py
    models.py
    vidkompy.py
tests/
  test_package.py
.cursorrules
.gitignore
.pre-commit-config.yaml
CHANGELOG.md
CLAUDE.md
LICENSE
package.toml
PROGRESS.md
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".giga/specifications.json">
[
  {
    "fileName": "main-overview.mdc",
    "description": "Complete system overview covering the video overlay architecture, key components, and processing pipeline flow"
  },
  {
    "fileName": "alignment-algorithms.mdc",
    "description": "Detailed documentation of the spatial and temporal alignment algorithms, including template matching, feature matching, and frame/audio synchronization techniques"
  },
  {
    "fileName": "video-processing-flow.mdc",
    "description": "End-to-end data flow documentation covering video ingestion, frame processing, alignment calculations, overlay composition, and output generation"
  },
  {
    "fileName": "frame-matching-models.mdc",
    "description": "Technical specifications for frame matching and mapping models, including similarity scoring, keyframe selection, and frame rate handling logic"
  },
  {
    "fileName": "video-composition-rules.mdc",
    "description": "Documentation of the business rules and constraints for video composition, including foreground preservation, audio handling, and quality thresholds"
  }
]
</file>

<file path=".github/workflows/push.yml">
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/vidkompo --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/vidkompo
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="package.toml">
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows
</file>

<file path=".cursor/rules/alignment-algorithms.mdc">
---
description: Documents video alignment algorithms for spatial/temporal synchronization and frame matching implementations
globs: src/vidkompy/core/alignment*.py,src/vidkompy/core/dtw*.py,src/vidkompy/core/temporal*.py,src/vidkompy/core/spatial*.py
alwaysApply: false
---


# alignment-algorithms

## Spatial Alignment Core (90/100)
- Advanced template matching using OpenCV for precise sub-region detection
- Two-phase alignment approach:
  1. Initial coarse positioning using feature detection (ORB) 
  2. Fine-tuning using normalized cross-correlation
- Compensates for compression artifacts and brightness variations
- Automatic position optimization within background frame bounds

## Temporal Synchronization Engine (95/100)
- Dynamic Time Warping (DTW) implementation for frame sequence alignment
- Perceptual hash-based frame fingerprinting for robust matching
- Adaptive keyframe selection with confidence-based early termination
- Multi-modal alignment combining audio and visual cues
- Zero-drift frame interpolation maintaining foreground timing integrity

## Frame Matching System (85/100)
- Combined perceptual hash algorithms (pHash, ColorMoment) for frame signatures
- Progressive refinement matching:
  1. Quick hash-based candidate identification
  2. Structural similarity validation
  3. Content-aware position adjustment
- Maintains foreground frame sequence while adapting background timing

## Audio-Visual Integration (80/100)
- Cross-correlation based audio track alignment
- Fallback chain for temporal synchronization:
  1. Audio track correlation
  2. Frame content matching
  3. Duration-based centering
- Intelligent audio source selection prioritizing foreground quality

Relevant Files:
```
src/vidkompy/core/alignment_engine.py
src/vidkompy/core/dtw_aligner.py 
src/vidkompy/core/spatial_alignment.py
src/vidkompy/core/temporal_alignment.py
```

$END$
</file>

<file path=".cursor/rules/frame-matching-models.mdc">
---
description: Specifications for frame matching, similarity scoring, and temporal mapping models in video overlay system
globs: src/vidkompy/core/frame_fingerprint.py,src/vidkompy/core/temporal_alignment.py,src/vidkompy/core/dtw_aligner.py
alwaysApply: false
---


# frame-matching-models

Core frame matching and temporal alignment models implementing the intelligent video overlay system.

## Frame Fingerprinting System (Importance: 95)
- Perceptual hash generation for efficient frame comparison
- Multi-hash approach combining pHash, AverageHash and ColorMomentHash 
- Frame similarity scoring using custom Hamming distance thresholds
- Adaptive keyframe detection and extraction

## Temporal Mapping Models (Importance: 90)
- Dynamic Time Warping (DTW) alignment for optimal frame-to-frame mapping
- Zero-drift temporal synchronization preserving foreground frame timing
- Sakoe-Chiba band constraint implementation for restricted warping paths
- Keyframe interpolation system for smooth transitions

## Frame Matching Engine (Importance: 85) 
- Template-based precise matching for exact frame alignment
- Feature-based fast matching using ORB detection
- Confidence scoring for match quality assessment
- Adaptive search window optimization

## Key Implementation Files:
```
src/vidkompy/core/
├── frame_fingerprint.py    # Frame hash generation and comparison
├── temporal_alignment.py   # Frame mapping and synchronization
└── dtw_aligner.py         # Dynamic time warping implementation
```

$END$
</file>

<file path=".cursor/rules/video-composition-rules.mdc">
---
description: Defines core business rules for video composition, foreground preservation, and quality thresholds in video overlaying operations
globs: src/vidkompy/core/video_processor.py,src/vidkompy/core/alignment_engine.py,src/vidkompy/models/composition.py
alwaysApply: false
---


# video-composition-rules

## Core Composition Rules (Importance: 95)
1. Foreground video prioritization:
   - Foreground video is preserved as the "better quality" source
   - Never re-time or modify foreground video frames
   - Foreground frames dictate the output FPS
   - Foreground audio track takes precedence when both sources have audio

2. Background video adaptations:
   - Background video can be dynamically stretched/modified
   - Frame dropping/interpolation allowed for background synchronization
   - Background spatial position adjusted to accommodate foreground overlay

## Audio Handling Rules (Importance: 85)
1. Audio source selection:
   - If both videos have audio: Use foreground audio
   - If only one has audio: Use available audio track
   - Audio tracks used for temporal alignment when --match_time audio specified

2. Audio synchronization:
   - Audio track alignment determines frame timing
   - Audio cross-correlation used to find optimal time offset
   - Audio fallback to frame-based methods if unusable

## Quality and Resolution Rules (Importance: 90)
1. Resolution constraints:
   - Foreground video never larger than background video
   - Background video must fully contain foreground overlay
   - Output resolution matches background video dimensions

2. Frame preservation:
   - No quality loss allowed in foreground frames
   - Background frames may be transformed to match foreground timing
   - Output uses highest FPS between sources

## Temporal Alignment Rules (Importance: 88)
1. Frame selection:
   - Find optimal start/end frame pairs
   - Trim output to best matching segment
   - Drop non-matching frames from start/end
   - Preserve foreground frame sequence integrity

2. Duration handling:
   - Handle different video durations
   - Find best temporal offset for overlay
   - Trim to overlapping segments when --trim enabled

Relevant files:
- src/vidkompy/core/video_processor.py
- src/vidkompy/core/alignment_engine.py

$END$
</file>

<file path=".cursor/rules/video-processing-flow.mdc">
---
description: Documents video data flow for frame processing, alignment calculations, overlay composition, and output generation
globs: src/vidkompy/core/*.py,src/vidkompy/vidkompy*.py,src/vidkompy/models.py
alwaysApply: false
---


# video-processing-flow

## Frame Processing Flow
- Video frames are extracted and processed using a frame-by-frame pipeline:
  1. Frame extraction from source videos
  2. Frame fingerprinting using perceptual hashing
  3. Keyframe detection and analysis
  4. Frame matching and alignment calculation

Importance Score: 95 (Core video processing pipeline)

## Alignment Calculation Flow
- Two-stage alignment process:
  1. Spatial alignment using template/feature matching
  2. Temporal alignment using frame content or audio matching
- Generates frame mapping between foreground and background videos

Importance Score: 90 (Critical alignment logic)

## Overlay Composition Pipeline
1. Frame pair selection based on alignment mapping
2. Foreground frame overlay onto background frame
3. Output frame composition and encoding
4. Audio track selection and synchronization

Importance Score: 85 (Core video composition logic)

## Output Generation
- Handles synchronized video/audio output:
  - Uses foreground video's FPS as the target rate
  - Preserves foreground video timing
  - Adapts background video timing dynamically
  - Prioritizes foreground audio when available

Importance Score: 80 (Output generation workflow)

Key files:
- `src/vidkompy/core/video_processor.py`: Frame extraction and processing
- `src/vidkompy/core/alignment_engine.py`: Alignment coordination
- `src/vidkompy/core/spatial_alignment.py`: Spatial alignment
- `src/vidkompy/core/temporal_alignment.py`: Temporal alignment

$END$
</file>

<file path="src/vidkompy/core/__init__.py">
# this_file: src/vidkompy/core/__init__.py
</file>

<file path="src/vidkompy/core/frame_fingerprint.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/frame_fingerprint.py

"""
Fast frame fingerprinting system using perceptual hashing.

This module provides ultra-fast frame comparison capabilities that are
100-1000x faster than SSIM while maintaining good accuracy for similar frames.
"""

import cv2
import numpy as np
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
from loguru import logger
import time


class FrameFingerprinter:
    """Ultra-fast frame comparison using perceptual hashing.

    Why perceptual hashing:
    - 100-1000x faster than SSIM
    - Robust to compression artifacts and minor color/brightness changes
    - Compact representation (64 bits per frame)
    - Works well for finding similar frames

    Why multiple hash algorithms:
    - Different hashes capture different aspects of the image
    - Combining them improves robustness
    - Reduces false positives/negatives
    """

    def __init__(self, log_init: bool = True):
        """Initialize fingerprinter with multiple hash algorithms.

        Args:
            log_init: Whether to log initialization messages
        """
        self.hashers = {}
        self._init_hashers(log_init)

        # Cache for computed fingerprints
        self.fingerprint_cache: dict[str, dict[int, dict[str, np.ndarray]]] = {}

    def _init_hashers(self, log_init: bool = True):
        """Initialize available hash algorithms.

        Why these specific algorithms:
        - PHash: Frequency domain analysis, good for structure
        - AverageHash: Average color, good for brightness
        - ColorMomentHash: Color distribution, good for color changes
        - MarrHildrethHash: Edge detection, good for shapes
        """
        try:
            self.hashers["phash"] = cv2.img_hash.PHash_create()
            if log_init:
                logger.debug("✓ PHash initialized")
        except AttributeError:
            logger.warning("PHash not available")

        try:
            self.hashers["ahash"] = cv2.img_hash.AverageHash_create()
            if log_init:
                logger.debug("✓ AverageHash initialized")
        except AttributeError:
            logger.warning("AverageHash not available")

        try:
            self.hashers["dhash"] = cv2.img_hash.ColorMomentHash_create()
            if log_init:
                logger.debug("✓ ColorMomentHash initialized")
        except AttributeError:
            logger.warning("ColorMomentHash not available")

        try:
            self.hashers["mhash"] = cv2.img_hash.MarrHildrethHash_create()
            if log_init:
                logger.debug("✓ MarrHildrethHash initialized")
        except AttributeError:
            logger.warning("MarrHildrethHash not available")

        if not self.hashers:
            msg = (
                "No perceptual hash algorithms available. "
                "Please install opencv-contrib-python."
            )
            raise RuntimeError(msg)

        if log_init:
            logger.info(f"Initialized {len(self.hashers)} hash algorithms")

    def compute_fingerprint(self, frame: np.ndarray) -> dict[str, np.ndarray]:
        """Compute multi-algorithm fingerprint for a frame.

        Args:
            frame: Input frame as numpy array

        Returns:
            Dictionary of hash algorithm names to hash values

        Why multiple algorithms:
        - Redundancy reduces errors
        - Different algorithms catch different changes
        - Weighted combination improves accuracy
        """
        # Standardize frame size for consistent hashing
        std_size = (64, 64)
        std_frame = cv2.resize(frame, std_size, interpolation=cv2.INTER_AREA)

        # Convert to grayscale for most hashes
        if len(std_frame.shape) == 3:
            gray_frame = cv2.cvtColor(std_frame, cv2.COLOR_BGR2GRAY)
        else:
            gray_frame = std_frame

        fingerprint = {}

        # Compute each available hash
        for name, hasher in self.hashers.items():
            try:
                if name in ["phash", "ahash", "mhash"]:
                    # These work on grayscale
                    hash_value = hasher.compute(gray_frame)
                else:
                    # ColorMomentHash needs color
                    hash_value = hasher.compute(std_frame)

                fingerprint[name] = hash_value
            except Exception as e:
                logger.warning(f"Failed to compute {name}: {e}")

        # Add color histogram as additional feature
        if len(frame.shape) == 3:
            fingerprint["histogram"] = self._compute_color_histogram(std_frame)

        return fingerprint

    def _compute_color_histogram(self, frame: np.ndarray) -> np.ndarray:
        """Compute color histogram for additional discrimination.

        Why color histogram:
        - Captures global color distribution
        - Complements structure-based hashes
        - Fast to compute and compare
        """
        # Compute histogram for each channel
        hist_b = cv2.calcHist([frame], [0], None, [32], [0, 256])
        hist_g = cv2.calcHist([frame], [1], None, [32], [0, 256])
        hist_r = cv2.calcHist([frame], [2], None, [32], [0, 256])

        # Concatenate and normalize
        hist = np.concatenate([hist_b, hist_g, hist_r])
        hist = hist.flatten()
        hist = hist / (hist.sum() + 1e-7)  # Normalize

        return hist.astype(np.float32)

    def compare_fingerprints(
        self, fp1: dict[str, np.ndarray], fp2: dict[str, np.ndarray]
    ) -> float:
        """Compare two fingerprints and return similarity score.

        Args:
            fp1: First fingerprint
            fp2: Second fingerprint

        Returns:
            Similarity score between 0 and 1

        Why weighted combination:
        - PHash is most reliable for video frames
        - Other hashes help disambiguate
        - Histogram adds color information
        """
        scores = {}

        # Compare each hash type
        for name in fp1:
            if name not in fp2:
                continue

            if name == "histogram":
                # Use histogram correlation
                score = cv2.compareHist(
                    fp1["histogram"], fp2["histogram"], cv2.HISTCMP_CORREL
                )
                scores["histogram"] = max(0, score)  # Ensure non-negative
            else:
                # Use Hamming distance for hashes
                # Ensure uint8 type for NORM_HAMMING
                h1 = (
                    fp1[name].astype(np.uint8)
                    if fp1[name].dtype != np.uint8
                    else fp1[name]
                )
                h2 = (
                    fp2[name].astype(np.uint8)
                    if fp2[name].dtype != np.uint8
                    else fp2[name]
                )
                distance = cv2.norm(h1, h2, cv2.NORM_HAMMING)

                # Normalize to 0-1 similarity
                # Most hashes produce 64-bit (8 byte) values
                max_bits = h1.shape[0] * 8
                similarity = 1.0 - (distance / max_bits)
                scores[name] = similarity

        if not scores:
            return 0.0

        # Weighted combination based on reliability
        weights = {
            "phash": 0.4,  # Most reliable
            "ahash": 0.2,  # Good for brightness
            "dhash": 0.2,  # Good for color
            "mhash": 0.1,  # Good for edges
            "histogram": 0.1,  # Global color
        }

        total_weight = 0
        total_score = 0

        for name, score in scores.items():
            weight = weights.get(name, 0.1)
            total_score += score * weight
            total_weight += weight

        return total_score / total_weight if total_weight > 0 else 0.0

    def precompute_video_fingerprints(
        self,
        video_path: str,
        frame_indices: list[int],
        video_processor,
        resize_factor: float = 0.25,
    ) -> dict[int, dict[str, np.ndarray]]:
        """Precompute fingerprints for all specified frames in parallel.

        Args:
            video_path: Path to video file
            frame_indices: List of frame indices to process
            video_processor: VideoProcessor instance for frame extraction
            resize_factor: Factor to resize frames before hashing

        Returns:
            Dictionary mapping frame indices to fingerprints

        Why parallel processing:
        - Frame extraction is I/O bound (use threads)
        - Hash computation is CPU bound (use processes)
        - Significant speedup on multi-core systems
        """
        # Check cache first
        if video_path in self.fingerprint_cache:
            cached = self.fingerprint_cache[video_path]
            missing = [idx for idx in frame_indices if idx not in cached]
            if not missing:
                return {idx: cached[idx] for idx in frame_indices}
            frame_indices = missing

        logger.info(f"Computing fingerprints for {len(frame_indices)} frames...")
        start_time = time.time()

        # Step 1: Extract frames in batches (I/O bound)
        frames_dict = {}
        batch_size = 50

        for i in range(0, len(frame_indices), batch_size):
            batch_indices = frame_indices[i : i + batch_size]
            batch_frames = video_processor.extract_frames(
                video_path, batch_indices, resize_factor
            )

            for idx, frame in zip(batch_indices, batch_frames, strict=False):
                if frame is not None:
                    frames_dict[idx] = frame

        # Step 2: Compute fingerprints in parallel (CPU bound)
        fingerprints = {}

        # Use process pool for CPU-intensive hash computation
        with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:
            # Submit all tasks
            future_to_idx = {
                executor.submit(self._compute_fingerprint_worker, frame): idx
                for idx, frame in frames_dict.items()
            }

            # Collect results
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    fingerprint = future.result()
                    fingerprints[idx] = fingerprint
                except Exception as e:
                    logger.warning(
                        f"Failed to compute fingerprint for frame {idx}: {e}"
                    )

        # Update cache
        if video_path not in self.fingerprint_cache:
            self.fingerprint_cache[video_path] = {}
        self.fingerprint_cache[video_path].update(fingerprints)

        elapsed = time.time() - start_time
        fps = len(fingerprints) / elapsed if elapsed > 0 else 0
        logger.info(
            f"Computed {len(fingerprints)} fingerprints in {elapsed:.2f}s "
            f"({fps:.1f} fps)"
        )

        return fingerprints

    @staticmethod
    def _compute_fingerprint_worker(frame: np.ndarray) -> dict[str, np.ndarray]:
        """Worker function for parallel fingerprint computation.

        Static method to enable pickling for multiprocessing.
        """
        # Create a new instance in the worker process without logging
        fingerprinter = FrameFingerprinter(log_init=False)
        return fingerprinter.compute_fingerprint(frame)

    def clear_cache(self, video_path: str | None = None):
        """Clear fingerprint cache.

        Args:
            video_path: Specific video to clear, or None for all
        """
        if video_path:
            self.fingerprint_cache.pop(video_path, None)
        else:
            self.fingerprint_cache.clear()
</file>

<file path="src/vidkompy/__init__.py">
# this_file: src/vidkompy/__init__.py

from .__version__ import __version__

__all__ = ["__version__"]
</file>

<file path="src/vidkompy/__main__.py">
#!/usr/bin/env python
# this_file: src/vidkompy/__main__.py

"""Enable running vidkompy as a module with python -m vidkompy."""

import fire
from vidkompy.vidkompy import main

if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path=".cursorrules">
## Coding style

<guidelines>
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

Work in rounds: 

- Create `PROGRESS.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PROGRESS.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py312-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</guidelines>


## Project Overview

vidkompy is an intelligent video overlay tool that overlays foreground videos onto background videos with automatic alignment. The project prioritizes 
precision and intelligence over raw speed.

`vidkompy` is a Python CLI (Fire) tool that lets me overlay two videos: --bg (background) and --fg (foreground), and saves the result to --output (name is built automatically if not provided).

The tool needs to be robust and smart.

1. The bg video can be larger than the fg video. In that case, the tool needs to automatically find the best offset to overlay the fg video on the bg video.

2. The bg and fg videos can have different FPSes. In that case, the tool needs to use the higher FPS for the output video.

3. The bg and fg videos may have different durations (the difference would be typically rather slight). Ths tool needs to find the optimal time offset to overlay the fg video on the bg video. We need to find "the perfect pair of frames that when overlaid will form the START frame of the output video", and "the perfect pair of frames that when overlaid will form the END frame of the output video". The output video will be trimmed to go from that start to that end. Some starting and/or ending frames from either the fg or the bg video may be dropped. 

4. The bg and fg videos may have sound (it would be identical, but may be offset). If both have sound, the sound may be used to find the right time offset for the overlay (if I specify --match_time audio). The fg sound would be is for the output video. If only one has sound, that sound is used for the output video.

5. The content of the two videos would typically be very similar but not identical, because one video is typically a recompressed, edited variant of the other.

6. Overall, the fg video is thought to be "better quality", and temporally properly aligned. The bg video can be streched, modified, re-timed, frames added or dropped etc. 

7. All these conditions may be combined. That means, the tool needs to be able to find both the spatial and the temporal offsets. Spatially, the fg video is never larger than the bg video. Temporally, either video can be longer.



## Key Commands

### Development & Testing
```bash
# Run tests
hatch run test

# Run tests with coverage
hatch run test-cov

# Type checking
hatch run type-check

# Linting and formatting
hatch run lint      # Check code
hatch run fix       # Auto-fix issues
hatch run fmt       # Format code

# All lint checks
hatch run lint:all
```

### Running the Tool
```bash
python src/vidkompy/vidkompy_old.py --help|cat

INFO: Showing help with the command 'vidkompy_old.py -- --help'.

NAME
    vidkompy_old.py - Overlay foreground video onto background video with intelligent alignment.

SYNOPSIS
    vidkompy_old.py BG FG <flags>

DESCRIPTION
    Overlay foreground video onto background video with intelligent alignment.

POSITIONAL ARGUMENTS
    BG
        Type: str
        Background video path
    FG
        Type: str
        Foreground video path

FLAGS
    -o, --output=OUTPUT
        Type: Optional[str | None]
        Default: None
        Output video path (auto-generated if not provided)
    --match_space=MATCH_SPACE
        Type: str
        Default: 'precise'
        Spatial alignment method ('precise' or 'fast')
    --temporal_align=TEMPORAL_ALIGN
        Type: str
        Default: 'frames'
        Temporal alignment method ('audio', 'duration', or 'frames') [default: 'frames']
    --trim=TRIM
        Type: bool
        Default: True
        Trim output to overlapping segments only [default: True]
    -s, --skip_spatial_align=SKIP_SPATIAL_ALIGN
        Type: bool
        Default: False
        Skip spatial alignment (use center)
    -v, --verbose=VERBOSE
        Type: bool
        Default: False
        Enable verbose logging
    --max_keyframes=MAX_KEYFRAMES
        Type: int
        Default: 2000
        Maximum number of keyframes to use in frames mode [default: 2000]

NOTES
    You can also use flags syntax for POSITIONAL ARGUMENTS


# Direct execution (main implementation in vidkompy_old.py)
python -m vidkompy --bg tests/bg.mp4 --fg tests/fg.mp4 -o tests/output_new.mp4 --match_time precise --verbose
```

## Architecture Notes

1. **Main Implementation**: The core functionality resides in `src/vidkompy/vidkompy_old.py` (60KB). 

The `vidkompy.py` file is a placeholder for future refactoring.

2. **Temporal Alignment Modes**:
   - `audio`: Cross-correlation of audio tracks
   - `duration`: Centers foreground within background duration
   - `frames`: Advanced frame-by-frame content matching with dynamic mapping

3. **Spatial Alignment Methods**:
   - `template` (precise, default): OpenCV template matching for exact sub-regions
   - `feature` (fast): ORB feature matching for more robust alignment

4. **Known Issue**: The `frames` temporal alignment method has drift-and-catchup issues where the background portion runs at varying speeds 

5. **Dependencies**: The tool requires FFmpeg binary installed separately. Python dependencies are listed in the script header and include opencv-python, ffmpeg-python, scipy, numpy, soundfile, scikit-image, fire, rich, and loguru.

## Testing Approach

The project uses pytest with minimal test coverage currently (only version check). When adding new functionality:
- Place tests in `tests/` directory
- Use pytest markers for categorization (unit, integration, benchmark)
- Run with `hatch run test` or `hatch run test-cov` for coverage

START SPECIFICATION:
---
description: Apply when documenting high-level business logic and domain-specific implementations for video overlay and alignment systems, specifically for intelligent video composition tools that require precise spatial and temporal synchronization
globs: *.py,*.md
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


vidkompy is an intelligent video overlay system focused on precision alignment of video content. The core business logic revolves around automatically synchronizing and compositing video streams with different characteristics.

## Core Business Components

### Video Alignment Engine (Importance: 95)
- Automatic detection of optimal spatial positioning for foreground overlay
- Temporal synchronization using frame content or audio signals
- Smart handling of FPS and duration differences between videos
- File: src/vidkompy/vidkompy_old.py

### Temporal Alignment Methods (Importance: 90)
Three distinct alignment strategies:
- Audio-based cross-correlation for sound-driven synchronization
- Duration-based centering for simple timeline alignment
- Frame-based content matching for precise visual synchronization
- File: src/vidkompy/vidkompy_old.py

### Spatial Alignment Methods (Importance: 85)
Two precision-focused positioning approaches:
- Template matching for exact region identification
- Feature matching for compression-tolerant alignment
- File: src/vidkompy/vidkompy_old.py

### Quality Preservation Logic (Importance: 80)
- Foreground video quality preservation as primary directive
- Intelligent frame selection to maintain temporal integrity
- Dynamic background adaptation to match foreground timing
- File: src/vidkompy/vidkompy_old.py

### Audio Management (Importance: 75)
- Intelligent audio track selection between sources
- Audio-based synchronization when available
- Preservation of high-quality audio source
- File: src/vidkompy/vidkompy_old.py

## Workflow Integration

### Command Interface (Importance: 70)
- Intelligent default behaviors for minimal user intervention
- Automatic output filename generation
- Configurable alignment methods
- File: src/vidkompy/vidkompy_old.py

### Process Control (Importance: 65)
- Progress tracking and logging
- Error handling with user guidance
- Automatic fallback strategies
- File: src/vidkompy/vidkompy_old.py

$END$
END SPECIFICATION
</file>

<file path=".gitignore">
__pycache__/
__pypackages__/
._*
.cache
.coverage
.coverage.*
.cursorignore
.cursorindexingignore
.dmypy.json
.DS_Store
.eggs/
.env
.hypothesis/
.installed.cfg
.ipynb_checkpoints
.mypy_cache/
.nox/
.pdm-build/
.pdm-python
.pdm.toml
.pybuilder/
.pypirc
.pyre/
.pytest_cache/
.Python
.pytype/
.ropeproject
.ruff_cache/
.sass-cache
.scrapy
.specstory/.what-is-this.md
.Spotlight-V100
.spyderproject
.spyproject
.tox/
.Trashes
.venv
.webassets-cache
*.cover
*.egg
*.egg-info/
*.log
*.manifest
*.mo
*.out
*.pot
*.py,cover
*.py[cod]
*.pyc
*.sage.py
*.so
*.spec
*$py.class
/site
build/
celerybeat-schedule
celerybeat.pid
cover/
coverage.xml
cython_debug/
db.sqlite3
db.sqlite3-journal
Desktop.ini
develop-eggs/
dist/
dmypy.json
docs/_build/
downloads/
eggs/
env.bak/
env/
ENV/
htmlcov/
instance/
ipython_config.py
lib/
lib64/
local_settings.py
MANIFEST
node_modules
nosetests.xml
parts/
pip-delete-this-directory.txt
pip-log.txt
profile_default/
sdist/
share/python-wheels/
target/
Thumbs.db
var/
venv
venv.bak/
venv/
wheels/
</file>

<file path="CLAUDE.md">
## Coding style

<guidelines>
# When you write code

- Iterate gradually, avoiding major changes
- Minimize confirmations and checks
- Preserve existing code/structure unless necessary
- Use constants over magic numbers
- Check for existing solutions in the codebase before starting
- Check often the coherence of the code you’re writing with the rest of the code.
- Focus on minimal viable increments and ship early
- Write explanatory docstrings/comments that explain what and WHY this does, explain where and how the code is used/referred to elsewhere in the code
- Analyze code line-by-line
- Handle failures gracefully with retries, fallbacks, user guidance
- Address edge cases, validate assumptions, catch errors early
- Let the computer do the work, minimize user decisions
- Reduce cognitive load, beautify code
- Modularize repeated logic into concise, single-purpose functions
- Favor flat over nested structures
- Consistently keep, document, update and consult the holistic overview mental image of the codebase. 

Work in rounds: 

- Create `PROGRESS.md` as a detailed flat plan with `[ ]` items. 
- Identify the most important TODO items, and create `TODO.md` with `[ ]` items. 
- Implement the changes. 
- Update `PROGRESS.md` and `TODO.md` as you go. 
- After each round of changes, update `CHANGELOG.md` with the changes.
- Update `README.md` to reflect the changes.

## Keep track of paths

In each source file, maintain the up-to-date `this_file` record that shows the path of the current file relative to project root. Place the `this_file` record near the top of the file, as a comment after the shebangs, or in the YAML Markdown frontmatter.

## When you write Python

- Use `uv pip`, never `pip`
- Use `python -m` when running code
- PEP 8: Use consistent formatting and naming
- Write clear, descriptive names for functions and variables
- PEP 20: Keep code simple and explicit. Prioritize readability over cleverness
- Use type hints in their simplest form (list, dict, | for unions)
- PEP 257: Write clear, imperative docstrings
- Use f-strings. Use structural pattern matching where appropriate
- ALWAYS add "verbose" mode logugu-based logging, & debug-log
- For CLI Python scripts, use fire & rich, and start the script with

```
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["PKG1", "PKG2"]
# ///
# this_file: PATH_TO_CURRENT_FILE
```

Ask before extending/refactoring existing code in a way that may add complexity or break things.

When you’re finished, print "Wait, but" to go back, think & reflect, revise & improvement what you’ve done (but don’t invent functionality freely). Repeat this. But stick to the goal of "minimal viable next version". Lead two experts: "Ideot" for creative, unorthodox ideas, and "Critin" to critique flawed thinking and moderate for balanced discussions. The three of you shall illuminate knowledge with concise, beautiful responses, process methodically for clear answers, collaborate step-by-step, sharing thoughts and adapting. If errors are found, step back and focus on accuracy and progress.

## After Python changes run:

```
fd -e py -x autoflake {}; fd -e py -x pyupgrade --py312-plus {}; fd -e py -x ruff check --output-format=github --fix --unsafe-fixes {}; fd -e py -x ruff format --respect-gitignore --target-version py312 {}; python -m pytest;
```

Be creative, diligent, critical, relentless & funny!
</guidelines>


## Project Overview

vidkompy is an intelligent video overlay tool that overlays foreground videos onto background videos with automatic alignment. The project prioritizes 
precision and intelligence over raw speed.

`vidkompy` is a Python CLI (Fire) tool that lets me overlay two videos: --bg (background) and --fg (foreground), and saves the result to --output (name is built automatically if not provided).

The tool needs to be robust and smart.

1. The bg video can be larger than the fg video. In that case, the tool needs to automatically find the best offset to overlay the fg video on the bg video.

2. The bg and fg videos can have different FPSes. In that case, the tool needs to use the higher FPS for the output video.

3. The bg and fg videos may have different durations (the difference would be typically rather slight). Ths tool needs to find the optimal time offset to overlay the fg video on the bg video. We need to find "the perfect pair of frames that when overlaid will form the START frame of the output video", and "the perfect pair of frames that when overlaid will form the END frame of the output video". The output video will be trimmed to go from that start to that end. Some starting and/or ending frames from either the fg or the bg video may be dropped. 

4. The bg and fg videos may have sound (it would be identical, but may be offset). If both have sound, the sound may be used to find the right time offset for the overlay (if I specify --match_time audio). The fg sound would be is for the output video. If only one has sound, that sound is used for the output video.

5. The content of the two videos would typically be very similar but not identical, because one video is typically a recompressed, edited variant of the other.

6. Overall, the fg video is thought to be "better quality", and temporally properly aligned. The bg video can be streched, modified, re-timed, frames added or dropped etc. 

7. All these conditions may be combined. That means, the tool needs to be able to find both the spatial and the temporal offsets. Spatially, the fg video is never larger than the bg video. Temporally, either video can be longer.



## Key Commands

### Development & Testing
```bash
# Run tests
hatch run test

# Run tests with coverage
hatch run test-cov

# Type checking
hatch run type-check

# Linting and formatting
hatch run lint      # Check code
hatch run fix       # Auto-fix issues
hatch run fmt       # Format code

# All lint checks
hatch run lint:all
```

### Running the Tool
```bash
python src/vidkompy/vidkompy_old.py --help|cat

INFO: Showing help with the command 'vidkompy_old.py -- --help'.

NAME
    vidkompy_old.py - Overlay foreground video onto background video with intelligent alignment.

SYNOPSIS
    vidkompy_old.py BG FG <flags>

DESCRIPTION
    Overlay foreground video onto background video with intelligent alignment.

POSITIONAL ARGUMENTS
    BG
        Type: str
        Background video path
    FG
        Type: str
        Foreground video path

FLAGS
    -o, --output=OUTPUT
        Type: Optional[str | None]
        Default: None
        Output video path (auto-generated if not provided)
    --match_space=MATCH_SPACE
        Type: str
        Default: 'precise'
        Spatial alignment method ('precise' or 'fast')
    --temporal_align=TEMPORAL_ALIGN
        Type: str
        Default: 'frames'
        Temporal alignment method ('audio', 'duration', or 'frames') [default: 'frames']
    --trim=TRIM
        Type: bool
        Default: True
        Trim output to overlapping segments only [default: True]
    -s, --skip_spatial_align=SKIP_SPATIAL_ALIGN
        Type: bool
        Default: False
        Skip spatial alignment (use center)
    -v, --verbose=VERBOSE
        Type: bool
        Default: False
        Enable verbose logging
    --max_keyframes=MAX_KEYFRAMES
        Type: int
        Default: 2000
        Maximum number of keyframes to use in frames mode [default: 2000]

NOTES
    You can also use flags syntax for POSITIONAL ARGUMENTS


# Direct execution (main implementation in vidkompy_old.py)
python -m vidkompy --bg tests/bg.mp4 --fg tests/fg.mp4 -o tests/output_new.mp4 --match_time precise --verbose
```

## Architecture Notes

1. **Main Implementation**: The core functionality resides in `src/vidkompy/vidkompy_old.py` (60KB). 

The `vidkompy.py` file is a placeholder for future refactoring.

2. **Temporal Alignment Modes**:
   - `audio`: Cross-correlation of audio tracks
   - `duration`: Centers foreground within background duration
   - `frames`: Advanced frame-by-frame content matching with dynamic mapping

3. **Spatial Alignment Methods**:
   - `template` (precise, default): OpenCV template matching for exact sub-regions
   - `feature` (fast): ORB feature matching for more robust alignment

4. **Known Issue**: The `frames` temporal alignment method has drift-and-catchup issues where the background portion runs at varying speeds 

5. **Dependencies**: The tool requires FFmpeg binary installed separately. Python dependencies are listed in the script header and include opencv-python, ffmpeg-python, scipy, numpy, soundfile, scikit-image, fire, rich, and loguru.

## Testing Approach

The project uses pytest with minimal test coverage currently (only version check). When adding new functionality:
- Place tests in `tests/` directory
- Use pytest markers for categorization (unit, integration, benchmark)
- Run with `hatch run test` or `hatch run test-cov` for coverage

START SPECIFICATION:
---
description: Apply when documenting high-level business logic and domain-specific implementations for video overlay and alignment systems, specifically for intelligent video composition tools that require precise spatial and temporal synchronization
globs: *.py,*.md
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


vidkompy is an intelligent video overlay system focused on precision alignment of video content. The core business logic revolves around automatically synchronizing and compositing video streams with different characteristics.

## Core Business Components

### Video Alignment Engine (Importance: 95)
- Automatic detection of optimal spatial positioning for foreground overlay
- Temporal synchronization using frame content or audio signals
- Smart handling of FPS and duration differences between videos
- File: src/vidkompy/vidkompy_old.py

### Temporal Alignment Methods (Importance: 90)
Three distinct alignment strategies:
- Audio-based cross-correlation for sound-driven synchronization
- Duration-based centering for simple timeline alignment
- Frame-based content matching for precise visual synchronization
- File: src/vidkompy/vidkompy_old.py

### Spatial Alignment Methods (Importance: 85)
Two precision-focused positioning approaches:
- Template matching for exact region identification
- Feature matching for compression-tolerant alignment
- File: src/vidkompy/vidkompy_old.py

### Quality Preservation Logic (Importance: 80)
- Foreground video quality preservation as primary directive
- Intelligent frame selection to maintain temporal integrity
- Dynamic background adaptation to match foreground timing
- File: src/vidkompy/vidkompy_old.py

### Audio Management (Importance: 75)
- Intelligent audio track selection between sources
- Audio-based synchronization when available
- Preservation of high-quality audio source
- File: src/vidkompy/vidkompy_old.py

## Workflow Integration

### Command Interface (Importance: 70)
- Intelligent default behaviors for minimal user intervention
- Automatic output filename generation
- Configurable alignment methods
- File: src/vidkompy/vidkompy_old.py

### Process Control (Importance: 65)
- Progress tracking and logging
- Error handling with user guidance
- Automatic fallback strategies
- File: src/vidkompy/vidkompy_old.py

$END$
END SPECIFICATION
</file>

<file path="src/vidkompy/core/dtw_aligner.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/dtw_aligner.py

"""
Dynamic Time Warping (DTW) for video frame alignment.

This module implements DTW algorithm for finding the globally optimal
monotonic alignment between two video sequences, preventing temporal artifacts.
"""

import numpy as np
from collections.abc import Callable
from loguru import logger
from rich.progress import Progress, TextColumn, BarColumn, TimeRemainingColumn
from rich.console import Console

from vidkompy.models import FrameAlignment

console = Console()


class DTWAligner:
    """Dynamic Time Warping for video frame alignment.

    Why DTW over current greedy matching:
    - Guarantees monotonic alignment (no backward jumps)
    - Finds globally optimal path, not just local matches
    - Handles speed variations naturally
    - Proven algorithm from speech/time series analysis

    Why Sakoe-Chiba band constraint:
    - Reduces complexity from O(N²) to O(N×window)
    - Prevents extreme time warping
    - Makes algorithm practical for long videos
    """

    def __init__(self, window_constraint: int = 100):
        """Initialize DTW aligner with constraints.

        Args:
            window_constraint: Maximum deviation from diagonal path
                              (Sakoe-Chiba band width). Set to 0 to use default.
        """
        self.window = window_constraint
        self.default_window = 100

    def set_window(self, window: int):
        """Set the window constraint for DTW alignment.

        Args:
            window: Window size for sliding frame matching. 0 means use default.
        """
        if window > 0:
            self.window = window
        else:
            self.window = self.default_window

    def align_videos(
        self,
        fg_fingerprints: dict[int, dict[str, np.ndarray]],
        bg_fingerprints: dict[int, dict[str, np.ndarray]],
        fingerprint_compare_func: Callable,
        show_progress: bool = True,
    ) -> list[tuple[int, int, float]]:
        """Find optimal monotonic alignment using DTW.

        Args:
            fg_fingerprints: Foreground video fingerprints {frame_idx: fingerprint}
            bg_fingerprints: Background video fingerprints {frame_idx: fingerprint}
            fingerprint_compare_func: Function to compare two fingerprints (0-1 similarity)
            show_progress: Whether to show progress bar

        Returns:
            List of (bg_idx, fg_idx, confidence) tuples representing optimal alignment

        Why this approach:
        - Works on pre-computed fingerprints for speed
        - Returns confidence scores for quality assessment
        - Maintains fg frame order (as required)
        """
        fg_indices = sorted(fg_fingerprints.keys())
        bg_indices = sorted(bg_fingerprints.keys())

        n_fg = len(fg_indices)
        n_bg = len(bg_indices)

        logger.info(
            f"Starting DTW alignment: {n_fg} fg frames × {n_bg} bg frames, "
            f"window={self.window}"
        )

        # Build DTW cost matrix
        dtw_matrix = self._build_dtw_matrix(
            fg_indices,
            bg_indices,
            fg_fingerprints,
            bg_fingerprints,
            fingerprint_compare_func,
            show_progress,
        )

        # Find optimal path
        path = self._find_optimal_path(dtw_matrix, n_fg, n_bg)

        # Convert path to frame alignments with confidence scores
        alignments = self._path_to_alignments(
            path,
            fg_indices,
            bg_indices,
            fg_fingerprints,
            bg_fingerprints,
            fingerprint_compare_func,
        )

        logger.info(f"DTW completed: {len(alignments)} frame alignments")

        return alignments

    def _build_dtw_matrix(
        self,
        fg_indices: list[int],
        bg_indices: list[int],
        fg_fingerprints: dict[int, dict[str, np.ndarray]],
        bg_fingerprints: dict[int, dict[str, np.ndarray]],
        compare_func: Callable,
        show_progress: bool,
    ) -> np.ndarray:
        """Build DTW cost matrix with Sakoe-Chiba band constraint.

        Why band constraint:
        - Prevents extreme time warping
        - Reduces computation from O(N²) to O(N×window)
        - Enforces reasonable temporal alignment
        """
        n_fg = len(fg_indices)
        n_bg = len(bg_indices)

        # Initialize with infinity
        dtw = np.full((n_fg + 1, n_bg + 1), np.inf)
        dtw[0, 0] = 0

        # Progress tracking
        if show_progress:
            progress = Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TimeRemainingColumn(),
                console=console,
                transient=True,
            )
            task = progress.add_task("  Building DTW matrix...", total=n_fg)
            progress.start()

        # Fill DTW matrix with band constraint
        for i in range(1, n_fg + 1):
            # Sakoe-Chiba band: only compute within window of diagonal
            j_start = max(1, i - self.window)
            j_end = min(n_bg + 1, i + self.window)

            for j in range(j_start, j_end):
                # Get fingerprints
                fg_fp = fg_fingerprints[fg_indices[i - 1]]
                bg_fp = bg_fingerprints[bg_indices[j - 1]]

                # Compute cost (1 - similarity)
                similarity = compare_func(fg_fp, bg_fp)
                cost = 1.0 - similarity

                # DTW recursion: min of three possible paths
                dtw[i, j] = cost + min(
                    dtw[i - 1, j],  # Skip bg frame (insertion)
                    dtw[i, j - 1],  # Skip fg frame (deletion)
                    dtw[i - 1, j - 1],  # Match frames
                )

            if show_progress:
                progress.update(task, advance=1)

        if show_progress:
            progress.stop()

        return dtw

    def _find_optimal_path(
        self, dtw: np.ndarray, n_fg: int, n_bg: int
    ) -> list[tuple[int, int]]:
        """Backtrack through DTW matrix to find optimal path.

        Why backtracking:
        - Recovers the actual alignment from cost matrix
        - Guarantees monotonic path
        - Handles insertions/deletions/matches
        """
        path = []
        i, j = n_fg, n_bg

        # Backtrack from end to start
        while i > 0 or j > 0:
            path.append((i, j))

            if i == 0:
                j -= 1
            elif j == 0:
                i -= 1
            else:
                # Choose direction with minimum cost
                costs = [
                    (i - 1, j, dtw[i - 1, j]),  # From above
                    (i, j - 1, dtw[i, j - 1]),  # From left
                    (i - 1, j - 1, dtw[i - 1, j - 1]),  # From diagonal
                ]

                # Filter out invalid positions
                valid_costs = [
                    (pi, pj, cost) for pi, pj, cost in costs if cost != np.inf
                ]

                if valid_costs:
                    i, j, _ = min(valid_costs, key=lambda x: x[2])
                else:
                    # Fallback: move diagonally
                    i, j = i - 1, j - 1

        # Reverse to get forward path
        path.reverse()

        # Remove dummy start position
        if path and path[0] == (0, 0):
            path = path[1:]

        return path

    def _path_to_alignments(
        self,
        path: list[tuple[int, int]],
        fg_indices: list[int],
        bg_indices: list[int],
        fg_fingerprints: dict[int, dict[str, np.ndarray]],
        bg_fingerprints: dict[int, dict[str, np.ndarray]],
        compare_func: Callable,
    ) -> list[tuple[int, int, float]]:
        """Convert DTW path to frame alignments with confidence scores.

        Why confidence scores:
        - Helps identify problematic alignments
        - Enables quality-based filtering
        - Provides feedback for debugging
        """
        alignments = []

        for i, j in path:
            # Skip boundary positions
            if i == 0 or j == 0:
                continue

            # Get actual frame indices
            fg_idx = fg_indices[i - 1]
            bg_idx = bg_indices[j - 1]

            # Compute confidence as similarity score
            fg_fp = fg_fingerprints[fg_idx]
            bg_fp = bg_fingerprints[bg_idx]
            confidence = compare_func(fg_fp, bg_fp)

            alignments.append((bg_idx, fg_idx, confidence))

        # Remove duplicate fg frames (keep best match)
        unique_alignments = {}
        for bg_idx, fg_idx, conf in alignments:
            if fg_idx not in unique_alignments or conf > unique_alignments[fg_idx][1]:
                unique_alignments[fg_idx] = (bg_idx, conf)

        # Convert back to list format
        final_alignments = [
            (bg_idx, fg_idx, conf)
            for fg_idx, (bg_idx, conf) in sorted(unique_alignments.items())
        ]

        return final_alignments

    def create_frame_alignments(
        self,
        dtw_matches: list[tuple[int, int, float]],
        total_fg_frames: int,
        total_bg_frames: int,
    ) -> list[FrameAlignment]:
        """Create complete frame-to-frame alignment from DTW matches.

        Args:
            dtw_matches: List of (bg_idx, fg_idx, confidence) from DTW
            total_fg_frames: Total number of foreground frames
            total_bg_frames: Total number of background frames

        Returns:
            List of FrameAlignment objects for every fg frame

        Why interpolation:
        - DTW may skip some frames
        - Need alignment for EVERY fg frame
        - Smooth interpolation prevents jumps
        """
        if not dtw_matches:
            # Fallback to simple linear mapping
            return self._create_linear_alignment(total_fg_frames, total_bg_frames)

        # Sort by fg index
        dtw_matches.sort(key=lambda x: x[1])

        # Create alignment for every fg frame
        alignments = []

        for fg_idx in range(total_fg_frames):
            # Find surrounding DTW matches
            prev_match = None
            next_match = None

            for match in dtw_matches:
                if match[1] <= fg_idx:
                    prev_match = match
                elif match[1] > fg_idx and next_match is None:
                    next_match = match
                    break

            # Interpolate or extrapolate
            if prev_match is None and next_match is None:
                # No matches at all - use linear
                bg_idx = int(fg_idx * total_bg_frames / total_fg_frames)
                confidence = 0.5
            elif prev_match is None:
                # Before first match - extrapolate
                bg_idx = max(0, next_match[0] - (next_match[1] - fg_idx))
                confidence = next_match[2] * 0.8
            elif next_match is None:
                # After last match - extrapolate
                bg_idx = min(
                    total_bg_frames - 1, prev_match[0] + (fg_idx - prev_match[1])
                )
                confidence = prev_match[2] * 0.8
            # Between matches - interpolate
            elif prev_match[1] == next_match[1]:
                # Same fg frame
                bg_idx = prev_match[0]
                confidence = prev_match[2]
            else:
                # Linear interpolation
                ratio = (fg_idx - prev_match[1]) / (next_match[1] - prev_match[1])
                bg_idx = int(prev_match[0] + ratio * (next_match[0] - prev_match[0]))
                confidence = prev_match[2] * (1 - ratio) + next_match[2] * ratio

            # Ensure valid bg index
            bg_idx = max(0, min(bg_idx, total_bg_frames - 1))

            alignments.append(
                FrameAlignment(
                    fg_frame_idx=fg_idx,
                    bg_frame_idx=bg_idx,
                    similarity_score=confidence,
                )
            )

        return alignments

    def _create_linear_alignment(
        self, total_fg_frames: int, total_bg_frames: int
    ) -> list[FrameAlignment]:
        """Create simple linear frame mapping as fallback.

        Why we need fallback:
        - DTW might fail on very dissimilar videos
        - Better than no alignment at all
        - Maintains temporal order
        """
        ratio = total_bg_frames / total_fg_frames if total_fg_frames > 0 else 1.0

        alignments = []
        for fg_idx in range(total_fg_frames):
            bg_idx = min(int(fg_idx * ratio), total_bg_frames - 1)

            alignments.append(
                FrameAlignment(
                    fg_frame_idx=fg_idx,
                    bg_frame_idx=bg_idx,
                    similarity_score=0.5,  # Unknown confidence
                )
            )

        return alignments
</file>

<file path="src/vidkompy/core/spatial_alignment.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/spatial_alignment.py

"""
Spatial alignment module for finding optimal overlay positions.

Implements template matching and feature-based alignment methods.
"""

import cv2
import numpy as np
from loguru import logger

from vidkompy.models import SpatialAlignment


class SpatialAligner:
    """Handles spatial alignment of foreground on background frames.

    This module finds the optimal position to place the foreground video
    within the background video frame.

    Why spatial alignment is important:
    - Videos might be cropped differently
    - One video might be a subset/window of the other
    - Automatic alignment avoids manual positioning
    - Ensures content overlap for best visual result
    """

    def align(
        self,
        bg_frame: np.ndarray,
        fg_frame: np.ndarray,
        method: str = "precise",
        skip_alignment: bool = False,
    ) -> SpatialAlignment:
        """Find optimal position for foreground on background.

        Args:
            bg_frame: Background frame
            fg_frame: Foreground frame
            method: Alignment method ('precise'/'template' or 'fast'/'feature')
            skip_alignment: If True, just center the foreground

        Returns:
            SpatialAlignment with offset and scale
        """
        bg_h, bg_w = bg_frame.shape[:2]
        fg_h, fg_w = fg_frame.shape[:2]

        # Check if scaling needed
        scale_factor = 1.0
        if fg_w > bg_w or fg_h > bg_h:
            scale_factor = min(bg_w / fg_w, bg_h / fg_h)
            logger.warning(
                f"Foreground larger than background, scaling by {scale_factor:.3f}"
            )

            # Scale foreground
            new_w = int(fg_w * scale_factor)
            new_h = int(fg_h * scale_factor)
            fg_frame = cv2.resize(
                fg_frame, (new_w, new_h), interpolation=cv2.INTER_AREA
            )
            fg_h, fg_w = new_h, new_w

        if skip_alignment:
            # Center alignment
            x_offset = (bg_w - fg_w) // 2
            y_offset = (bg_h - fg_h) // 2
            return SpatialAlignment(
                x_offset=x_offset,
                y_offset=y_offset,
                scale_factor=scale_factor,
                confidence=1.0,
            )

        # Perform alignment
        if method in ["precise", "template"]:
            return self._template_matching(bg_frame, fg_frame, scale_factor)
        elif method in ["fast", "feature"]:
            return self._feature_matching(bg_frame, fg_frame, scale_factor)
        else:
            logger.warning(f"Unknown spatial method {method}, using center alignment")
            x_offset = (bg_w - fg_w) // 2
            y_offset = (bg_h - fg_h) // 2
            return SpatialAlignment(
                x_offset=x_offset,
                y_offset=y_offset,
                scale_factor=scale_factor,
                confidence=0.5,
            )

    def _template_matching(
        self, bg_frame: np.ndarray, fg_frame: np.ndarray, scale_factor: float
    ) -> SpatialAlignment:
        """Find best position using template matching.

        Uses normalized cross-correlation to find the best match.

        Why template matching:
        - Works perfectly when FG is an exact subset of BG
        - Very fast for exact matches
        - High confidence scores for good matches
        - OpenCV implementation is highly optimized

        Why we use grayscale:
        - 3x faster than color matching
        - More robust to color shifts
        - Structural alignment matters more than color

        Limitations:
        - Fails if videos have different compression/artifacts
        - Sensitive to brightness/contrast changes
        - Requires FG to be subset of BG
        """
        logger.debug("Using template matching for spatial alignment")

        # Convert to grayscale
        bg_gray = cv2.cvtColor(bg_frame, cv2.COLOR_BGR2GRAY)
        fg_gray = cv2.cvtColor(fg_frame, cv2.COLOR_BGR2GRAY)

        # Apply template matching
        result = cv2.matchTemplate(bg_gray, fg_gray, cv2.TM_CCOEFF_NORMED)

        # Find best match
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)

        # Top-left corner of match
        x_offset, y_offset = max_loc

        logger.info(
            f"Template match found at ({x_offset}, {y_offset}) "
            f"with confidence {max_val:.3f}"
        )

        return SpatialAlignment(
            x_offset=x_offset,
            y_offset=y_offset,
            scale_factor=scale_factor,
            confidence=float(max_val),
        )

    def _feature_matching(
        self, bg_frame: np.ndarray, fg_frame: np.ndarray, scale_factor: float
    ) -> SpatialAlignment:
        """Find alignment using feature matching (ORB).

        More robust to changes but potentially less precise.

        Why feature matching:
        - Works even with compression artifacts
        - Handles brightness/contrast changes
        - Can match videos with slight differences
        - Robust to noise and distortions

        Why ORB (Oriented FAST and Rotated BRIEF):
        - Patent-free alternative to SIFT/SURF
        - Very fast computation
        - Good balance of speed and accuracy
        - Rotation invariant

        Why homography:
        - Handles perspective changes
        - Can detect more complex transformations
        - RANSAC removes outlier matches

        Limitations:
        - Less precise than template matching for exact subsets
        - Requires textured content (fails on uniform areas)
        - May produce false matches needing validation
        """
        logger.debug("Using feature matching for spatial alignment")

        # Convert to grayscale
        bg_gray = cv2.cvtColor(bg_frame, cv2.COLOR_BGR2GRAY)
        fg_gray = cv2.cvtColor(fg_frame, cv2.COLOR_BGR2GRAY)

        # Initialize ORB detector
        orb = cv2.ORB_create(nfeatures=500)

        # Find keypoints and descriptors
        kp1, des1 = orb.detectAndCompute(bg_gray, None)
        kp2, des2 = orb.detectAndCompute(fg_gray, None)

        if des1 is None or des2 is None or len(des1) < 4 or len(des2) < 4:
            logger.warning("Not enough features for matching, using center alignment")
            bg_h, bg_w = bg_frame.shape[:2]
            fg_h, fg_w = fg_frame.shape[:2]
            return SpatialAlignment(
                x_offset=(bg_w - fg_w) // 2,
                y_offset=(bg_h - fg_h) // 2,
                scale_factor=scale_factor,
                confidence=0.3,
            )

        # Match features
        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        matches = bf.match(des1, des2)

        if len(matches) < 4:
            logger.warning("Not enough matches, using center alignment")
            bg_h, bg_w = bg_frame.shape[:2]
            fg_h, fg_w = fg_frame.shape[:2]
            return SpatialAlignment(
                x_offset=(bg_w - fg_w) // 2,
                y_offset=(bg_h - fg_h) // 2,
                scale_factor=scale_factor,
                confidence=0.3,
            )

        # Sort matches by distance
        matches = sorted(matches, key=lambda x: x.distance)

        # Extract matched points
        src_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)

        # Find homography
        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

        if M is None:
            logger.warning("Failed to find homography, using center alignment")
            bg_h, bg_w = bg_frame.shape[:2]
            fg_h, fg_w = fg_frame.shape[:2]
            return SpatialAlignment(
                x_offset=(bg_w - fg_w) // 2,
                y_offset=(bg_h - fg_h) // 2,
                scale_factor=scale_factor,
                confidence=0.3,
            )

        # Transform corner points
        h, w = fg_gray.shape
        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(
            -1, 1, 2
        )
        dst = cv2.perspectiveTransform(pts, M)

        # Calculate offset from top-left corner
        x_offset = int(dst[0][0][0])
        y_offset = int(dst[0][0][1])

        # Calculate confidence based on inliers
        matches_mask = mask.ravel().tolist() if mask is not None else []
        inlier_ratio = sum(matches_mask) / len(matches_mask) if matches_mask else 0

        logger.info(
            f"Feature match found at ({x_offset}, {y_offset}) "
            f"with {sum(matches_mask)} inliers ({inlier_ratio:.2%})"
        )

        return SpatialAlignment(
            x_offset=x_offset,
            y_offset=y_offset,
            scale_factor=scale_factor,
            confidence=inlier_ratio,
        )
</file>

<file path="src/vidkompy/core/video_processor.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/video_processor.py

"""
Core video processing functionality.

Handles video I/O, metadata extraction, and frame operations.
"""

import cv2
import ffmpeg
import numpy as np
from pathlib import Path
from loguru import logger
from rich.progress import (
    Progress,
    TextColumn,
    BarColumn,
    TimeRemainingColumn,
)
from rich.console import Console

from vidkompy.models import VideoInfo

console = Console()


class VideoProcessor:
    """Handles core video processing operations.

    This module provides the foundation for all video I/O operations.
    It abstracts away the complexity of video codecs and formats.

    Why separate video processing:
    - Isolates platform-specific code
    - Makes testing easier (can mock video I/O)
    - Single place for optimization
    - Handles codec compatibility issues

    Why both OpenCV and FFmpeg:
    - OpenCV: Frame-accurate reading, computer vision operations
    - FFmpeg: Audio handling, codec support, fast encoding
    """

    def get_video_info(self, video_path: str) -> VideoInfo:
        """Extract video metadata using ffprobe.

        Why ffprobe instead of OpenCV:
        - More reliable metadata extraction
        - Handles all video formats
        - Provides accurate duration/framerate
        - Detects audio streams properly

        Why we need this info:
        - FPS determines temporal alignment strategy
        - Resolution needed for spatial alignment
        - Duration for progress estimation
        - Audio presence for alignment method selection

        Args:
            video_path: Path to video file

        Returns:
            VideoInfo object with metadata

        Raises:
            ValueError: If video cannot be probed
        """
        logger.debug(f"Probing video: {video_path}")

        try:
            probe = ffmpeg.probe(video_path)

            # Find video stream
            video_stream = next(
                (s for s in probe["streams"] if s["codec_type"] == "video"), None
            )

            if not video_stream:
                msg = f"No video stream found in {video_path}"
                raise ValueError(msg)

            # Extract properties
            width = int(video_stream["width"])
            height = int(video_stream["height"])

            # Parse frame rate
            fps_str = video_stream.get("r_frame_rate", "0/1")
            if "/" in fps_str:
                num, den = map(int, fps_str.split("/"))
                fps = num / den if den != 0 else 0
            else:
                fps = float(fps_str)

            duration = float(probe["format"].get("duration", 0))

            # Calculate frame count
            frame_count = int(video_stream.get("nb_frames", 0))
            if frame_count == 0 and duration > 0 and fps > 0:
                frame_count = int(duration * fps)

            # Check audio
            audio_stream = next(
                (s for s in probe["streams"] if s["codec_type"] == "audio"), None
            )

            has_audio = audio_stream is not None
            audio_sample_rate = None
            audio_channels = None

            if audio_stream:
                audio_sample_rate = int(audio_stream.get("sample_rate", 0))
                audio_channels = int(audio_stream.get("channels", 0))

            info = VideoInfo(
                width=width,
                height=height,
                fps=fps,
                duration=duration,
                frame_count=frame_count,
                has_audio=has_audio,
                audio_sample_rate=audio_sample_rate,
                audio_channels=audio_channels,
                path=video_path,
            )

            logger.info(
                f"Video info for {Path(video_path).name}: "
                f"{width}x{height}, {fps:.2f} fps, {duration:.2f}s, "
                f"{frame_count} frames, audio: {'yes' if has_audio else 'no'}"
            )

            return info

        except Exception as e:
            logger.error(f"Failed to probe video {video_path}: {e}")
            raise

    def extract_frames(
        self, video_path: str, frame_indices: list[int], resize_factor: float = 1.0
    ) -> list[np.ndarray]:
        """Extract specific frames from video.

        Why selective frame extraction:
        - Loading full video would exhaust memory
        - We only need specific frames for matching
        - Random access is fast with modern codecs

        Why resize option:
        - Faster processing on smaller frames
        - SSIM works fine at lower resolution
        - Reduces memory usage significantly

        Why progress bar for large extractions:
        - Frame seeking can be slow on some codecs
        - Users need feedback for long operations
        - Helps identify performance issues

        Args:
            video_path: Path to video file
            frame_indices: List of frame indices to extract
            resize_factor: Factor to resize frames (for performance)

        Returns:
            List of frames as numpy arrays
        """
        frames = []
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            logger.error(f"Failed to open video: {video_path}")
            return frames

        try:
            # Only show progress for large frame extractions
            if len(frame_indices) > 50:
                with Progress(
                    TextColumn("[progress.description]{task.description}"),
                    BarColumn(),
                    TimeRemainingColumn(),
                    console=console,
                    transient=True,
                ) as progress:
                    task = progress.add_task(
                        f"    Extracting {len(frame_indices)} frames...",
                        total=len(frame_indices),
                    )

                    for idx in frame_indices:
                        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                        ret, frame = cap.read()

                        if ret:
                            if resize_factor != 1.0:
                                height, width = frame.shape[:2]
                                new_width = int(width * resize_factor)
                                new_height = int(height * resize_factor)
                                frame = cv2.resize(frame, (new_width, new_height))
                            frames.append(frame)
                        else:
                            logger.warning(
                                f"Failed to read frame {idx} from {video_path}"
                            )

                        progress.update(task, advance=1)
            else:
                # No progress bar for small extractions
                for idx in frame_indices:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                    ret, frame = cap.read()

                    if ret:
                        if resize_factor != 1.0:
                            height, width = frame.shape[:2]
                            new_width = int(width * resize_factor)
                            new_height = int(height * resize_factor)
                            frame = cv2.resize(frame, (new_width, new_height))
                        frames.append(frame)
                    else:
                        logger.warning(f"Failed to read frame {idx} from {video_path}")

        finally:
            cap.release()

        return frames

    def extract_frame_range(
        self,
        video_path: str,
        start_frame: int,
        end_frame: int,
        step: int = 1,
        resize_factor: float = 1.0,
    ) -> list[tuple[int, np.ndarray]]:
        """Extract a range of frames with their indices.

        Args:
            video_path: Path to video
            start_frame: Starting frame index
            end_frame: Ending frame index (exclusive)
            step: Frame step size
            resize_factor: Resize factor for frames

        Returns:
            List of (frame_index, frame) tuples
        """
        frames = []
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            logger.error(f"Failed to open video: {video_path}")
            return frames

        try:
            for idx in range(start_frame, end_frame, step):
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()

                if ret:
                    if resize_factor != 1.0:
                        height, width = frame.shape[:2]
                        new_width = int(width * resize_factor)
                        new_height = int(height * resize_factor)
                        frame = cv2.resize(frame, (new_width, new_height))
                    frames.append((idx, frame))
                else:
                    break

        finally:
            cap.release()

        return frames

    def extract_audio(
        self, video_path: str, output_path: str, sample_rate: int = 16000
    ) -> bool:
        """Extract audio from video to WAV file.

        Args:
            video_path: Input video path
            output_path: Output WAV path
            sample_rate: Target sample rate

        Returns:
            True if extraction successful
        """
        logger.debug(f"Extracting audio from {video_path}")

        try:
            stream = ffmpeg.input(video_path)
            stream = ffmpeg.output(
                stream,
                output_path,
                acodec="pcm_s16le",
                ac=1,  # Mono
                ar=sample_rate,
                loglevel="error",
            )
            ffmpeg.run(stream, overwrite_output=True)
            return True

        except ffmpeg.Error as e:
            logger.error(f"Audio extraction failed: {e.stderr.decode()}")
            return False

    def create_video_writer(
        self, output_path: str, width: int, height: int, fps: float, codec: str = "mp4v"
    ) -> cv2.VideoWriter:
        """Create OpenCV video writer.

        Why H.264/mp4v codec:
        - Universal compatibility
        - Good compression ratio
        - Hardware acceleration available
        - Supports high resolutions

        Why we write silent video first:
        - OpenCV VideoWriter doesn't handle audio
        - Gives us perfect frame control
        - Audio added later with FFmpeg

        Args:
            output_path: Output video path
            width: Video width
            height: Video height
            fps: Frame rate
            codec: Video codec (default mp4v)

        Returns:
            VideoWriter object
        """
        fourcc = cv2.VideoWriter_fourcc(*codec)
        writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        if not writer.isOpened():
            msg = f"Failed to create video writer for {output_path}"
            raise ValueError(msg)

        return writer
</file>

<file path="tests/test_package.py">
"""Test suite for vidkompy."""


def test_version():
    """Verify package exposes version."""
    import vidkompy

    assert vidkompy.__version__
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to vidkompy will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Fixed
- **Critical Performance**: Fixed compositing bottleneck by implementing sequential reading with generators
- **UI Bug**: Resolved "Only one live display may be active at once" error from nested Progress contexts
- **Compositing Speed**: Eliminated costly random seeks in video files during frame composition
- **Progress UX**: Removed annoying spinner displays for quick operations

### Added
- Sequential frame generators for optimal video reading performance
- Detailed frame composition progress bar with percentage, frame count, and time remaining
- Spatial alignment results now logged in non-verbose mode for better visibility
- Temporal alignment results now logged in non-verbose mode showing method, offset, and frame count
- Comprehensive docstrings explaining the "why" behind design decisions in all core modules
- SPEC4.md: Detailed performance improvement plan with DTW algorithm and perceptual hashing
- Border-based temporal alignment mode with mask generation
- Smooth alpha blending for frame edges
- Sliding window constraint for frame matching optimization

### Changed
- **Performance**: Compositing now uses forward-only sequential reads instead of random seeks (10-100x speedup)
- **Performance**: Significantly reduced keyframe sampling when using SSIM (e.g., in border mode fallback), drastically improving speed for that specific path.
- **Progress UX**: Quick tasks (video analysis, spatial alignment) now use simple logging instead of spinners
- **Progress Bars**: Frame composition shows meaningful progress bar instead of percentage logging
- **Default Mode**: Border-based temporal alignment is now the default for improved accuracy
- Progress bars now show time remaining for better user experience
- Maintained useful progress bars for time-intensive operations (DTW, cost matrix building)

### Documentation
- Added detailed docstrings to alignment_engine.py explaining architecture decisions
- Added detailed docstrings to spatial_alignment.py explaining algorithm choices
- Added detailed docstrings to temporal_alignment.py explaining current limitations
- Added detailed docstrings to video_processor.py explaining tool choices
- Created SPEC4.md with comprehensive improvement plan addressing performance and quality issues

### Technical Details
- Removed SpinnerColumn from inner progress bars to prevent conflicts
- Added TimeRemainingColumn for better progress estimation
- Made outer progress transient to reduce visual clutter
</file>

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(fd:*)",
      "Bash(chmod:*)",
      "Bash(python:*)",
      "Bash(ls:*)",
      "Bash(ffprobe:*)",
      "Bash(cp:*)",
      "Bash(rm:*)",
      "Bash(mkdir:*)",
      "Bash(grep:*)",
      "Bash(time python:*)",
      "Bash(find:*)"
    ],
    "deny": []
  }
}
</file>

<file path="src/vidkompy/models.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/models.py

"""
Data models and enums for vidkompy.

Contains all shared data structures used across the application.
"""

from dataclasses import dataclass
from enum import Enum


class MatchTimeMode(Enum):
    """Temporal alignment modes."""

    BORDER = "border"  # Border-based matching (new default)
    FAST = "fast"  # Try audio first, fallback to frames
    PRECISE = "precise"  # Use frame-based matching


class SpatialMethod(Enum):
    """Spatial alignment methods."""

    TEMPLATE = "template"  # Precise template matching
    FEATURE = "feature"  # Fast feature-based matching
    CENTER = "center"  # Simple center alignment


class TemporalMethod(Enum):
    """Temporal alignment algorithm methods."""

    DTW = "dtw"  # Dynamic Time Warping (new default)
    CLASSIC = "classic"  # Original keyframe matching
    FRAMES = "frames"  # Legacy alias for classic


@dataclass
class VideoInfo:
    """Video metadata container."""

    width: int
    height: int
    fps: float
    duration: float
    frame_count: int
    has_audio: bool
    audio_sample_rate: int | None = None
    audio_channels: int | None = None
    path: str = ""

    @property
    def resolution(self) -> tuple[int, int]:
        """Get video resolution as (width, height)."""
        return (self.width, self.height)

    @property
    def aspect_ratio(self) -> float:
        """Calculate aspect ratio."""
        return self.width / self.height if self.height > 0 else 0


@dataclass
class FrameAlignment:
    """Represents alignment between a foreground and background frame."""

    fg_frame_idx: int  # Foreground frame index (never changes)
    bg_frame_idx: int  # Corresponding background frame index
    similarity_score: float  # Similarity between frames (0-1)

    def __repr__(self) -> str:
        return f"FrameAlignment(fg={self.fg_frame_idx}, bg={self.bg_frame_idx}, sim={self.similarity_score:.3f})"


@dataclass
class SpatialAlignment:
    """Spatial offset for overlaying foreground on background."""

    x_offset: int
    y_offset: int
    scale_factor: float = 1.0
    confidence: float = 1.0

    @property
    def offset(self) -> tuple[int, int]:
        """Get offset as tuple."""
        return (self.x_offset, self.y_offset)


@dataclass
class TemporalAlignment:
    """Temporal alignment results."""

    offset_seconds: float  # Time offset in seconds
    frame_alignments: list[FrameAlignment]  # Frame-by-frame mapping
    method_used: str  # Method that produced this alignment
    confidence: float = 1.0

    @property
    def start_frame(self) -> int | None:
        """Get first aligned foreground frame."""
        return self.frame_alignments[0].fg_frame_idx if self.frame_alignments else None

    @property
    def end_frame(self) -> int | None:
        """Get last aligned foreground frame."""
        return self.frame_alignments[-1].fg_frame_idx if self.frame_alignments else None


@dataclass
class ProcessingOptions:
    """Options for video processing."""

    time_mode: MatchTimeMode
    space_method: str
    skip_spatial: bool
    trim: bool
    max_keyframes: int = 2000
    verbose: bool = False
    border_thickness: int = 8
    blend: bool = False
    window: int = 0
</file>

<file path="pyproject.toml">
[project]
name = 'vidkompy'
dynamic = ['version']
description = ''
readme = 'README.md'
requires-python = '>=3.10'
license = 'MIT'
keywords = []
classifiers = [
    'Development Status :: 4 - Beta',
    'Programming Language :: Python',
    'Programming Language :: Python :: 3.10',
    'Programming Language :: Python :: 3.11',
    'Programming Language :: Python :: 3.12',
    'Programming Language :: Python :: Implementation :: CPython',
    'Programming Language :: Python :: Implementation :: PyPy',
]
dependencies = [
    'fire',
    'rich',
    'loguru',
    'opencv-contrib-python',
    'numpy',
    'scipy',
    'ffmpeg-python',
    'soundfile',
    'scikit-image',
]

[[project.authors]]
name = 'Adam Twardoch'
email = 'adam+github@twardoch.com'

[project.optional-dependencies]
dev = [
    'pre-commit>=4.2.0',
    'ruff>=0.1.0',
    'mypy>=1.0.0',
    'pyupgrade>=3.19.0',
]
test = [
    'pytest>=8.3.5',
    'pytest-cov>=6.1.1',
]
all = [
    'fire',
    'rich',
    'loguru',
    'opencv-contrib-python',
    'numpy',
    'scipy',
    'ffmpeg-python',
    'soundfile',
    'scikit-image',
    'pre-commit>=4.2.0',
    'ruff>=0.1.0',
    'mypy>=1.0.0',
    'pyupgrade>=3.19.0',
    'pytest>=8.3.5',
    'pytest-cov>=6.1.1',
    'hatchling>=1.21.0',
    'hatch-vcs>=0.3.0',
]

[project.scripts]

[project.urls]
Documentation = 'https://github.com/twardoch/vidkompy#readme'
Issues = 'https://github.com/twardoch/vidkompy/issues'
Source = 'https://github.com/twardoch/vidkompy'

[build-system]
build-backend = 'hatchling.build'
requires = [
    'hatchling>=1.21.0',
    'hatch-vcs>=0.3.0',
]
[tool.coverage.paths]
vidkompy = [
    'src/vidkompy',
    '*/vidkompy/src/vidkompy',
]
tests = [
    'tests',
    '*/vidkompy/tests',
]

[tool.coverage.report]
exclude_lines = [
    'no cov',
    'if __name__ == .__main__.:',
    'if TYPE_CHECKING:',
]

[tool.coverage.run]
source_pkgs = [
    'vidkompy',
    'tests',
]
branch = true
parallel = true
omit = ['src/vidkompy/__about__.py']
[tool.hatch.build.hooks.vcs]
version-file = 'src/vidkompy/__version__.py'
[tool.hatch.build.targets.wheel]
packages = ['src/vidkompy']
[tool.hatch.envs.default]
dependencies = []

[tool.hatch.envs.default.scripts]
test = 'pytest {args:tests}'
test-cov = 'pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/vidkompy --cov=tests {args:tests}'
type-check = 'mypy src/vidkompy tests'
lint = [
    'ruff check src/vidkompy tests',
    'ruff format --respect-gitignore src/vidkompy tests',
]
fix = [
    'ruff check  --fix --unsafe-fixes src/vidkompy tests',
    'ruff format --respect-gitignore src/vidkompy tests',
]
[[tool.hatch.envs.all.matrix]]
python = [
    '3.10',
    '3.11',
    '3.12',
]

[tool.hatch.envs.lint]
detached = true
dependencies = []

[tool.hatch.envs.lint.scripts]
typing = 'mypy --install-types --non-interactive {args:src/vidkompy tests}'
style = [
    'ruff check {args:.}',
    'ruff format --respect-gitignore {args:.}',
]
fmt = [
    'ruff format --respect-gitignore {args:.}',
    'ruff check --fix {args:.}',
]
all = [
    'style',
    'typing',
]

[tool.hatch.envs.test]
dependencies = []

[tool.hatch.envs.test.scripts]
test = 'python -m pytest -n auto -p no:briefcase {args:tests}'
test-cov = 'python -m pytest -n auto -p no:briefcase --cov-report=term-missing --cov-config=pyproject.toml --cov=src/vidkompy --cov=tests {args:tests}'
bench = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only'
bench-save = 'python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json'

[tool.hatch.version]
source = 'vcs'

[tool.hatch.version.raw-options]
version_scheme = 'post-release'

[tool.mypy]
python_version = '3.10'
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[tool.ruff]
target-version = 'py310'
line-length = 88

[tool.ruff.lint]
extend-select = [
    'A',
    'ARG',
    'B',
    'C',
    'DTZ',
    'E',
    'EM',
    'F',
    'FBT',
    'I',
    'ICN',
    'ISC',
    'N',
    'PLC',
    'PLE',
    'PLR',
    'PLW',
    'Q',
    'RUF',
    'S',
    'T',
    'TID',
    'UP',
    'W',
    'YTT',
]
ignore = [
    'ARG001',
    'E501',
    'I001',
    'RUF001',
    'PLR2004',
    'EXE003',
    'ISC001',
]

[tool.ruff.per-file-ignores]
"tests/*" = ['S101']
[tool.pytest.ini_options]
addopts = '-v --durations=10 -p no:briefcase'
asyncio_mode = 'auto'
console_output_style = 'progress'
filterwarnings = [
    'ignore::DeprecationWarning',
    'ignore::UserWarning',
]
log_cli = true
log_cli_level = 'INFO'
markers = [
    '''benchmark: marks tests as benchmarks (select with '-m benchmark')''',
    'unit: mark a test as a unit test',
    'integration: mark a test as an integration test',
    'permutation: tests for permutation functionality',
    'parameter: tests for parameter parsing',
    'prompt: tests for prompt parsing',
]
norecursedirs = [
    '.*',
    'build',
    'dist',
    'venv',
    '__pycache__',
    '*.egg-info',
    '_private',
]
python_classes = ['Test*']
python_files = ['test_*.py']
python_functions = ['test_*']
testpaths = ['tests']

[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = 'file'
save-data = true
compare = [
    'min',
    'max',
    'mean',
    'stddev',
    'median',
    'iqr',
    'ops',
    'rounds',
]
</file>

<file path="README.md">
# vidkompy

`vidkompy` is an intelligent command-line tool designed to overlay a foreground video onto a background video with high-precision automatic alignment. The system is engineered to handle differences in resolution, frame rate, duration, and audio, prioritizing content integrity and accuracy over raw processing speed.

The core philosophy is to treat the **foreground video as the definitive source of quality and timing**. All its frames are preserved without modification or retiming. The background video is dynamically adapted—stretched, retimed, and selectively sampled—to synchronize perfectly with the foreground content.

---

## How It Works: The Processing Pipeline

The alignment and composition process is orchestrated by the `AlignmentEngine` and follows a meticulous, multi-stage pipeline:

1.  **Video Analysis**: The process begins by probing both background and foreground videos using `ffprobe` to extract essential metadata. This includes resolution, frames per second (FPS), duration, frame count, and audio presence. This information is used to check for compatibility and inform subsequent alignment decisions.

2.  **Spatial Alignment**: The engine determines the optimal (x, y) coordinates to place the foreground video on top of the background. A sample frame from the middle of each video is used for this calculation, and the system computes the offset needed to position the foreground video correctly within the background frame.

3.  **Temporal Alignment**: This is the most critical phase, where the videos are synchronized in time. `vidkompy` finds the perfect background frame to match _every single_ foreground frame, ensuring the foreground's timing is flawlessly preserved. This creates a detailed frame-by-frame mapping using advanced algorithms like Dynamic Time Warping.

4.  **Video Composition**: With alignment data computed, the final video is composed:

- First, a silent video is created using OpenCV. The engine iterates through the frame alignment map, reads the corresponding frames from each video, overlays them using the spatial offset, and writes the composite frame to a temporary file. The output FPS is set to the foreground video's FPS to ensure no foreground frames are dropped.
- Next, an audio track is added using FFmpeg. The tool intelligently selects the audio source, **prioritizing the foreground video's audio**. The audio is synchronized with the composed video, and the final output file is generated.

---

## Alignment Algorithms in Detail

`vidkompy` employs sophisticated algorithms for both temporal and spatial alignment, with different modes to balance speed and precision.

### Temporal Alignment: Finding the Perfect Sync

Temporal alignment synchronizes the two videos over time. This is the most complex and powerful feature of `vidkompy` .

#### **`--temporal_align dtw`** (Default, Recommended)

This method uses **Dynamic Time Warping (DTW)** with **Perceptual Hashing** to achieve the most robust and accurate temporal alignment, completely avoiding the timing drift and "catch-up" issues seen in simpler methods.

It creates a unique "fingerprint" for every frame in both videos. Then, it finds the optimal way to stretch and squeeze the background video's timeline to best match the foreground's timeline, guaranteeing a smooth, continuous result.

**How it works**:

1.  **Frame Fingerprinting**: Instead of comparing raw pixels (which is slow), the system first computes a "perceptual fingerprint" for a sample of frames from each video. It uses a combination of multiple hashing algorithms (pHash, AverageHash, ColorMomentHash) that capture different visual aspects like structure, brightness, and color distribution. This results in a compact and robust representation of each frame.
2.  **Cost Matrix**: It builds a cost matrix where each cell `(i, j)` represents the "distance" (dissimilarity) between the fingerprint of foreground frame `i` and background frame `j`.
3.  **Dynamic Time Warping**: The DTW algorithm then finds the lowest-cost path through this matrix from start to finish. This path represents the optimal "warping" of the background's timeline to match the foreground's. A **Sakoe-Chiba band** constraint is used to keep the alignment within a reasonable time window, dramatically speeding up the calculation from O(N²) to O(N).
4.  **Interpolation**: The final alignment map for _every single_ foreground frame is generated by interpolating between the points on the optimal DTW path. This ensures that every foreground frame has a corresponding background frame, resulting in perfectly smooth motion.

#### **`--temporal_align classic`**

This is the legacy frame-matching method. It can be faster for some content but is susceptible to temporal drift.

It finds a few good matching "anchor" frames (keyframes) between the two videos and then guesses the frames in between.

**How it works**:

1.  **Keyframe Matching**: It samples a limited number of frames (keyframes) from both videos and finds the best matches using **Structural Similarity (SSIM)**.
2.  **Monotonic Filtering**: The matches are filtered to ensure a logical time progression (i.e., time only moves forward).
3.  **Interpolation**: For all the frames _between_ the keyframes, the corresponding background frame is estimated using simple linear interpolation. This is where drift can occur if the videos have different playback speeds between keyframes.

#### **`--match_time fast`** (Audio-Based)

This mode leverages audio tracks for a very fast initial alignment. It can be used with either `dtw` or `classic` temporal alignment.

It listens to both audio tracks and slides one until they sound perfectly synced up.

**How it works**: The audio from both videos is extracted, and the system computes the **cross-correlation** between them. The peak of the correlation reveals the precise time offset. If successful, this offset is used to create the frame map. If audio is missing or doesn't match, it automatically falls back to the configured frame-based method ( `dtw` or `classic` ).

### Spatial Alignment: Finding the Perfect Position

Spatial alignment determines where the foreground video sits within the larger background frame. If the foreground is larger than the background, it is automatically scaled down to fit while preserving its aspect ratio.

#### ** `--match_space precise` / `template` ** (Default)

It treats the foreground frame like a puzzle piece and finds exactly where it fits inside the background frame.

**How it works**: This method uses **Template Matching**. It takes the entire foreground frame as a template and performs a pixel-by-pixel search across the background frame to find the location with the highest normalized cross-correlation. It's extremely accurate when the foreground is an exact, un-altered crop of the background.

#### ** `--match_space fast` / `feature` **

It finds hundreds of unique interest points (like corners and edges) in both frames and matches them up to figure out the position.

**How it works**: This method uses **Feature Matching** with the **ORB (Oriented FAST and Rotated BRIEF)** algorithm. It detects key feature points in both frames and matches them. This approach is more robust against variations in brightness, contrast, or compression artifacts. If too few features are found, it safely falls back to centering the foreground.

#### **`--skip_spatial_align`**

If this flag is used, all alignment calculations are skipped, and the foreground video is simply centered within the background frame.

---

## Usage

### Prerequisites

You must have the **FFmpeg** binary installed on your system and accessible in your system's PATH. `vidkompy` depends on it for all video and audio processing tasks.

### Installation

The tool is a Python package and can be installed locally:

```bash
# Clone the repository and install using uv (or pip)
git clone https://github.com/your-repo/vidkompy.git
cd vidkompy
uv pip install .
```

### Command-Line Interface (CLI)

The tool is run from the command line. The primary arguments are the paths to the background and foreground videos.

```bash
python -m vidkompy --bg <background_video> --fg <foreground_video> [OPTIONS]
```

**Key Arguments:**

- `--bg` (str): Path to the background video file. **[Required]**
- `--fg` (str): Path to the foreground video file. **[Required]**
- `-o`, `--output` (str): Path for the final output video. If not provided, a name is automatically generated (e.g., `bg-stem_overlay_fg-stem.mp4`).
- `--match_time` (str): The high-level temporal alignment strategy.
  - `'precise'` (default): Uses the chosen frame-based method directly for maximum accuracy.
  - `'fast'`: Attempts audio-based alignment first and falls back to the frame-based method.
- `--temporal_align` (str): The core algorithm for frame-based temporal alignment.
  - `'dtw'` (default): Dynamic Time Warping with perceptual hashing. Highly robust and accurate.
  - `'classic'`: The legacy keyframe matching and interpolation method.
- `--match_space` (str): The spatial alignment method.
  - `'precise'` or `'template'` (default): Slower but more exact pixel matching.
  - `'fast'` or `'feature'`: Faster, more robust feature-based matching.
- `--trim` (bool): If `True` (default), the output video is trimmed to the duration of the aligned segment.
- `--skip_spatial_align` (bool): If `True` (default: `False`), skips spatial alignment and centers the foreground video.
- `--verbose` (bool): Enables detailed debug logging for troubleshooting.
- `--max_keyframes` (int): Sets the approximate number of frames to sample for temporal alignment (default: 2000). Higher values can increase accuracy but also memory usage and processing time.
</file>

<file path="src/vidkompy/core/alignment_engine.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/alignment_engine.py

"""
Main alignment engine that coordinates spatial and temporal alignment.

This is the high-level orchestrator that manages the complete video
overlay process.
"""

import tempfile
import cv2
import ffmpeg
import numpy as np
from pathlib import Path
from loguru import logger
from rich.progress import (
    Progress,
    BarColumn,
    TextColumn,
    TimeRemainingColumn,
)
from rich.console import Console

from vidkompy.models import (
    VideoInfo,
    MatchTimeMode,
    TemporalMethod,
    SpatialAlignment,
    TemporalAlignment,
    FrameAlignment,
)
from .video_processor import VideoProcessor
from .spatial_alignment import SpatialAligner
from .temporal_alignment import TemporalAligner


console = Console()


class AlignmentEngine:
    """Orchestrates the complete video alignment and overlay process.

    This is the main coordinator that manages the entire video overlay workflow.
    It handles the high-level process flow while delegating specific tasks to
    specialized components.

    Why this architecture:
    - Separation of concerns: Each component (spatial, temporal, processing) has a single responsibility
    - Flexibility: Easy to swap alignment algorithms or add new methods
    - Testability: Each component can be tested independently
    - Progress tracking: Centralized progress reporting for better UX
    """

    def __init__(
        self,
        processor: VideoProcessor,
        verbose: bool = False,
        max_keyframes: int = 2000,
    ):
        """Initialize alignment engine.

        Args:
            processor: Video processor instance
            verbose: Enable verbose logging
            max_keyframes: Maximum keyframes for frame matching
        """
        self.processor = processor
        self.spatial_aligner = SpatialAligner()
        self.temporal_aligner = TemporalAligner(processor, max_keyframes)
        self.verbose = verbose

    def process(
        self,
        bg_path: str,
        fg_path: str,
        output_path: str,
        time_mode: MatchTimeMode,
        space_method: str,
        temporal_method: TemporalMethod,
        skip_spatial: bool,
        trim: bool,
        border_thickness: int = 8,
        blend: bool = False,
        window: int = 0,
    ):
        """Process video overlay with alignment.

        Args:
            bg_path: Background video path
            fg_path: Foreground video path
            output_path: Output video path
            time_mode: Temporal alignment mode
            space_method: Spatial alignment method
            temporal_method: Temporal algorithm to use (DTW or classic)
            skip_spatial: Skip spatial alignment
            trim: Trim to overlapping segment
            border_thickness: Border thickness for border matching mode
            blend: Enable smooth blending at frame edges
            window: Sliding window size for frame matching
        """
        # Analyze videos - quick task, use simple logging
        logger.info("Analyzing videos...")
        bg_info = self.processor.get_video_info(bg_path)
        fg_info = self.processor.get_video_info(fg_path)

        # Log compatibility
        self._log_compatibility(bg_info, fg_info)

        # Spatial alignment - quick task, use simple logging
        logger.info("Computing spatial alignment...")
        spatial_alignment = self._compute_spatial_alignment(
            bg_info, fg_info, space_method, skip_spatial
        )

        # Log spatial alignment results in non-verbose mode too
        if not skip_spatial:
            logger.info(
                f"Spatial alignment result: offset=({spatial_alignment.x_offset}, {spatial_alignment.y_offset}), "
                f"scale={spatial_alignment.scale_factor:.3f}, confidence={spatial_alignment.confidence:.3f}"
            )

        # Temporal alignment - potentially time-intensive, use progress tracking
        logger.info("Computing temporal alignment...")
        temporal_alignment = self._compute_temporal_alignment(
            bg_info,
            fg_info,
            time_mode,
            temporal_method,
            trim,
            spatial_alignment,
            border_thickness,
            window,
        )

        # Log temporal alignment results
        logger.info(
            f"Temporal alignment result: method={temporal_alignment.method_used}, "
            f"offset={temporal_alignment.offset_seconds:.3f}s, "
            f"frames={len(temporal_alignment.frame_alignments)}, "
            f"confidence={temporal_alignment.confidence:.3f}"
        )

        # Compose final video - time-intensive, use progress tracking
        logger.info("Composing output video...")
        self._compose_video(
            bg_info,
            fg_info,
            output_path,
            spatial_alignment,
            temporal_alignment,
            trim,
            blend,
            border_thickness,
        )

        logger.info(f"✅ Processing complete: {output_path}")

    def _log_compatibility(self, bg_info: VideoInfo, fg_info: VideoInfo):
        """Log video compatibility information.

        Why we log compatibility:
        - Early warning of potential issues (e.g., fg larger than bg)
        - Helps users understand processing decisions
        - Aids in debugging alignment problems
        - Documents the input characteristics for reproducibility
        """
        logger.info("Video compatibility check:")
        logger.info(
            f"  Resolution: {bg_info.width}x{bg_info.height} vs {fg_info.width}x{fg_info.height}"
        )
        logger.info(f"  FPS: {bg_info.fps:.2f} vs {fg_info.fps:.2f}")
        logger.info(f"  Duration: {bg_info.duration:.2f}s vs {fg_info.duration:.2f}s")
        logger.info(
            f"  Audio: {'yes' if bg_info.has_audio else 'no'} vs {'yes' if fg_info.has_audio else 'no'}"
        )

        if fg_info.width > bg_info.width or fg_info.height > bg_info.height:
            logger.warning(
                "⚠️  Foreground is larger than background - will be scaled down"
            )

    def _compute_spatial_alignment(
        self, bg_info: VideoInfo, fg_info: VideoInfo, method: str, skip: bool
    ) -> SpatialAlignment:
        """Compute spatial alignment using sample frames.

        Why we use middle frames for alignment:
        - Middle frames typically have the main content fully visible
        - Avoids potential black frames or transitions at start/end
        - Single frame is usually sufficient for static camera shots
        - Fast computation while maintaining accuracy

        Why we support skipping:
        - Sometimes users know the alignment (e.g., already centered)
        - Useful for testing temporal alignment independently
        - Speeds up processing when spatial alignment isn't needed
        """
        if skip:
            logger.info("Skipping spatial alignment - centering foreground")
            x_offset = (bg_info.width - fg_info.width) // 2
            y_offset = (bg_info.height - fg_info.height) // 2
            return SpatialAlignment(x_offset, y_offset, 1.0, 1.0)

        # Extract sample frames for alignment
        bg_frames = self.processor.extract_frames(
            bg_info.path, [bg_info.frame_count // 2]
        )
        fg_frames = self.processor.extract_frames(
            fg_info.path, [fg_info.frame_count // 2]
        )

        if not bg_frames or not fg_frames:
            logger.error("Failed to extract frames for spatial alignment")
            x_offset = (bg_info.width - fg_info.width) // 2
            y_offset = (bg_info.height - fg_info.height) // 2
            return SpatialAlignment(x_offset, y_offset, 1.0, 0.0)

        return self.spatial_aligner.align(bg_frames[0], fg_frames[0], method, skip)

    def _compute_temporal_alignment(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        mode: MatchTimeMode,
        temporal_method: TemporalMethod,
        trim: bool,
        spatial_alignment: SpatialAlignment,
        border_thickness: int,
        window: int,
    ) -> TemporalAlignment:
        """Compute temporal alignment based on mode.

        Why we have multiple modes:
        - BORDER: Border-based matching focusing on frame edges (new default)
        - PRECISE: Frame-based matching for maximum accuracy
        - FAST: Audio-based when available, falls back to frames

        Why audio can be faster:
        - Audio correlation is a 1D signal comparison
        - Works well when videos have identical audio tracks
        - Provides global offset quickly

        Why we always fall back to frames:
        - Not all videos have audio
        - Audio might be out of sync with video
        - Frame matching handles all cases
        """
        # Configure temporal aligner based on method and window
        self.temporal_aligner.use_dtw = temporal_method == TemporalMethod.DTW
        if window > 0:
            self.temporal_aligner.dtw_aligner.set_window(window)

        if mode == MatchTimeMode.BORDER:
            # Use border-based alignment with mask
            logger.info(
                f"Using border-based temporal alignment (border thickness: {border_thickness}px)"
            )
            border_mask = self.temporal_aligner.create_border_mask(
                spatial_alignment, fg_info, bg_info, border_thickness
            )
            return self.temporal_aligner.align_frames_with_mask(
                bg_info, fg_info, trim, border_mask
            )

        elif mode == MatchTimeMode.PRECISE:
            # Always use frame-based alignment
            return self.temporal_aligner.align_frames(bg_info, fg_info, trim)

        elif mode == MatchTimeMode.FAST:
            # Try audio first if available
            if bg_info.has_audio and fg_info.has_audio:
                logger.info("Attempting audio-based temporal alignment")

                with tempfile.TemporaryDirectory() as tmpdir:
                    bg_audio = Path(tmpdir) / "bg_audio.wav"
                    fg_audio = Path(tmpdir) / "fg_audio.wav"

                    # Extract audio
                    if self.processor.extract_audio(
                        bg_info.path, str(bg_audio)
                    ) and self.processor.extract_audio(fg_info.path, str(fg_audio)):
                        offset = self.temporal_aligner.align_audio(
                            str(bg_audio), str(fg_audio)
                        )

                        # Create simple frame alignment based on audio offset
                        frame_alignments = self._create_audio_based_alignment(
                            bg_info, fg_info, offset, trim
                        )

                        return TemporalAlignment(
                            offset_seconds=offset,
                            frame_alignments=frame_alignments,
                            method_used="audio",
                            confidence=0.8,
                        )
                    else:
                        logger.warning(
                            "Audio extraction failed, falling back to frames"
                        )
            else:
                logger.info("No audio available, using frame-based alignment")

            # Fallback to frame-based
            return self.temporal_aligner.align_frames(bg_info, fg_info, trim)

    def _create_audio_based_alignment(
        self, bg_info: VideoInfo, fg_info: VideoInfo, offset_seconds: float, trim: bool
    ) -> list[FrameAlignment]:
        """Create frame alignments based on audio offset."""
        alignments = []

        # Calculate frame offset
        bg_frame_offset = int(offset_seconds * bg_info.fps)

        # Determine range
        if trim:
            start_fg = 0
            end_fg = min(
                fg_info.frame_count,
                int((bg_info.duration - offset_seconds) * fg_info.fps),
            )
        else:
            start_fg = 0
            end_fg = fg_info.frame_count

        # Create alignments
        fps_ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0

        for fg_idx in range(start_fg, end_fg):
            bg_idx = int(fg_idx * fps_ratio + bg_frame_offset)
            bg_idx = max(0, min(bg_idx, bg_info.frame_count - 1))

            alignments.append(
                FrameAlignment(
                    fg_frame_idx=fg_idx,
                    bg_frame_idx=bg_idx,
                    similarity_score=0.7,  # Assumed good match from audio
                )
            )

        return alignments

    def _compose_video(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        output_path: str,
        spatial: SpatialAlignment,
        temporal: TemporalAlignment,
        trim: bool,
        blend: bool = False,
        border_thickness: int = 8,
    ):
        """Compose the final output video.

        Why we use a two-step process (silent video + audio):
        - OpenCV doesn't handle audio, but provides frame-accurate control
        - FFmpeg handles audio well but can have frame accuracy issues
        - Combining both gives us the best of both worlds

        Why we prefer FG audio:
        - FG video is considered "better quality" per requirements
        - FG frames drive the output timing
        - Keeping FG audio maintains sync with FG visuals
        """
        logger.info(f"Composing video with {temporal.method_used} temporal alignment")

        # Use OpenCV for frame-accurate composition
        with tempfile.TemporaryDirectory() as tmpdir:
            # Create silent video first
            temp_video = Path(tmpdir) / "temp_silent.mp4"

            self._compose_with_opencv(
                bg_info,
                fg_info,
                str(temp_video),
                spatial,
                temporal.frame_alignments,
                blend,
                border_thickness,
            )

            # Add audio
            self._add_audio_track(
                str(temp_video), output_path, bg_info, fg_info, temporal
            )

    def _frame_generator(self, video_path: str):
        """Yields frames from a video file sequentially.

        This generator provides sequential frame access which is much faster
        than random seeking. It's designed to eliminate the costly seek operations
        that slow down compositing.

        Args:
            video_path: Path to video file

        Yields:
            tuple: (frame_index, frame_array) for each frame
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            msg = f"Cannot open video file: {video_path}"
            raise OSError(msg)

        frame_idx = 0
        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                yield frame_idx, frame
                frame_idx += 1
        finally:
            cap.release()

    def _compose_with_opencv(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        output_path: str,
        spatial: SpatialAlignment,
        alignments: list[FrameAlignment],
        blend: bool = False,
        border_thickness: int = 8,
    ):
        """Compose video using sequential reads for maximum performance.

        This optimized version eliminates random seeking by reading both video
        files sequentially. This provides a 10-100x speedup compared to the
        previous random-access approach.

        How it works:
        - Create generators that read each video file sequentially
        - Advance each generator to the frame we need
        - Since alignments are typically in ascending order, we mostly move forward
        - Eliminates costly seek operations that were the main bottleneck
        """
        if not alignments:
            logger.warning("No frame alignments provided. Cannot compose video.")
            return

        writer = self.processor.create_video_writer(
            output_path,
            bg_info.width,
            bg_info.height,
            fg_info.fps,  # Use FG fps to preserve all FG frames
        )

        bg_gen = self._frame_generator(bg_info.path)
        fg_gen = self._frame_generator(fg_info.path)

        current_bg_frame = None
        current_fg_frame = None
        current_bg_idx = -1
        current_fg_idx = -1

        frames_written = 0
        total_frames = len(alignments)

        # Create blend mask if needed
        blend_mask = None
        if blend:
            blend_mask = self.temporal_aligner.create_blend_mask(
                spatial, fg_info, bg_info, border_thickness
            )
            logger.info(f"Using blend mode with {border_thickness}px gradient")

        try:
            # Use proper progress bar for video composition
            with Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total} frames)"),
                TimeRemainingColumn(),
                console=console,
                transient=False,
            ) as progress:
                task = progress.add_task("Composing frames", total=total_frames)

                for i, alignment in enumerate(alignments):
                    needed_fg_idx = alignment.fg_frame_idx
                    needed_bg_idx = alignment.bg_frame_idx

                    # Advance foreground generator to the needed frame
                    while current_fg_idx < needed_fg_idx:
                        try:
                            current_fg_idx, current_fg_frame = next(fg_gen)
                        except StopIteration:
                            logger.error("Reached end of foreground video unexpectedly")
                            break

                    # Advance background generator to the needed frame
                    while current_bg_idx < needed_bg_idx:
                        try:
                            current_bg_idx, current_bg_frame = next(bg_gen)
                        except StopIteration:
                            logger.error("Reached end of background video unexpectedly")
                            break

                    if current_fg_frame is None or current_bg_frame is None:
                        logger.error("Frame generator did not yield a frame. Aborting.")
                        break

                    # We now have the correct pair of frames
                    composite = self._overlay_frames(
                        current_bg_frame, current_fg_frame, spatial, blend_mask
                    )
                    writer.write(composite)
                    frames_written += 1

                    # Update progress bar
                    progress.update(task, advance=1)

        except StopIteration:
            logger.warning("Reached end of a video stream unexpectedly.")
        finally:
            writer.release()
            logger.info(f"Wrote {frames_written} frames to {output_path}")

    def _overlay_frames(
        self,
        bg_frame: np.ndarray,
        fg_frame: np.ndarray,
        spatial: SpatialAlignment,
        blend_mask: np.ndarray | None = None,
    ) -> np.ndarray:
        """Overlay foreground on background with spatial alignment and optional blending."""
        composite = bg_frame.copy()

        # Apply scaling if needed
        if spatial.scale_factor != 1.0:
            new_w = int(fg_frame.shape[1] * spatial.scale_factor)
            new_h = int(fg_frame.shape[0] * spatial.scale_factor)
            fg_frame = cv2.resize(
                fg_frame, (new_w, new_h), interpolation=cv2.INTER_AREA
            )
            # Also scale blend mask if provided
            if blend_mask is not None:
                blend_mask = cv2.resize(
                    blend_mask, (new_w, new_h), interpolation=cv2.INTER_AREA
                )

        fg_h, fg_w = fg_frame.shape[:2]
        bg_h, bg_w = bg_frame.shape[:2]

        # Calculate ROI with bounds checking
        x_start = max(0, spatial.x_offset)
        y_start = max(0, spatial.y_offset)
        x_end = min(bg_w, spatial.x_offset + fg_w)
        y_end = min(bg_h, spatial.y_offset + fg_h)

        # Calculate foreground crop if needed
        fg_x_start = max(0, -spatial.x_offset)
        fg_y_start = max(0, -spatial.y_offset)
        fg_x_end = fg_x_start + (x_end - x_start)
        fg_y_end = fg_y_start + (y_end - y_start)

        # Overlay
        if x_end > x_start and y_end > y_start:
            fg_crop = fg_frame[fg_y_start:fg_y_end, fg_x_start:fg_x_end]
            bg_slice = composite[y_start:y_end, x_start:x_end]

            if blend_mask is not None:
                # Apply alpha blending using the blend mask
                mask_crop = blend_mask[fg_y_start:fg_y_end, fg_x_start:fg_x_end]

                # Ensure mask has same dimensions for broadcasting
                if len(fg_crop.shape) == 3:  # Color image
                    mask_crop = np.stack([mask_crop] * 3, axis=2)

                # Alpha blend: result = fg * alpha + bg * (1 - alpha)
                composite[y_start:y_end, x_start:x_end] = (
                    fg_crop.astype(np.float32) * mask_crop
                    + bg_slice.astype(np.float32) * (1.0 - mask_crop)
                ).astype(np.uint8)
            else:
                # Simple overlay (original behavior)
                composite[y_start:y_end, x_start:x_end] = fg_crop

        return composite

    def _add_audio_track(
        self,
        video_path: str,
        output_path: str,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        temporal: TemporalAlignment,
    ):
        """Add audio track to the composed video."""
        # Prefer foreground audio as it's "better quality"
        if fg_info.has_audio:
            audio_source = fg_info.path
            audio_offset = 0.0  # FG audio is already aligned
            logger.info("Using foreground audio track")
        elif bg_info.has_audio:
            audio_source = bg_info.path
            audio_offset = -temporal.offset_seconds  # Compensate for alignment
            logger.info("Using background audio track")
        else:
            # No audio, just copy video
            logger.info("No audio tracks available")
            Path(video_path).rename(output_path)
            return

        # Merge audio with ffmpeg
        try:
            input_video = ffmpeg.input(video_path)

            if audio_offset != 0:
                input_audio = ffmpeg.input(audio_source, itsoffset=audio_offset)
            else:
                input_audio = ffmpeg.input(audio_source)

            stream = ffmpeg.output(
                input_video["v"],
                input_audio["a"],
                output_path,
                c="copy",
                acodec="aac",
                shortest=None,
            )

            ffmpeg.run(stream, overwrite_output=True, capture_stderr=True)

        except ffmpeg.Error as e:
            logger.error(f"Audio merge failed: {e.stderr.decode()}")
            # Fallback - save without audio
            Path(video_path).rename(output_path)
            logger.warning("Saved video without audio")
</file>

<file path="src/vidkompy/__version__.py">
# file generated by setuptools-scm
# don't change, don't track in version control

__all__ = ["__version__", "__version_tuple__", "version", "version_tuple"]

TYPE_CHECKING = False
if TYPE_CHECKING:
    VERSION_TUPLE = tuple[int | str, ...]
else:
    VERSION_TUPLE = object

version: str
__version__: str
__version_tuple__: VERSION_TUPLE
version_tuple: VERSION_TUPLE

__version__ = version = "0.0.post3+g39be67c.d20250524"
__version_tuple__ = version_tuple = (0, 0, "post3", "g39be67c.d20250524")
</file>

<file path="PROGRESS.md">
# Progress: vidkompy Performance Improvements

## Previous Rounds Complete

### Phase 1-6: Core Architecture ✓
- [x] Modular architecture implemented
- [x] Spatial alignment (template/feature) working
- [x] Temporal alignment (audio/frames) working
- [x] Frame preservation guaranteed
- [x] Basic functionality verified

## Current Round: Progress Reporting Improvements ✓ COMPLETED

### Progress UI Cleanup ✓ COMPLETED
- [x] **Removed Spinner Progress**: Eliminated annoying spinner displays for quick tasks
- [x] **Simple Logging**: Quick tasks (video analysis, spatial alignment) now use simple logger.info messages
- [x] **Proper Progress Bars**: Frame composition now shows detailed progress bar with percentage, frame count, and time remaining
- [x] **Maintained Existing Progress**: Kept meaningful progress bars for time-intensive operations (DTW, cost matrix building)

### Previous Round: Compositing Performance Optimization ✓ COMPLETED
- [x] **Compositing Speedup**: Implemented sequential reading with generators 
- [x] **Progress Context Fix**: Resolved "Only one live display may be active at once" error
- [x] **Verification**: Confirmed 10-100x speedup potential by eliminating random seeks
- [x] Add spatial alignment result logging in non-verbose mode
- [x] Add temporal alignment result logging in non-verbose mode  
- [x] Fix progress indicator flickering issue (removed spinner conflicts)
- [x] Add detailed docstrings explaining the why
- [x] Create varia/SPEC4.md with improvement plan

### Summary of Recent Changes:
1. **Improved Progress UX**: Removed annoying spinners, kept useful progress bars for substantial tasks
2. **Better Information Density**: Frame composition now shows frame count and time estimates
3. **Cleaner Logging**: Quick operations use simple logging instead of progress displays
4. **Previous Improvements**: Sequential frame reading, improved logging, enhanced documentation

## Next Round: Performance Optimization (from SPEC2.md)

### Phase 7: Fast Frame Similarity Metrics

- [ ] Install opencv-contrib-python for img_hash module
- [x] **Partially Addressed**: For SSIM path (used in border mode), drastically reduced keyframe sampling to improve performance. Perceptual hashing is used by default for DTW/non-border modes.
- [ ] Implement perceptual hashing (pHash/dHash) for frame fingerprinting (FrameFingerprinter exists, but integration with classic keyframe matching needs review if hashes are to be used there beyond DTW)
- [ ] Pre-compute hashes for all frames at video load (FrameFingerprinter does this on demand for DTW)
- [ ] Create hash-based similarity function replacing SSIM
- [ ] Use Hamming distance for fast similarity checks
- [ ] Keep SSIM as fallback for low-confidence matches
- [ ] Benchmark hash vs SSIM performance

### Phase 8: Optimize Keyframe Matching

- [ ] Implement adaptive search windows based on previous matches
- [ ] Add confidence-based early termination
- [ ] Reduce initial downsampling from 0.25 to 0.125 for speed
- [ ] Cache extracted frames to avoid redundant reads
- [ ] Implement dynamic keyframe sampling (coarse → fine)
- [ ] Add temporal consistency checks

### Phase 9: Advanced Temporal Alignment (DTW)

- [ ] Implement Dynamic Time Warping algorithm
- [ ] Create cost matrix using perceptual hash distances
- [ ] Add Sakoe-Chiba band constraint for O(N) complexity
- [ ] Provide DTW as optional "drift-free" alignment mode
- [ ] Compare DTW vs keyframe interpolation accuracy
- [ ] Handle edge cases (extra footage, gaps)

### Phase 10: Performance Optimizations

- [ ] Add multiprocessing.Pool for frame hash computations
- [ ] Parallelize similarity matrix calculations
- [ ] Implement batched frame extraction with threading
- [ ] Add detailed progress reporting with time estimates
- [ ] Profile and optimize memory usage
- [ ] Consider numba JIT for critical loops

### Phase 11: Testing & Validation

- [ ] Create performance benchmark suite
- [ ] Test with tests/bg.mp4 and tests/fg.mp4
- [ ] Measure speedup: target 10-20x improvement
- [ ] Validate zero drift with DTW mode
- [ ] Test various video scenarios (different fps, durations)
- [ ] Add unit tests for new hash/DTW components

### Phase 12: Integration & Documentation

- [ ] Update CLI with new options (--temporal_align dtw)
- [ ] Update CHANGELOG.md with performance improvements
- [ ] Document new alignment modes in README.md
- [ ] Add performance tuning guide
- [ ] Clean up and optimize imports
- [ ] Run final formatting/linting

## Performance Targets

- Current: ~2-5 fps for frame matching (8s video times out)
- Target: 40-80 fps (process 8s video in <1s)
- Zero temporal drift with DTW mode
- Memory usage < 1GB for 1080p videos

## Implementation Priority

1. **Perceptual hashing** (Phase 7) - Biggest impact, easiest
2. **Adaptive windows** (Phase 8) - Quick win, moderate effort
3. **Parallelization** (Phase 10) - Good speedup, moderate effort
4. **DTW** (Phase 9) - Eliminates drift, complex but valuable
</file>

<file path="src/vidkompy/vidkompy.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fire", "rich", "loguru", "opencv-python", "numpy", "scipy", "ffmpeg-python", "soundfile", "scikit-image"]
# ///
# this_file: src/vidkompy/vidkompy.py

"""
Intelligent video overlay tool with automatic spatial and temporal alignment.

This tool overlays foreground videos onto background videos with smart alignment:
- Preserves all foreground frames without retiming
- Finds optimal background frames for each foreground frame
- Supports audio-based and frame-based temporal alignment
- Automatic spatial alignment with template/feature matching
"""

import sys
import fire
from loguru import logger
from pathlib import Path

from .core.video_processor import VideoProcessor
from .core.alignment_engine import AlignmentEngine
from .models import MatchTimeMode, TemporalMethod


def main(
    bg: str,
    fg: str,
    output: str | None = None,
    match_time: str = "border",
    match_space: str = "precise",
    temporal_align: str = "dtw",
    skip_spatial_align: bool = False,
    trim: bool = True,
    verbose: bool = False,
    max_keyframes: int = 2000,
    border: int = 8,
    blend: bool = False,
    window: int = 0,
):
    """Overlay foreground video onto background video with intelligent alignment.

    Args:
        bg: Background video path
        fg: Foreground video path
        output: Output video path (auto-generated if not provided)
        match_time: Temporal alignment - 'border' (border matching, default), 'fast' (audio then frames), or 'precise' (frames)
        match_space: Spatial alignment - 'precise' (template) or 'fast' (feature)
        temporal_align: Temporal algorithm - 'dtw' (new default) or 'classic'
        skip_spatial_align: Skip spatial alignment, center foreground
        trim: Trim output to overlapping segments only
        verbose: Enable verbose logging
        max_keyframes: Maximum keyframes for frame-based alignment
        border: Border thickness for border matching mode (default: 8)
        blend: Enable smooth blending at frame edges
        window: Sliding window size for frame matching (0 = no window)
    """
    # Setup logging
    logger.remove()
    if verbose:
        logger.add(
            sys.stderr,
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan> - <level>{message}</level>",
            level="DEBUG",
        )
    else:
        logger.add(
            sys.stderr,
            format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
            level="INFO",
        )

    # Validate inputs
    bg_path = Path(bg)
    fg_path = Path(fg)

    if not bg_path.exists():
        logger.error(f"Background video not found: {bg}")
        return

    if not fg_path.exists():
        logger.error(f"Foreground video not found: {fg}")
        return

    # Generate output path if needed
    if output is None:
        output = f"{bg_path.stem}_overlay_{fg_path.stem}.mp4"
        logger.info(f"Output path: {output}")

    # Validate output path
    output_path = Path(output)
    if output_path.exists():
        logger.warning(f"Output file already exists: {output}")
        logger.warning("It will be overwritten")

    # Ensure output directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Validate match_time mode
    try:
        time_mode = MatchTimeMode(match_time)
    except ValueError:
        logger.error(
            f"Invalid match_time mode: {match_time}. Use 'border', 'fast', or 'precise'"
        )
        return

    # Validate match_space mode
    valid_space_methods = ["precise", "template", "fast", "feature"]
    if match_space not in valid_space_methods:
        logger.error(
            f"Invalid match_space mode: {match_space}. Use one of: {', '.join(valid_space_methods)}"
        )
        return

    # Normalize space method names
    if match_space == "precise":
        match_space = "template"
    elif match_space == "fast":
        match_space = "feature"

    # Validate temporal_align
    try:
        temporal_method = TemporalMethod(temporal_align)
    except ValueError:
        # Try common aliases
        if temporal_align in ["frames", "keyframes"]:
            temporal_method = TemporalMethod.CLASSIC
        else:
            logger.error(
                f"Invalid temporal_align: {temporal_align}. Use 'dtw' or 'classic'"
            )
            return

    # Validate max_keyframes
    if max_keyframes < 10:
        logger.error(f"max_keyframes must be at least 10, got {max_keyframes}")
        return
    elif max_keyframes > 10000:
        logger.warning(
            f"max_keyframes is very high ({max_keyframes}), this may be slow"
        )

    # Create processor and engine
    processor = VideoProcessor()
    engine = AlignmentEngine(
        processor=processor, verbose=verbose, max_keyframes=max_keyframes
    )

    # Process the videos
    try:
        engine.process(
            bg_path=str(bg_path),
            fg_path=str(fg_path),
            output_path=output,
            time_mode=time_mode,
            space_method=match_space,
            temporal_method=temporal_method,
            skip_spatial=skip_spatial_align,
            trim=trim,
            border_thickness=border,
            blend=blend,
            window=window,
        )
    except Exception as e:
        logger.error(f"Processing failed: {e}")
        raise


if __name__ == "__main__":
    fire.Fire(main)
</file>

<file path="src/vidkompy/core/temporal_alignment.py">
#!/usr/bin/env python3
# this_file: src/vidkompy/core/temporal_alignment.py

"""
Temporal alignment module for synchronizing videos.

Implements audio-based and frame-based temporal alignment with
emphasis on preserving all foreground frames without retiming.
"""

import cv2
import numpy as np
import soundfile as sf
from scipy import signal
from skimage.metrics import structural_similarity as ssim
from loguru import logger
from rich.progress import (
    Progress,
    TextColumn,
    BarColumn,
    TimeRemainingColumn,
)
from rich.console import Console
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp

from vidkompy.models import VideoInfo, FrameAlignment, TemporalAlignment
from .video_processor import VideoProcessor
from .frame_fingerprint import FrameFingerprinter
from .dtw_aligner import DTWAligner

console = Console()


class TemporalAligner:
    """Handles temporal alignment between videos.

    This module synchronizes two videos in time, finding which frames
    correspond between them. This is the most complex part of vidkompy.

    Why temporal alignment is critical:
    - Videos may start at different times
    - Frame rates might differ
    - Some frames might be added/dropped in one video
    - The FG video timing must be preserved (it's the reference)

    Current implementation uses keyframe matching with interpolation.
    Future versions will use Dynamic Time Warping (see SPEC4.md).
    """

    def __init__(self, processor: VideoProcessor, max_keyframes: int = 2000):
        """Initialize temporal aligner.

        Args:
            processor: Video processor instance
            max_keyframes: Maximum keyframes for frame matching
        """
        self.processor = processor
        self.max_keyframes = max_keyframes
        self.use_perceptual_hash = True
        self.hash_cache: dict[str, dict[int, dict[str, np.ndarray]]] = {}
        self._current_mask: np.ndarray | None = None

        self.fingerprinter: FrameFingerprinter | None = None
        self.dtw_aligner = DTWAligner(window_constraint=100)
        self.use_dtw = True
        self.hasher: cv2.img_hash.PHash | None = None

        try:
            if hasattr(cv2, "img_hash") and hasattr(cv2.img_hash, "PHash_create"):
                self.hasher = cv2.img_hash.PHash_create()
                if self.hasher is not None:
                    logger.info("✓ Perceptual hashing enabled (pHash)")
                    self.use_perceptual_hash = True
                else:
                    logger.warning(
                        "cv2.img_hash.PHash_create() returned None. "
                        "Perceptual hashing disabled. Falling back to SSIM."
                    )
                    self.use_perceptual_hash = False
            else:
                logger.warning(
                    "cv2.img_hash.PHash_create not found. "
                    "Perceptual hashing disabled. Falling back to SSIM."
                )
                self.use_perceptual_hash = False
        except AttributeError:
            logger.warning(
                "cv2.img_hash module or PHash_create not available. "
                "Ensure opencv-contrib-python is correctly installed. "
                "Perceptual hashing disabled. Falling back to SSIM."
            )
            self.use_perceptual_hash = False
        except Exception as e:
            logger.error(f"Unexpected error initializing perceptual hasher: {e}")
            self.use_perceptual_hash = False

        if self.hasher is None:
            if self.use_perceptual_hash:
                logger.warning("Hasher is None, forcing use_perceptual_hash to False.")
            self.use_perceptual_hash = False

    def align_audio(self, bg_audio_path: str, fg_audio_path: str) -> float:
        """Compute temporal offset using audio cross-correlation.

        Args:
            bg_audio_path: Background audio WAV file
            fg_audio_path: Foreground audio WAV file

        Returns:
            Offset in seconds (positive means FG starts later)
        """
        logger.debug("Computing audio cross-correlation")

        # Load audio
        bg_audio, bg_sr = sf.read(bg_audio_path)
        fg_audio, fg_sr = sf.read(fg_audio_path)

        if bg_sr != fg_sr:
            logger.warning(f"Sample rates differ: {bg_sr} vs {fg_sr}")
            return 0.0

        # Compute cross-correlation
        correlation = signal.correlate(bg_audio, fg_audio, mode="full", method="fft")

        # Find peak
        peak_idx = np.argmax(np.abs(correlation))

        # Convert to time offset
        center = len(bg_audio) - 1
        lag_samples = peak_idx - center
        offset_seconds = lag_samples / bg_sr

        # Calculate confidence
        peak_value = np.abs(correlation[peak_idx])
        avg_value = np.mean(np.abs(correlation))
        confidence = peak_value / avg_value if avg_value > 0 else 0

        logger.info(
            f"Audio alignment: offset={offset_seconds:.3f}s, "
            f"confidence={confidence:.2f}"
        )

        return offset_seconds

    def align_frames(
        self, bg_info: VideoInfo, fg_info: VideoInfo, trim: bool = False
    ) -> TemporalAlignment:
        """Align videos using frame content matching.

        This method ensures ALL foreground frames are preserved without
        retiming. It finds the optimal background frame for each foreground
        frame.

        Args:
            bg_info: Background video metadata
            fg_info: Foreground video metadata
            trim: Whether to trim to overlapping segment

        Returns:
            TemporalAlignment with frame mappings
        """
        logger.info("Starting frame-based temporal alignment")

        # Use DTW if enabled (default)
        if self.use_dtw:
            return self._align_frames_dtw(bg_info, fg_info, trim)

        # Otherwise use original keyframe matching
        keyframe_matches = self._find_keyframe_matches(bg_info, fg_info)

        if not keyframe_matches:
            logger.warning("No keyframe matches found, using direct mapping")
            # Fallback to simple frame mapping
            return self._create_direct_mapping(bg_info, fg_info)

        # Build complete frame alignment preserving all FG frames
        frame_alignments = self._build_frame_alignments(
            bg_info, fg_info, keyframe_matches, trim
        )

        # Calculate overall temporal offset
        first_match = keyframe_matches[0]
        offset_seconds = (first_match[0] / bg_info.fps) - (first_match[1] / fg_info.fps)

        return TemporalAlignment(
            offset_seconds=offset_seconds,
            frame_alignments=frame_alignments,
            method_used="frames",
            confidence=self._calculate_alignment_confidence(keyframe_matches),
        )

    def _precompute_frame_hashes(
        self, video_path: str, frame_indices: list[int], resize_factor: float = 0.125
    ) -> dict[int, np.ndarray]:
        """Pre-compute perceptual hashes for frames in parallel."""
        if not self.use_perceptual_hash or self.hasher is None:
            return {}

        # Check cache first
        if video_path in self.hash_cache:
            cached = self.hash_cache[video_path]
            if all(idx in cached for idx in frame_indices):
                return {idx: cached[idx] for idx in frame_indices}

        logger.debug(f"Pre-computing hashes for {len(frame_indices)} frames")
        start_time = time.time()

        # Extract frames
        frames = self.processor.extract_frames(video_path, frame_indices, resize_factor)

        # Compute hashes in parallel
        hashes = {}
        with ThreadPoolExecutor(max_workers=mp.cpu_count()) as executor:
            future_to_idx = {
                executor.submit(self._compute_frame_hash, frame): idx
                for idx, frame in zip(frame_indices, frames, strict=False)
                if frame is not None
            }

            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    hash_value = future.result()
                    hashes[idx] = hash_value
                except Exception as e:
                    logger.warning(f"Failed to compute hash for frame {idx}: {e}")

        # Update cache
        if video_path not in self.hash_cache:
            self.hash_cache[video_path] = {}
        self.hash_cache[video_path].update(hashes)

        elapsed = time.time() - start_time
        logger.debug(f"Computed {len(hashes)} hashes in {elapsed:.2f}s")

        return hashes

    def _find_keyframe_matches(
        self, bg_info: VideoInfo, fg_info: VideoInfo
    ) -> list[tuple[int, int, float]]:
        """Find matching keyframes between videos using monotonic dynamic programming.

        Returns:
            List of (bg_frame_idx, fg_frame_idx, similarity) tuples

        Why keyframe matching:
        - Can't compare all frame pairs (too expensive)
        - Keyframes capture important moments
        - Interpolation fills in between keyframes

        Why uniform sampling:
        - Ensures coverage of entire video
        - Predictable behavior
        - Works for any content type

        Current issues (see SPEC4.md for solutions):
        - Independent matching breaks monotonicity
        - Cost matrix computation is slow
        - Poor interpolation between sparse keyframes
        """
        # Determine sampling rate
        effective_target_keyframes = min(self.max_keyframes, fg_info.frame_count)

        if (
            not self.use_perceptual_hash or self.hasher is None
        ):  # Indicates SSIM will be used
            # If SSIM is used and the number of keyframes is high, warn the user.
            if effective_target_keyframes > 200:  # Threshold for SSIM warning
                logger.warning(
                    f"SSIM mode active for keyframe matching with a high target of {effective_target_keyframes} keyframes. "
                    f"This may be very slow. Consider reducing --max_keyframes or ensuring perceptual hashing is available."
                )
            else:
                logger.info(
                    f"SSIM mode active for keyframe matching. Target keyframes: {effective_target_keyframes}"
                )
        else:
            logger.info(
                f"Perceptual hashing mode active. Target keyframes: {effective_target_keyframes}"
            )

        sample_interval = max(1, fg_info.frame_count // effective_target_keyframes)

        logger.info(f"Sampling every {sample_interval} frames for keyframe matching")

        # Prepare indices - sample uniformly across the video
        fg_indices = list(range(0, fg_info.frame_count, sample_interval))
        # Always include first and last frames
        if 0 not in fg_indices:
            fg_indices.insert(0, 0)
        if fg_info.frame_count - 1 not in fg_indices:
            fg_indices.append(fg_info.frame_count - 1)

        # Sample more densely from background to allow flexibility
        bg_sample_interval = max(1, sample_interval // 2)
        bg_indices = list(range(0, bg_info.frame_count, bg_sample_interval))

        logger.info(
            f"Sampling {len(fg_indices)} FG frames and {len(bg_indices)} BG frames"
        )

        # Pre-compute all hashes if available
        if self.use_perceptual_hash and self.hasher is not None:
            logger.info("Pre-computing perceptual hashes...")
            fg_hashes = self._precompute_frame_hashes(fg_info.path, fg_indices, 0.125)
            bg_hashes = self._precompute_frame_hashes(bg_info.path, bg_indices, 0.125)

            if not fg_hashes or not bg_hashes:
                logger.error("Failed to compute hashes, falling back to SSIM")
                self.use_perceptual_hash = False

        # Build cost matrix using dynamic programming approach
        logger.info("Building cost matrix for dynamic programming alignment...")
        cost_matrix = self._build_cost_matrix(bg_info, fg_info, bg_indices, fg_indices)

        if cost_matrix is None:
            logger.error("Failed to build cost matrix")
            return []

        # Find optimal monotonic path through cost matrix
        matches = self._find_optimal_path(cost_matrix, bg_indices, fg_indices)

        # Validate matches with higher quality check if needed
        if len(matches) < 10:
            logger.warning(
                f"Only found {len(matches)} matches, attempting refinement..."
            )
            matches = self._refine_matches(
                matches, bg_info, fg_info, bg_indices, fg_indices
            )

        logger.info(f"Found {len(matches)} monotonic keyframe matches")

        if self.use_perceptual_hash:
            logger.info("✓ Perceptual hashing provided significant speedup")

        return matches

    def _compute_frame_similarity(
        self, frame1: np.ndarray, frame2: np.ndarray, mask: np.ndarray | None = None
    ) -> float:
        """Compute similarity between two frames using perceptual hash or SSIM.

        Args:
            frame1: First frame to compare
            frame2: Second frame to compare
            mask: Optional binary mask to restrict comparison to specific regions
        """
        if mask is not None:
            frame1 = self._apply_mask_to_frame(frame1, mask)
            frame2 = self._apply_mask_to_frame(frame2, mask)

        if self.use_perceptual_hash and self.hasher is not None:
            hash1 = self._compute_frame_hash(frame1)
            hash2 = self._compute_frame_hash(frame2)
            distance = cv2.norm(hash1, hash2, cv2.NORM_HAMMING)
            max_distance = 64
            similarity = 1.0 - (distance / max_distance)
            return float(similarity)
        else:
            return self._compute_ssim_similarity(frame1, frame2)

    def _compute_ssim_similarity(self, frame1: np.ndarray, frame2: np.ndarray) -> float:
        """Compute similarity between two frames using SSIM."""
        # Convert to grayscale
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)

        # Ensure same size
        if gray1.shape != gray2.shape:
            gray2 = cv2.resize(gray2, (gray1.shape[1], gray1.shape[0]))

        # Compute SSIM
        score, _ = ssim(gray1, gray2, full=True)
        return float(score)

    def _compute_frame_hash(self, frame: np.ndarray) -> np.ndarray:
        """Compute perceptual hash of a frame."""
        # Resize to standard size for consistent hashing
        resized = cv2.resize(frame, (32, 32), interpolation=cv2.INTER_AREA)

        # Convert to grayscale if needed
        if len(resized.shape) == 3:
            resized = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)

        # Compute hash
        hash_value = self.hasher.compute(resized)
        return hash_value

    def _build_cost_matrix(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        bg_indices: list[int],
        fg_indices: list[int],
    ) -> np.ndarray | None:
        """Build cost matrix for dynamic programming alignment.

        Lower cost = better match. Uses perceptual hashes if available.
        """
        n_fg = len(fg_indices)
        n_bg = len(bg_indices)

        # Initialize cost matrix
        cost_matrix = np.full((n_fg, n_bg), np.inf)

        # Use hashes if available
        if (
            self.use_perceptual_hash
            and self.hasher is not None
            and bg_info.path in self.hash_cache
            and fg_info.path in self.hash_cache
        ):
            logger.debug("Building cost matrix using perceptual hashes")

            for i, fg_idx in enumerate(fg_indices):
                fg_hash = self.hash_cache[fg_info.path].get(fg_idx)
                if fg_hash is None:
                    continue

                for j, bg_idx in enumerate(bg_indices):
                    bg_hash = self.hash_cache[bg_info.path].get(bg_idx)
                    if bg_hash is None:
                        continue

                    # Hamming distance as cost
                    distance = cv2.norm(fg_hash, bg_hash, cv2.NORM_HAMMING)
                    cost_matrix[i, j] = distance
        else:
            logger.debug("Building cost matrix using frame extraction")
            # Fallback to extracting and comparing frames
            resize_factor = 0.25

            with Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TimeRemainingColumn(),
                console=console,
                transient=True,
            ) as progress:
                task = progress.add_task("  Comparing frames...", total=n_fg * n_bg)

                for i, fg_idx in enumerate(fg_indices):
                    fg_frames = self.processor.extract_frames(
                        fg_info.path, [fg_idx], resize_factor
                    )
                    if not fg_frames or fg_frames[0] is None:
                        progress.update(task, advance=n_bg)
                        continue

                    fg_frame = fg_frames[0]

                    for j, bg_idx in enumerate(bg_indices):
                        bg_frames = self.processor.extract_frames(
                            bg_info.path, [bg_idx], resize_factor
                        )
                        if not bg_frames or bg_frames[0] is None:
                            progress.update(task, advance=1)
                            continue

                        bg_frame = bg_frames[0]

                        # Use SSIM as similarity, convert to cost
                        similarity = self._compute_ssim_similarity(bg_frame, fg_frame)
                        cost_matrix[i, j] = 1.0 - similarity

                        progress.update(task, advance=1)

        # Add temporal consistency penalty
        # Penalize large jumps in time
        for i in range(n_fg):
            for j in range(n_bg):
                # Expected position based on linear time mapping
                expected_j = int(i * n_bg / n_fg)
                time_penalty = 0.1 * abs(j - expected_j) / n_bg
                cost_matrix[i, j] += time_penalty

        return cost_matrix

    def _find_optimal_path(
        self, cost_matrix: np.ndarray, bg_indices: list[int], fg_indices: list[int]
    ) -> list[tuple[int, int, float]]:
        """Find optimal monotonic path through cost matrix using dynamic programming."""
        n_fg, n_bg = cost_matrix.shape

        # Dynamic programming table
        dp = np.full_like(cost_matrix, np.inf)
        parent = np.zeros_like(cost_matrix, dtype=int)

        # Initialize first row - first fg frame can match any bg frame
        dp[0, :] = cost_matrix[0, :]

        # Fill DP table
        for i in range(1, n_fg):
            for j in range(n_bg):
                # Can only come from previous bg frames (monotonic constraint)
                for k in range(j + 1):
                    if dp[i - 1, k] + cost_matrix[i, j] < dp[i, j]:
                        dp[i, j] = dp[i - 1, k] + cost_matrix[i, j]
                        parent[i, j] = k

        # Find best path by backtracking from minimum cost in last row
        min_j = np.argmin(dp[-1, :])
        path = []

        # Backtrack
        j = min_j
        for i in range(n_fg - 1, -1, -1):
            # Convert cost back to similarity
            similarity = 1.0 - cost_matrix[i, j]
            path.append((bg_indices[j], fg_indices[i], similarity))

            if i > 0:
                j = parent[i, j]

        path.reverse()

        # Filter out low-quality matches
        filtered_path = [
            match
            for match in path
            if match[2] > 0.5  # Similarity threshold
        ]

        return filtered_path

    def _refine_matches(
        self,
        initial_matches: list[tuple[int, int, float]],
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        bg_indices: list[int],
        fg_indices: list[int],
    ) -> list[tuple[int, int, float]]:
        """Refine matches by adding intermediate keyframes where needed."""
        if len(initial_matches) < 2:
            return initial_matches

        refined = [initial_matches[0]]

        for i in range(1, len(initial_matches)):
            prev_match = refined[-1]
            curr_match = initial_matches[i]

            # Check if there's a large gap
            fg_gap = curr_match[1] - prev_match[1]
            curr_match[0] - prev_match[0]

            if fg_gap > 50:  # Large gap in foreground frames
                # Add intermediate keyframe
                mid_fg = (prev_match[1] + curr_match[1]) // 2
                mid_bg = (prev_match[0] + curr_match[0]) // 2

                # Find closest sampled indices
                closest_fg = min(fg_indices, key=lambda x: abs(x - mid_fg))
                closest_bg = min(bg_indices, key=lambda x: abs(x - mid_bg))

                # Compute similarity for intermediate frame
                if (
                    self.use_perceptual_hash
                    and fg_info.path in self.hash_cache
                    and bg_info.path in self.hash_cache
                ):
                    fg_hash = self.hash_cache[fg_info.path].get(closest_fg)
                    bg_hash = self.hash_cache[bg_info.path].get(closest_bg)

                    if fg_hash is not None and bg_hash is not None:
                        distance = cv2.norm(fg_hash, bg_hash, cv2.NORM_HAMMING)
                        similarity = 1.0 - (distance / 64.0)

                        if similarity > 0.5:
                            refined.append((closest_bg, closest_fg, similarity))

            refined.append(curr_match)

        return refined

    def _filter_monotonic(
        self, matches: list[tuple[int, int, float]]
    ) -> list[tuple[int, int, float]]:
        """Filter matches to ensure monotonic progression.

        This is now only used as a safety check since the DP algorithm
        already ensures monotonicity.
        """
        if not matches:
            return matches

        # Sort by foreground index
        matches.sort(key=lambda x: x[1])

        # Verify monotonicity (should already be monotonic from DP)
        filtered = []
        last_bg_idx = -1

        for bg_idx, fg_idx, sim in matches:
            if bg_idx > last_bg_idx:
                filtered.append((bg_idx, fg_idx, sim))
                last_bg_idx = bg_idx
            else:
                logger.warning(
                    f"Unexpected non-monotonic match: bg[{bg_idx}] for fg[{fg_idx}]"
                )

        return filtered

    def _build_frame_alignments(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        keyframe_matches: list[tuple[int, int, float]],
        trim: bool,
    ) -> list[FrameAlignment]:
        """Build complete frame-to-frame alignment.

        Creates alignment for EVERY foreground frame, finding the optimal
        background frame based on keyframe matches.
        """
        alignments = []

        # Determine range of foreground frames to process
        if trim and keyframe_matches:
            start_fg = keyframe_matches[0][1]
            end_fg = keyframe_matches[-1][1] + 1
        else:
            start_fg = 0
            end_fg = fg_info.frame_count

        logger.info(f"Building alignment for FG frames {start_fg} to {end_fg - 1}")

        # For each foreground frame, find optimal background frame
        for fg_idx in range(start_fg, end_fg):
            bg_idx = self._interpolate_bg_frame(
                fg_idx, keyframe_matches, bg_info, fg_info
            )

            # Ensure bg_idx is valid
            bg_idx = max(0, min(bg_idx, bg_info.frame_count - 1))

            # Estimate similarity based on nearby keyframes
            similarity = self._estimate_similarity(fg_idx, keyframe_matches)

            alignments.append(
                FrameAlignment(
                    fg_frame_idx=fg_idx,
                    bg_frame_idx=bg_idx,
                    similarity_score=similarity,
                )
            )

        logger.info(f"Created {len(alignments)} frame alignments")
        return alignments

    def _interpolate_bg_frame(
        self,
        fg_idx: int,
        keyframe_matches: list[tuple[int, int, float]],
        bg_info: VideoInfo,
        fg_info: VideoInfo,
    ) -> int:
        """Interpolate background frame index for given foreground frame.

        Uses smooth interpolation between keyframe matches to avoid
        sudden jumps or speed changes.
        """
        if not keyframe_matches:
            # Simple ratio-based mapping
            ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0
            return int(fg_idx * ratio)

        # Find surrounding keyframes
        prev_match = None
        next_match = None

        for match in keyframe_matches:
            if match[1] <= fg_idx:
                prev_match = match
            elif match[1] > fg_idx and next_match is None:
                next_match = match
                break

        # Handle edge cases
        if prev_match is None:
            prev_match = keyframe_matches[0]
        if next_match is None:
            next_match = keyframe_matches[-1]

        # If at or beyond edges, extrapolate
        if fg_idx <= prev_match[1]:
            # Before first keyframe
            fps_ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0
            offset = (prev_match[1] - fg_idx) * fps_ratio
            return int(prev_match[0] - offset)

        if fg_idx >= next_match[1]:
            # After last keyframe
            fps_ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0
            offset = (fg_idx - next_match[1]) * fps_ratio
            return int(next_match[0] + offset)

        # Interpolate between keyframes
        if prev_match[1] == next_match[1]:
            return prev_match[0]

        # Calculate position ratio with smooth interpolation
        ratio = (fg_idx - prev_match[1]) / (next_match[1] - prev_match[1])

        # Apply smoothstep for more natural motion
        smooth_ratio = ratio * ratio * (3.0 - 2.0 * ratio)

        # Interpolate background frame
        bg_idx = prev_match[0] + smooth_ratio * (next_match[0] - prev_match[0])

        return int(bg_idx)

    def _estimate_similarity(
        self, fg_idx: int, keyframe_matches: list[tuple[int, int, float]]
    ) -> float:
        """Estimate similarity score for a frame based on nearby keyframes."""
        if not keyframe_matches:
            return 0.5

        # Find closest keyframe
        min_dist = float("inf")
        closest_sim = 0.5

        for _, kf_fg_idx, similarity in keyframe_matches:
            dist = abs(fg_idx - kf_fg_idx)
            if dist < min_dist:
                min_dist = dist
                closest_sim = similarity

        # Decay similarity based on distance
        decay_rate = 0.95**min_dist
        return closest_sim * decay_rate

    def _calculate_alignment_confidence(
        self, keyframe_matches: list[tuple[int, int, float]]
    ) -> float:
        """Calculate overall confidence in the alignment."""
        if not keyframe_matches:
            return 0.0

        # Average similarity of matches
        avg_similarity = sum(m[2] for m in keyframe_matches) / len(keyframe_matches)

        # Coverage (how well distributed the matches are)
        coverage = len(keyframe_matches) / max(len(keyframe_matches), 20)

        return min(1.0, avg_similarity * coverage)

    def _create_direct_mapping(
        self, bg_info: VideoInfo, fg_info: VideoInfo
    ) -> TemporalAlignment:
        """Create simple direct frame mapping as fallback."""
        fps_ratio = bg_info.fps / fg_info.fps if fg_info.fps > 0 else 1.0

        alignments = []
        for fg_idx in range(fg_info.frame_count):
            bg_idx = int(fg_idx * fps_ratio)
            bg_idx = min(bg_idx, bg_info.frame_count - 1)

            alignments.append(
                FrameAlignment(
                    fg_frame_idx=fg_idx, bg_frame_idx=bg_idx, similarity_score=0.5
                )
            )

        return TemporalAlignment(
            offset_seconds=0.0,
            frame_alignments=alignments,
            method_used="direct",
            confidence=0.3,
        )

    def _align_frames_dtw(
        self, bg_info: VideoInfo, fg_info: VideoInfo, trim: bool = False
    ) -> TemporalAlignment:
        """Align videos using Dynamic Time Warping for guaranteed monotonic alignment.

        This is the new improved method that replaces the problematic keyframe matching.

        Why DTW:
        - Guarantees monotonic alignment (no backward jumps)
        - Finds globally optimal path
        - Handles speed variations naturally
        - No more drift or catch-up issues
        """
        logger.info("Using DTW-based temporal alignment")

        # Initialize fingerprinter if needed
        if self.fingerprinter is None:
            try:
                self.fingerprinter = FrameFingerprinter()
            except Exception as e:
                logger.error(f"Failed to initialize fingerprinter: {e}")
                logger.warning("Falling back to classic alignment")
                self.use_dtw = False
                return self.align_frames(bg_info, fg_info, trim)

        # Determine frames to sample
        fg_sample_interval = max(1, fg_info.frame_count // self.max_keyframes)
        bg_sample_interval = max(1, bg_info.frame_count // (self.max_keyframes * 2))

        fg_indices = list(range(0, fg_info.frame_count, fg_sample_interval))
        bg_indices = list(range(0, bg_info.frame_count, bg_sample_interval))

        # Always include first and last frames
        if 0 not in fg_indices:
            fg_indices.insert(0, 0)
        if fg_info.frame_count - 1 not in fg_indices:
            fg_indices.append(fg_info.frame_count - 1)
        if 0 not in bg_indices:
            bg_indices.insert(0, 0)
        if bg_info.frame_count - 1 not in bg_indices:
            bg_indices.append(bg_info.frame_count - 1)

        logger.info(
            f"DTW sampling: {len(fg_indices)} FG frames, {len(bg_indices)} BG frames"
        )

        # Compute fingerprints for sampled frames
        logger.info("Computing frame fingerprints...")

        fg_fingerprints = self.fingerprinter.precompute_video_fingerprints(
            fg_info.path, fg_indices, self.processor, resize_factor=0.25
        )

        bg_fingerprints = self.fingerprinter.precompute_video_fingerprints(
            bg_info.path, bg_indices, self.processor, resize_factor=0.25
        )

        if not fg_fingerprints or not bg_fingerprints:
            logger.error("Failed to compute fingerprints")
            logger.warning("Falling back to classic alignment")
            self.use_dtw = False
            return self.align_frames(bg_info, fg_info, trim)

        # Run DTW alignment
        logger.info("Running DTW alignment...")
        dtw_matches = self.dtw_aligner.align_videos(
            fg_fingerprints,
            bg_fingerprints,
            self.fingerprinter.compare_fingerprints,
            show_progress=True,
        )

        if not dtw_matches:
            logger.warning("DTW produced no matches, using direct mapping")
            return self._create_direct_mapping(bg_info, fg_info)

        # Create complete frame alignments from DTW matches
        frame_alignments = self.dtw_aligner.create_frame_alignments(
            dtw_matches, fg_info.frame_count, bg_info.frame_count
        )

        # Apply trimming if requested
        if trim and frame_alignments:
            # Find actual matched range
            bg_indices_used = [a.bg_frame_idx for a in frame_alignments]
            min(bg_indices_used)
            max(bg_indices_used)

            # Trim to only include frames with good matches
            trimmed_alignments = []
            for alignment in frame_alignments:
                if alignment.similarity_score > 0.5:  # Quality threshold
                    trimmed_alignments.append(alignment)

            if trimmed_alignments:
                frame_alignments = trimmed_alignments

        # Calculate overall temporal offset
        if dtw_matches:
            first_match = dtw_matches[0]
            offset_seconds = (first_match[0] / bg_info.fps) - (
                first_match[1] / fg_info.fps
            )
        else:
            offset_seconds = 0.0

        # Calculate overall confidence
        if frame_alignments:
            avg_confidence = sum(a.similarity_score for a in frame_alignments) / len(
                frame_alignments
            )
        else:
            avg_confidence = 0.5

        logger.info(
            f"DTW alignment complete: {len(frame_alignments)} frames, "
            f"offset={offset_seconds:.3f}s, confidence={avg_confidence:.3f}"
        )

        return TemporalAlignment(
            offset_seconds=offset_seconds,
            frame_alignments=frame_alignments,
            method_used="dtw",
            confidence=avg_confidence,
        )

    def create_border_mask(
        self,
        spatial_alignment,
        fg_info: VideoInfo,
        bg_info: VideoInfo,
        border_thickness: int = 8,
    ) -> np.ndarray:
        """Create border mask for border-based temporal alignment.

        The border mask defines the region around the foreground video edges where
        background video is visible. This is used for similarity comparison in border mode.

        Args:
            spatial_alignment: Result from spatial alignment containing x/y offsets
            fg_info: Foreground video information
            bg_info: Background video information
            border_thickness: Thickness of border region in pixels

        Returns:
            Binary mask where 1 indicates border region, 0 indicates non-border
        """
        # Get foreground position on background canvas
        x_offset = spatial_alignment.x_offset
        y_offset = spatial_alignment.y_offset
        fg_width = fg_info.width
        fg_height = fg_info.height
        bg_width = bg_info.width
        bg_height = bg_info.height

        # Create mask same size as background
        mask = np.zeros((bg_height, bg_width), dtype=np.uint8)

        # Define foreground rectangle bounds
        fg_left = x_offset
        fg_right = x_offset + fg_width
        fg_top = y_offset
        fg_bottom = y_offset + fg_height

        # Ensure bounds are within background
        fg_left = max(0, fg_left)
        fg_right = min(bg_width, fg_right)
        fg_top = max(0, fg_top)
        fg_bottom = min(bg_height, fg_bottom)

        # Define border regions based on which edges have visible background

        # Top border (if fg doesn't touch top edge)
        if fg_top > 0:
            border_top = max(0, fg_top - border_thickness)
            mask[border_top:fg_top, fg_left:fg_right] = 1

        # Bottom border (if fg doesn't touch bottom edge)
        if fg_bottom < bg_height:
            border_bottom = min(bg_height, fg_bottom + border_thickness)
            mask[fg_bottom:border_bottom, fg_left:fg_right] = 1

        # Left border (if fg doesn't touch left edge)
        if fg_left > 0:
            border_left = max(0, fg_left - border_thickness)
            mask[fg_top:fg_bottom, border_left:fg_left] = 1

        # Right border (if fg doesn't touch right edge)
        if fg_right < bg_width:
            border_right = min(bg_width, fg_right + border_thickness)
            mask[fg_top:fg_bottom, fg_right:border_right] = 1

        logger.debug(f"Created border mask: {np.sum(mask)} pixels in border region")
        return mask

    def _apply_mask_to_frame(self, frame: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """Apply binary mask to frame, setting non-masked areas to black.

        Args:
            frame: Input frame (H, W, C) or (H, W)
            mask: Binary mask (H, W) where 1 = keep, 0 = zero out

        Returns:
            Masked frame with same dimensions as input
        """
        if len(frame.shape) == 3:
            # Color frame - apply mask to all channels
            masked = frame.copy()
            for c in range(frame.shape[2]):
                masked[:, :, c] = frame[:, :, c] * mask
        else:
            # Grayscale frame
            masked = frame * mask

        return masked

    def create_blend_mask(
        self,
        spatial_alignment,
        fg_info: VideoInfo,
        bg_info: VideoInfo,
        border_thickness: int = 8,
    ) -> np.ndarray:
        """Create blend mask for smooth edge transitions.

        Creates a gradient mask that transitions from fully opaque (1.0) in the center
        of the foreground to fully transparent (0.0) at the edges where background is visible.

        Args:
            spatial_alignment: Result from spatial alignment containing x/y offsets
            fg_info: Foreground video information
            bg_info: Background video information
            border_thickness: Width of gradient transition in pixels

        Returns:
            Float mask with values 0.0-1.0 for alpha blending
        """
        # Get foreground position on background canvas
        x_offset = spatial_alignment.x_offset
        y_offset = spatial_alignment.y_offset
        fg_width = fg_info.width
        fg_height = fg_info.height
        bg_width = bg_info.width
        bg_height = bg_info.height

        # Create mask same size as foreground (will be placed on background)
        mask = np.ones((fg_height, fg_width), dtype=np.float32)

        # Determine which edges need blending (where bg is visible)
        blend_top = y_offset > 0
        blend_bottom = (y_offset + fg_height) < bg_height
        blend_left = x_offset > 0
        blend_right = (x_offset + fg_width) < bg_width

        # Create gradient on edges that need blending
        for y in range(fg_height):
            for x in range(fg_width):
                alpha = 1.0

                # Top edge gradient
                if blend_top and y < border_thickness:
                    alpha = min(alpha, y / border_thickness)

                # Bottom edge gradient
                if blend_bottom and y >= (fg_height - border_thickness):
                    alpha = min(alpha, (fg_height - 1 - y) / border_thickness)

                # Left edge gradient
                if blend_left and x < border_thickness:
                    alpha = min(alpha, x / border_thickness)

                # Right edge gradient
                if blend_right and x >= (fg_width - border_thickness):
                    alpha = min(alpha, (fg_width - 1 - x) / border_thickness)

                mask[y, x] = max(0.0, min(1.0, alpha))

        logger.debug(f"Created blend mask with {border_thickness}px gradient")
        return mask

    def align_frames_with_mask(
        self,
        bg_info: VideoInfo,
        fg_info: VideoInfo,
        trim: bool = False,
        mask: np.ndarray | None = None,
    ) -> TemporalAlignment:
        self._current_mask = mask

        original_use_dtw = self.use_dtw
        original_use_perceptual_hash = self.use_perceptual_hash

        effective_use_dtw = self.use_dtw
        effective_use_perceptual_hash = self.use_perceptual_hash

        if mask is not None:
            logger.info("Border mode active for temporal alignment.")
            if self.use_dtw:
                logger.warning(
                    "Border mode forces classic (keyframe/SSIM) alignment to ensure "
                    "mask is respected, even if DTW is selected. DTW with perceptual "
                    "hashing does not yet support masked regions effectively."
                )
                effective_use_dtw = False

            if not effective_use_dtw and self.use_perceptual_hash:
                logger.info(
                    "In border mode (classic path), forcing SSIM comparison (disabling "
                    "perceptual hash) to ensure mask is applied."
                )
                self.use_perceptual_hash = False
                effective_use_perceptual_hash = False
            elif not effective_use_dtw and not self.use_perceptual_hash:
                effective_use_perceptual_hash = False

        alignment_result: TemporalAlignment
        try:
            # Attempt to initialize fingerprinter for DTW if needed and not already done
            if effective_use_dtw and self.fingerprinter is None:
                try:
                    self.fingerprinter = FrameFingerprinter()
                except Exception as e:
                    logger.error(f"Failed to init FrameFingerprinter for DTW: {e}")
                    logger.warning("Falling back to classic alignment.")
                    effective_use_dtw = False
                    self.use_perceptual_hash = original_use_perceptual_hash
                    effective_use_perceptual_hash = original_use_perceptual_hash

            if effective_use_dtw and self.fingerprinter is not None:
                logger.info("Using DTW-based temporal alignment (full frame).")
                alignment_result = self._align_frames_dtw(bg_info, fg_info, trim)
            else:
                log_hash_status = (
                    "Enabled"
                    if self.use_perceptual_hash and self.hasher
                    else "Disabled (SSIM)"
                )
                logger.info(
                    f"Using classic (keyframe-based) temporal alignment. "
                    f"Hash/SSIM: {log_hash_status}."
                )
                keyframe_matches = self._find_keyframe_matches(bg_info, fg_info)

                if not keyframe_matches:
                    logger.warning("No keyframe matches found, using direct mapping.")
                    alignment_result = self._create_direct_mapping(bg_info, fg_info)
                else:
                    frame_alignments = self._build_frame_alignments(
                        bg_info, fg_info, keyframe_matches, trim
                    )
                    first_match = keyframe_matches[0]
                    offset_seconds = (first_match[0] / bg_info.fps) - (
                        first_match[1] / fg_info.fps
                    )
                    confidence = self._calculate_alignment_confidence(keyframe_matches)

                    method_detail = (
                        "hash"
                        if effective_use_perceptual_hash and self.hasher
                        else "SSIM"
                    )
                    base_method_str = "border" if mask is not None else "frames"
                    method_used_str = f"{base_method_str} (classic/{method_detail})"

                    alignment_result = TemporalAlignment(
                        offset_seconds=offset_seconds,
                        frame_alignments=frame_alignments,
                        method_used=method_used_str,
                        confidence=confidence,
                    )
            return alignment_result
        finally:
            self._current_mask = None
            self.use_dtw = original_use_dtw
            self.use_perceptual_hash = original_use_perceptual_hash

    def _find_keyframe_matches_with_mask(
        self, bg_info: VideoInfo, fg_info: VideoInfo, mask: np.ndarray | None = None
    ) -> list[tuple[int, int, float]]:
        """Find matching keyframes between videos with optional mask support."""
        # This is similar to _find_keyframe_matches but uses masked similarity
        # For now, we'll modify the existing method to support masks
        return self._find_keyframe_matches(bg_info, fg_info)
</file>

</files>
